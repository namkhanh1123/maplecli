{"chunks": [{"filepath": "chat_client.py", "start_line": 10, "end_line": 158, "type": "class", "name": "ChatClient", "content": "class ChatClient:\n    \"\"\"Handles all communication with the OpenAI-compatible API.\"\"\"\n    def __init__(self, api_base: str, api_key: Optional[str], console: Console):\n        self.api_base = api_base\n        self.api_key = api_key\n        self.console = console\n        self.timeout = 30\n        self.max_retries = 3\n\n    def list_models(self, model_type: Optional[str] = None) -> List[Dict[str, any]]:\n        \"\"\"Fetches the list of available models from the API.\"\"\"\n        headers = {}\n        if self.api_key:\n            headers[\"Authorization\"] = f\"Bearer {self.api_key}\"\n\n        try:\n            with self.console.status(\"[bold green]Fetching models...[/bold green]\"):\n                response = requests.get(f\"{self.api_base}/models\", headers=headers, timeout=self.timeout)\n            if response.status_code == 200:\n                data = response.json()\n                if \"data\" in data and isinstance(data[\"data\"], list):\n                    models = [model for model in data[\"data\"] if \"id\" in model]\n                    if model_type:\n                        return [m for m in models if model_type in m.get(\"type\", [])]\n                    return models\n                else:\n                    self.console.print(\"[bold red]Error: Unexpected API response format[/bold red]\")\n                    return []\n            else:\n                self.console.print(f\"[bold red]Error: Failed to fetch models with status code {response.status_code}[/bold red]\")\n                return []\n        except requests.exceptions.RequestException as e:\n            self.console.print(f\"[bold red]Error: {e}[/bold red]\")\n            return []\n\n    def stream_chat(self, history: List[Dict[str, str]], model: str, \n                    temperature: float = 0.7, max_tokens: Optional[int] = None) -> Tuple[List[Dict[str, str]], str]:\n        \"\"\"Sends a prompt to the chat API and streams the response.\"\"\"\n        headers = {\"Content-Type\": \"application/json\"}\n        if self.api_key:\n            headers[\"Authorization\"] = f\"Bearer {self.api_key}\"\n\n        data = {\"model\": model, \"messages\": history, \"stream\": True, \"temperature\": temperature}\n        if max_tokens:\n            data[\"max_tokens\"] = max_tokens\n\n        assistant_response = \"\"\n        print()\n        self.console.print(\"[bold white]\u250c\u2500[/bold white][bold cyan]Assistant[/bold cyan][bold white]:[/bold white]\")\n        self.console.print(\"[bold white]\u2502[/bold white] [dim]Thinking...[/dim]\", end=\"\")\n        \n        for attempt in range(self.max_retries):\n            try:\n                with requests.post(f\"{self.api_base}/chat/completions\", headers=headers, json=data, stream=True, timeout=self.timeout) as response:\n                    if response.status_code == 200:\n                        print(\"\\r\" + \" \" * 80 + \"\\r\", end=\"\", flush=True)\n                        self.console.print(\"[bold white]\u2514\u2500>[/bold white] \", end=\"\")\n                        \n                        for chunk in response.iter_lines():\n                            if chunk:\n                                chunk_str = chunk.decode('utf-8')\n                                if chunk_str.startswith(\"data: \"):\n                                    chunk_str = chunk_str[6:]\n                                if chunk_str == \"[DONE]\":\n                                    break\n                                try:\n                                    json_chunk = json.loads(chunk_str)\n                                    if \"choices\" in json_chunk and len(json_chunk[\"choices\"]) > 0:\n                                        content = json_chunk[\"choices\"][0].get(\"delta\", {}).get(\"content\")\n                                        if content:\n                                            assistant_response += content\n                                            print(content, end=\"\", flush=True)\n                                except json.JSONDecodeError:\n                                    pass\n                        print()\n                        break\n                    else:\n                        error_message = f\"API request failed with status code {response.status_code}\"\n                        self.console.print(f\"\\n[bold red]Error: {error_message}[/bold red]\")\n                        if attempt >= self.max_retries - 1:\n                            raise Exception(error_message)\n            except requests.exceptions.RequestException as e:\n                self.console.print(f\"\\n[bold red]Error: {e}[/bold red]\")\n                if attempt >= self.max_retries - 1:\n                    raise\n        \n        history.append({\"role\": \"assistant\", \"content\": assistant_response})\n        return history, assistant_response\n\n    def generate_image(self, prompt: str, model: str = \"dall-e-3\", size: str = \"1024x1024\", quality: str = \"standard\", n: int = 1) -> List[str]:\n        \"\"\"Generates images from text prompt.\"\"\"\n        headers = {\"Content-Type\": \"application/json\"}\n        if self.api_key:\n            headers[\"Authorization\"] = f\"Bearer {self.api_key}\"\n\n        data = {\"model\": model, \"prompt\": prompt, \"n\": n, \"size\": size, \"quality\": quality}\n        \n        try:\n            self.console.print(\"[bold cyan]Generating image...[/bold cyan]\")\n            response = requests.post(f\"{self.api_base}/images/generations\", headers=headers, json=data, timeout=60)\n            if response.status_code == 200:\n                return [img.get(\"url\") or img.get(\"b64_json\") for img in response.json().get(\"data\", [])]\n            else:\n                self.console.print(f\"[bold red]Image generation failed with status {response.status_code}[/bold red]\")\n                return []\n        except requests.exceptions.RequestException as e:\n            self.console.print(f\"[bold red]Error: {e}[/bold red]\")\n            return []\n\n    def generate_video(self, prompt: str, model: str = \"sora-2\") -> Optional[str]:\n        \"\"\"Generates video from text prompt.\"\"\"\n        headers = {\"Content-Type\": \"application/json\"}\n        if self.api_key:\n            headers[\"Authorization\"] = f\"Bearer {self.api_key}\"\n\n        data = {\"model\": model, \"prompt\": prompt}\n        \n        try:\n            self.console.print(\"[bold cyan]Generating video...[/bold cyan]\")\n            response = requests.post(f\"{self.api_base}/videos\", headers=headers, json=data, timeout=300)\n            if response.status_code == 200:\n                result = response.json()\n                return result.get(\"url\") or (result.get(\"data\", [{}])[0].get(\"url\") if result.get(\"data\") else None)\n            else:\n                self.console.print(f\"[bold red]Video generation failed with status {response.status_code}[/bold red]\")\n                return None\n        except requests.exceptions.RequestException as e:\n            self.console.print(f\"[bold red]Error: {e}[/bold red]\")\n            return None\n\n    def text_to_speech(self, text: str, model: str = \"tts-1\", voice: str = \"alloy\", speed: float = 1.0) -> Optional[bytes]:\n        \"\"\"Converts text to speech.\"\"\"\n        headers = {\"Content-Type\": \"application/json\"}\n        if self.api_key:\n            headers[\"Authorization\"] = f\"Bearer {self.api_key}\"\n\n        data = {\"model\": model, \"input\": text, \"voice\": voice, \"speed\": speed}\n        \n        try:\n            self.console.print(\"[bold cyan]Generating speech...[/bold cyan]\")\n            response = requests.post(f\"{self.api_base}/audio/speech\", headers=headers, json=data, timeout=60)\n            if response.status_code == 200:\n                return response.content\n            else:\n                self.console.print(f\"[bold red]TTS failed with status {response.status_code}[/bold red]\")\n                return None\n        except requests.exceptions.RequestException as e:\n            self.console.print(f\"[bold red]Error: {e}[/bold red]\")\n            return None", "docstring": "Handles all communication with the OpenAI-compatible API.", "searchable_text": "ChatClient Handles all communication with the OpenAI-compatible API. class ChatClient:\n    \"\"\"Handles all communication with the OpenAI-compatible API.\"\"\"\n    def __init__(self, api_base: str, api_key: Optional[str], console: Console):\n        self.api_base = api_base\n        self.api_key = api_key\n        self.console = console\n        self.timeout = 30\n        self.max_retries = 3\n\n    def list_models(self, model_type: Optional[str] = None) -> List[Dict[str, any]]:\n        \"\"\"Fetches the list of available models from the API.\"\"\"\n        headers = {}\n        if self.api_key:\n            headers[\"Authorization\"] = f\"Bearer {self.api_key}\"\n\n        try:\n            with self.console.status(\"[bold green]Fetching models...[/bold green]\"):\n                response = requests.get(f\"{self.api_base}/models\", headers=headers, timeout=self.timeout)\n            if response.status_code == 200:\n                data = response.json()\n                if \"data\" in data and isinstance(data[\"data\"], list):\n                    models = [model for model in data[\"data\"] if \"id\" in model]\n                    if model_type:\n                        return [m for m in models if model_type in m.get(\"type\", [])]\n                    return models\n                else:\n                    self.console.print(\"[bold red]Error: Unexpected API response format[/bold red]\")\n                    return []\n            else:\n                self.console.print(f\"[bold red]Error: Failed to fetch models with status code {response.status_code}[/bold red]\")\n                return []\n        except requests.exceptions.RequestException as e:\n            self.console.print(f\"[bold red]Error: {e}[/bold red]\")\n            return []\n\n    def stream_chat(self, history: List[Dict[str, str]], model: str, \n                    temperature: float = 0.7, max_tokens: Optional[int] = None) -> Tuple[List[Dict[str, str]], str]:\n        \"\"\"Sends a prompt to the chat API and streams the response.\"\"\"\n        headers = {\"Content-Type\": \"application/json\"}\n        if self.api_key:\n            headers[\"Authorization\"] = f\"Bearer {self.api_key}\"\n\n        data = {\"model\": model, \"messages\": history, \"stream\": True, \"temperature\": temperature}\n        if max_tokens:\n            data[\"max_tokens\"] = max_tokens\n\n        assistant_response = \"\"\n        print()\n        self.console.print(\"[bold white]\u250c\u2500[/bold white][bold cyan]Assistant[/bold cyan][bold white]:[/bold white]\")\n        self.console.print(\"[bold white]\u2502[/bold white] [dim]Thinking...[/dim]\", end=\"\")\n        \n        for attempt in range(self.max_retries):\n            try:\n                with requests.post(f\"{self.api_base}/chat/completions\", headers=headers, json=data, stream=True, timeout=self.timeout) as response:\n                    if response.status_code == 200:\n                        print(\"\\r\" + \" \" * 80 + \"\\r\", end=\"\", flush=True)\n                        self.console.print(\"[bold white]\u2514\u2500>[/bold white] \", end=\"\")\n                        \n                        for chunk in response.iter_lines():\n                            if chunk:\n                                chunk_str = chunk.decode('utf-8')\n                                if chunk_str.startswith(\"data: \"):\n                                    chunk_str = chunk_str[6:]\n                                if chunk_str == \"[DONE]\":\n                                    break\n                                try:\n                                    json_chunk = json.loads(chunk_str)\n                                    if \"choices\" in json_chunk and len(json_chunk[\"choices\"]) > 0:\n                                        content = json_chunk[\"choices\"][0].get(\"delta\", {}).get(\"content\")\n                                        if content:\n                                            assistant_response += content\n                                            print(content, end=\"\", flush=True)\n                                except json.JSONDecodeError:\n                                    pass\n                        print()\n                        break\n                    else:\n                        error_message = f\"API request failed with status code {response.status_code}\"\n                        self.console.print(f\"\\n[bold red]Error: {error_message}[/bold red]\")\n                        if attempt >= self.max_retries - 1:\n                            raise Exception(error_message)\n            except requests.exceptions.RequestException as e:\n                self.console.print(f\"\\n[bold red]Error: {e}[/bold red]\")\n                if attempt >= self.max_retries - 1:\n                    raise\n        \n        history.append({\"role\": \"assistant\", \"content\": assistant_response})\n        return history, assistant_response\n\n    def generate_image(self, prompt: str, model: str = \"dall-e-3\", size: str = \"1024x1024\", quality: str = \"standard\", n: int = 1) -> List[str]:\n        \"\"\"Generates images from text prompt.\"\"\"\n        headers = {\"Content-Type\": \"application/json\"}\n        if self.api_key:\n            headers[\"Authorization\"] = f\"Bearer {self.api_key}\"\n\n        data = {\"model\": model, \"prompt\": prompt, \"n\": n, \"size\": size, \"quality\": quality}\n        \n        try:\n            self.console.print(\"[bold cyan]Generating image...[/bold cyan]\")\n            response = requests.post(f\"{self.api_base}/images/generations\", headers=headers, json=data, timeout=60)\n            if response.status_code == 200:\n                return [img.get(\"url\") or img.get(\"b64_json\") for img in response.json().get(\"data\", [])]\n            else:\n                self.console.print(f\"[bold red]Image generation failed with status {response.status_code}[/bold red]\")\n                return []\n        except requests.exceptions.RequestException as e:\n            self.console.print(f\"[bold red]Error: {e}[/bold red]\")\n            return []\n\n    def generate_video(self, prompt: str, model: str = \"sora-2\") -> Optional[str]:\n        \"\"\"Generates video from text prompt.\"\"\"\n        headers = {\"Content-Type\": \"application/json\"}\n        if self.api_key:\n            headers[\"Authorization\"] = f\"Bearer {self.api_key}\"\n\n        data = {\"model\": model, \"prompt\": prompt}\n        \n        try:\n            self.console.print(\"[bold cyan]Generating video...[/bold cyan]\")\n            response = requests.post(f\"{self.api_base}/videos\", headers=headers, json=data, timeout=300)\n            if response.status_code == 200:\n                result = response.json()\n                return result.get(\"url\") or (result.get(\"data\", [{}])[0].get(\"url\") if result.get(\"data\") else None)\n            else:\n                self.console.print(f\"[bold red]Video generation failed with status {response.status_code}[/bold red]\")\n                return None\n        except requests.exceptions.RequestException as e:\n            self.console.print(f\"[bold red]Error: {e}[/bold red]\")\n            return None\n\n    def text_to_speech(self, text: str, model: str = \"tts-1\", voice: str = \"alloy\", speed: float = 1.0) -> Optional[bytes]:\n        \"\"\"Converts text to speech.\"\"\"\n        headers = {\"Content-Type\": \"application/json\"}\n        if self.api_key:\n            headers[\"Authorization\"] = f\"Bearer {self.api_key}\"\n\n        data = {\"model\": model, \"input\": text, \"voice\": voice, \"speed\": speed}\n        \n        try:\n            self.console.print(\"[bold cyan]Generating speech...[/bold cyan]\")\n            response = requests.post(f\"{self.api_base}/audio/speech\", headers=headers, json=data, timeout=60)\n            if response.status_code == 200:\n                return response.content\n            else:\n                self.console.print(f\"[bold red]TTS failed with status {response.status_code}[/bold red]\")\n                return None\n        except requests.exceptions.RequestException as e:\n            self.console.print(f\"[bold red]Error: {e}[/bold red]\")\n            return None"}, {"filepath": "chat_client.py", "start_line": 12, "end_line": 17, "type": "function", "name": "__init__", "content": "    def __init__(self, api_base: str, api_key: Optional[str], console: Console):\n        self.api_base = api_base\n        self.api_key = api_key\n        self.console = console\n        self.timeout = 30\n        self.max_retries = 3", "docstring": "", "searchable_text": "__init__      def __init__(self, api_base: str, api_key: Optional[str], console: Console):\n        self.api_base = api_base\n        self.api_key = api_key\n        self.console = console\n        self.timeout = 30\n        self.max_retries = 3"}, {"filepath": "chat_client.py", "start_line": 19, "end_line": 43, "type": "function", "name": "list_models", "content": "    def list_models(self, model_type: Optional[str] = None) -> List[Dict[str, any]]:\n        \"\"\"Fetches the list of available models from the API.\"\"\"\n        headers = {}\n        if self.api_key:\n            headers[\"Authorization\"] = f\"Bearer {self.api_key}\"\n\n        try:\n            with self.console.status(\"[bold green]Fetching models...[/bold green]\"):\n                response = requests.get(f\"{self.api_base}/models\", headers=headers, timeout=self.timeout)\n            if response.status_code == 200:\n                data = response.json()\n                if \"data\" in data and isinstance(data[\"data\"], list):\n                    models = [model for model in data[\"data\"] if \"id\" in model]\n                    if model_type:\n                        return [m for m in models if model_type in m.get(\"type\", [])]\n                    return models\n                else:\n                    self.console.print(\"[bold red]Error: Unexpected API response format[/bold red]\")\n                    return []\n            else:\n                self.console.print(f\"[bold red]Error: Failed to fetch models with status code {response.status_code}[/bold red]\")\n                return []\n        except requests.exceptions.RequestException as e:\n            self.console.print(f\"[bold red]Error: {e}[/bold red]\")\n            return []", "docstring": "Fetches the list of available models from the API.", "searchable_text": "list_models Fetches the list of available models from the API.     def list_models(self, model_type: Optional[str] = None) -> List[Dict[str, any]]:\n        \"\"\"Fetches the list of available models from the API.\"\"\"\n        headers = {}\n        if self.api_key:\n            headers[\"Authorization\"] = f\"Bearer {self.api_key}\"\n\n        try:\n            with self.console.status(\"[bold green]Fetching models...[/bold green]\"):\n                response = requests.get(f\"{self.api_base}/models\", headers=headers, timeout=self.timeout)\n            if response.status_code == 200:\n                data = response.json()\n                if \"data\" in data and isinstance(data[\"data\"], list):\n                    models = [model for model in data[\"data\"] if \"id\" in model]\n                    if model_type:\n                        return [m for m in models if model_type in m.get(\"type\", [])]\n                    return models\n                else:\n                    self.console.print(\"[bold red]Error: Unexpected API response format[/bold red]\")\n                    return []\n            else:\n                self.console.print(f\"[bold red]Error: Failed to fetch models with status code {response.status_code}[/bold red]\")\n                return []\n        except requests.exceptions.RequestException as e:\n            self.console.print(f\"[bold red]Error: {e}[/bold red]\")\n            return []"}, {"filepath": "chat_client.py", "start_line": 45, "end_line": 97, "type": "function", "name": "stream_chat", "content": "    def stream_chat(self, history: List[Dict[str, str]], model: str, \n                    temperature: float = 0.7, max_tokens: Optional[int] = None) -> Tuple[List[Dict[str, str]], str]:\n        \"\"\"Sends a prompt to the chat API and streams the response.\"\"\"\n        headers = {\"Content-Type\": \"application/json\"}\n        if self.api_key:\n            headers[\"Authorization\"] = f\"Bearer {self.api_key}\"\n\n        data = {\"model\": model, \"messages\": history, \"stream\": True, \"temperature\": temperature}\n        if max_tokens:\n            data[\"max_tokens\"] = max_tokens\n\n        assistant_response = \"\"\n        print()\n        self.console.print(\"[bold white]\u250c\u2500[/bold white][bold cyan]Assistant[/bold cyan][bold white]:[/bold white]\")\n        self.console.print(\"[bold white]\u2502[/bold white] [dim]Thinking...[/dim]\", end=\"\")\n        \n        for attempt in range(self.max_retries):\n            try:\n                with requests.post(f\"{self.api_base}/chat/completions\", headers=headers, json=data, stream=True, timeout=self.timeout) as response:\n                    if response.status_code == 200:\n                        print(\"\\r\" + \" \" * 80 + \"\\r\", end=\"\", flush=True)\n                        self.console.print(\"[bold white]\u2514\u2500>[/bold white] \", end=\"\")\n                        \n                        for chunk in response.iter_lines():\n                            if chunk:\n                                chunk_str = chunk.decode('utf-8')\n                                if chunk_str.startswith(\"data: \"):\n                                    chunk_str = chunk_str[6:]\n                                if chunk_str == \"[DONE]\":\n                                    break\n                                try:\n                                    json_chunk = json.loads(chunk_str)\n                                    if \"choices\" in json_chunk and len(json_chunk[\"choices\"]) > 0:\n                                        content = json_chunk[\"choices\"][0].get(\"delta\", {}).get(\"content\")\n                                        if content:\n                                            assistant_response += content\n                                            print(content, end=\"\", flush=True)\n                                except json.JSONDecodeError:\n                                    pass\n                        print()\n                        break\n                    else:\n                        error_message = f\"API request failed with status code {response.status_code}\"\n                        self.console.print(f\"\\n[bold red]Error: {error_message}[/bold red]\")\n                        if attempt >= self.max_retries - 1:\n                            raise Exception(error_message)\n            except requests.exceptions.RequestException as e:\n                self.console.print(f\"\\n[bold red]Error: {e}[/bold red]\")\n                if attempt >= self.max_retries - 1:\n                    raise\n        \n        history.append({\"role\": \"assistant\", \"content\": assistant_response})\n        return history, assistant_response", "docstring": "Sends a prompt to the chat API and streams the response.", "searchable_text": "stream_chat Sends a prompt to the chat API and streams the response.     def stream_chat(self, history: List[Dict[str, str]], model: str, \n                    temperature: float = 0.7, max_tokens: Optional[int] = None) -> Tuple[List[Dict[str, str]], str]:\n        \"\"\"Sends a prompt to the chat API and streams the response.\"\"\"\n        headers = {\"Content-Type\": \"application/json\"}\n        if self.api_key:\n            headers[\"Authorization\"] = f\"Bearer {self.api_key}\"\n\n        data = {\"model\": model, \"messages\": history, \"stream\": True, \"temperature\": temperature}\n        if max_tokens:\n            data[\"max_tokens\"] = max_tokens\n\n        assistant_response = \"\"\n        print()\n        self.console.print(\"[bold white]\u250c\u2500[/bold white][bold cyan]Assistant[/bold cyan][bold white]:[/bold white]\")\n        self.console.print(\"[bold white]\u2502[/bold white] [dim]Thinking...[/dim]\", end=\"\")\n        \n        for attempt in range(self.max_retries):\n            try:\n                with requests.post(f\"{self.api_base}/chat/completions\", headers=headers, json=data, stream=True, timeout=self.timeout) as response:\n                    if response.status_code == 200:\n                        print(\"\\r\" + \" \" * 80 + \"\\r\", end=\"\", flush=True)\n                        self.console.print(\"[bold white]\u2514\u2500>[/bold white] \", end=\"\")\n                        \n                        for chunk in response.iter_lines():\n                            if chunk:\n                                chunk_str = chunk.decode('utf-8')\n                                if chunk_str.startswith(\"data: \"):\n                                    chunk_str = chunk_str[6:]\n                                if chunk_str == \"[DONE]\":\n                                    break\n                                try:\n                                    json_chunk = json.loads(chunk_str)\n                                    if \"choices\" in json_chunk and len(json_chunk[\"choices\"]) > 0:\n                                        content = json_chunk[\"choices\"][0].get(\"delta\", {}).get(\"content\")\n                                        if content:\n                                            assistant_response += content\n                                            print(content, end=\"\", flush=True)\n                                except json.JSONDecodeError:\n                                    pass\n                        print()\n                        break\n                    else:\n                        error_message = f\"API request failed with status code {response.status_code}\"\n                        self.console.print(f\"\\n[bold red]Error: {error_message}[/bold red]\")\n                        if attempt >= self.max_retries - 1:\n                            raise Exception(error_message)\n            except requests.exceptions.RequestException as e:\n                self.console.print(f\"\\n[bold red]Error: {e}[/bold red]\")\n                if attempt >= self.max_retries - 1:\n                    raise\n        \n        history.append({\"role\": \"assistant\", \"content\": assistant_response})\n        return history, assistant_response"}, {"filepath": "chat_client.py", "start_line": 99, "end_line": 117, "type": "function", "name": "generate_image", "content": "    def generate_image(self, prompt: str, model: str = \"dall-e-3\", size: str = \"1024x1024\", quality: str = \"standard\", n: int = 1) -> List[str]:\n        \"\"\"Generates images from text prompt.\"\"\"\n        headers = {\"Content-Type\": \"application/json\"}\n        if self.api_key:\n            headers[\"Authorization\"] = f\"Bearer {self.api_key}\"\n\n        data = {\"model\": model, \"prompt\": prompt, \"n\": n, \"size\": size, \"quality\": quality}\n        \n        try:\n            self.console.print(\"[bold cyan]Generating image...[/bold cyan]\")\n            response = requests.post(f\"{self.api_base}/images/generations\", headers=headers, json=data, timeout=60)\n            if response.status_code == 200:\n                return [img.get(\"url\") or img.get(\"b64_json\") for img in response.json().get(\"data\", [])]\n            else:\n                self.console.print(f\"[bold red]Image generation failed with status {response.status_code}[/bold red]\")\n                return []\n        except requests.exceptions.RequestException as e:\n            self.console.print(f\"[bold red]Error: {e}[/bold red]\")\n            return []", "docstring": "Generates images from text prompt.", "searchable_text": "generate_image Generates images from text prompt.     def generate_image(self, prompt: str, model: str = \"dall-e-3\", size: str = \"1024x1024\", quality: str = \"standard\", n: int = 1) -> List[str]:\n        \"\"\"Generates images from text prompt.\"\"\"\n        headers = {\"Content-Type\": \"application/json\"}\n        if self.api_key:\n            headers[\"Authorization\"] = f\"Bearer {self.api_key}\"\n\n        data = {\"model\": model, \"prompt\": prompt, \"n\": n, \"size\": size, \"quality\": quality}\n        \n        try:\n            self.console.print(\"[bold cyan]Generating image...[/bold cyan]\")\n            response = requests.post(f\"{self.api_base}/images/generations\", headers=headers, json=data, timeout=60)\n            if response.status_code == 200:\n                return [img.get(\"url\") or img.get(\"b64_json\") for img in response.json().get(\"data\", [])]\n            else:\n                self.console.print(f\"[bold red]Image generation failed with status {response.status_code}[/bold red]\")\n                return []\n        except requests.exceptions.RequestException as e:\n            self.console.print(f\"[bold red]Error: {e}[/bold red]\")\n            return []"}, {"filepath": "chat_client.py", "start_line": 119, "end_line": 138, "type": "function", "name": "generate_video", "content": "    def generate_video(self, prompt: str, model: str = \"sora-2\") -> Optional[str]:\n        \"\"\"Generates video from text prompt.\"\"\"\n        headers = {\"Content-Type\": \"application/json\"}\n        if self.api_key:\n            headers[\"Authorization\"] = f\"Bearer {self.api_key}\"\n\n        data = {\"model\": model, \"prompt\": prompt}\n        \n        try:\n            self.console.print(\"[bold cyan]Generating video...[/bold cyan]\")\n            response = requests.post(f\"{self.api_base}/videos\", headers=headers, json=data, timeout=300)\n            if response.status_code == 200:\n                result = response.json()\n                return result.get(\"url\") or (result.get(\"data\", [{}])[0].get(\"url\") if result.get(\"data\") else None)\n            else:\n                self.console.print(f\"[bold red]Video generation failed with status {response.status_code}[/bold red]\")\n                return None\n        except requests.exceptions.RequestException as e:\n            self.console.print(f\"[bold red]Error: {e}[/bold red]\")\n            return None", "docstring": "Generates video from text prompt.", "searchable_text": "generate_video Generates video from text prompt.     def generate_video(self, prompt: str, model: str = \"sora-2\") -> Optional[str]:\n        \"\"\"Generates video from text prompt.\"\"\"\n        headers = {\"Content-Type\": \"application/json\"}\n        if self.api_key:\n            headers[\"Authorization\"] = f\"Bearer {self.api_key}\"\n\n        data = {\"model\": model, \"prompt\": prompt}\n        \n        try:\n            self.console.print(\"[bold cyan]Generating video...[/bold cyan]\")\n            response = requests.post(f\"{self.api_base}/videos\", headers=headers, json=data, timeout=300)\n            if response.status_code == 200:\n                result = response.json()\n                return result.get(\"url\") or (result.get(\"data\", [{}])[0].get(\"url\") if result.get(\"data\") else None)\n            else:\n                self.console.print(f\"[bold red]Video generation failed with status {response.status_code}[/bold red]\")\n                return None\n        except requests.exceptions.RequestException as e:\n            self.console.print(f\"[bold red]Error: {e}[/bold red]\")\n            return None"}, {"filepath": "chat_client.py", "start_line": 140, "end_line": 158, "type": "function", "name": "text_to_speech", "content": "    def text_to_speech(self, text: str, model: str = \"tts-1\", voice: str = \"alloy\", speed: float = 1.0) -> Optional[bytes]:\n        \"\"\"Converts text to speech.\"\"\"\n        headers = {\"Content-Type\": \"application/json\"}\n        if self.api_key:\n            headers[\"Authorization\"] = f\"Bearer {self.api_key}\"\n\n        data = {\"model\": model, \"input\": text, \"voice\": voice, \"speed\": speed}\n        \n        try:\n            self.console.print(\"[bold cyan]Generating speech...[/bold cyan]\")\n            response = requests.post(f\"{self.api_base}/audio/speech\", headers=headers, json=data, timeout=60)\n            if response.status_code == 200:\n                return response.content\n            else:\n                self.console.print(f\"[bold red]TTS failed with status {response.status_code}[/bold red]\")\n                return None\n        except requests.exceptions.RequestException as e:\n            self.console.print(f\"[bold red]Error: {e}[/bold red]\")\n            return None", "docstring": "Converts text to speech.", "searchable_text": "text_to_speech Converts text to speech.     def text_to_speech(self, text: str, model: str = \"tts-1\", voice: str = \"alloy\", speed: float = 1.0) -> Optional[bytes]:\n        \"\"\"Converts text to speech.\"\"\"\n        headers = {\"Content-Type\": \"application/json\"}\n        if self.api_key:\n            headers[\"Authorization\"] = f\"Bearer {self.api_key}\"\n\n        data = {\"model\": model, \"input\": text, \"voice\": voice, \"speed\": speed}\n        \n        try:\n            self.console.print(\"[bold cyan]Generating speech...[/bold cyan]\")\n            response = requests.post(f\"{self.api_base}/audio/speech\", headers=headers, json=data, timeout=60)\n            if response.status_code == 200:\n                return response.content\n            else:\n                self.console.print(f\"[bold red]TTS failed with status {response.status_code}[/bold red]\")\n                return None\n        except requests.exceptions.RequestException as e:\n            self.console.print(f\"[bold red]Error: {e}[/bold red]\")\n            return None"}, {"filepath": "cli.py", "start_line": 76, "end_line": 307, "type": "class", "name": "CLI", "content": "class CLI:\n    \"\"\"Handles the command-line interface and chat loop.\"\"\"\n    def __init__(self):\n        self.console = Console()\n        self.config_manager = ConfigManager(self.console)\n        self.chat_client: Optional[ChatClient] = None\n        self.temperature = 0.7\n        self.max_tokens: Optional[int] = None\n        self.yolo_mode = False\n        self.current_project: Optional[str] = None\n        self.code_analyzer: Optional[CodeAnalyzer] = None\n        self.context_engine: Optional['ContextEngine'] = None\n        self.symbol_resolver: Optional['SymbolResolver'] = None\n        self.query_analyzer: Optional['QueryAnalyzer'] = None\n\n    def run(self) -> None:\n        \"\"\"Runs the main CLI logic.\"\"\"\n        parser = argparse.ArgumentParser(description=\"A CLI for OpenAI-compatible APIs.\")\n        subparsers = parser.add_subparsers(dest=\"command\")\n\n        chat_parser = subparsers.add_parser(\"chat\", help=\"Start an interactive chat session.\")\n        chat_parser.add_argument(\"--model\", help=\"The model to use for the chat.\")\n        chat_parser.add_argument(\"--system\", help=\"Set a system prompt for the chat session.\")\n        \n        image_parser = subparsers.add_parser(\"image\", help=\"Generate images from text prompts.\")\n        image_parser.add_argument(\"prompt\", nargs=\"?\", help=\"Text description of the image to generate\")\n\n        # ... (other parsers for video, tts, models)\n\n        args = parser.parse_args()\n\n        if args.command == \"chat\":\n            self.config_manager.load_config()\n            self.chat_client = ChatClient(self.config_manager.api_base, self.config_manager.api_key, self.console)\n            self.start_chat_session(args.model, args.system)\n        elif args.command == \"image\":\n            self.config_manager.load_config()\n            self.chat_client = ChatClient(self.config_manager.api_base, self.config_manager.api_key, self.console)\n            self.handle_image_generation(args)\n        else:\n            parser.print_help()\n\n    def start_chat_session(self, model: Optional[str], system_prompt: Optional[str]) -> None:\n        \"\"\"Starts and manages the interactive chat session.\"\"\"\n        if not model:\n            # Get available models and let user choose\n            models = self.chat_client.list_models()\n            if not models:\n                self.console.print(\"[bold red]Error: Could not fetch models. Using default 'gpt-3.5-turbo'[/bold red]\")\n                model = \"gpt-3.5-turbo\"\n            elif len(models) == 1:\n                model = models[0][\"id\"]\n                self.console.print(f\"[cyan]Using model: {model}[/cyan]\")\n            else:\n                self.console.print(\"\\n[bold magenta]\u2554\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2557[/bold magenta]\")\n                self.console.print(\"[bold magenta]\u2551[/bold magenta]  [bold white on magenta] \ud83e\udd16 Select Model [/bold white on magenta]                                       [bold magenta]\u2551[/bold magenta]\")\n                self.console.print(\"[bold magenta]\u255a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u255d[/bold magenta]\\n\")\n                \n                # Display models in a nice grid\n                for i, m in enumerate(models, 1):\n                    model_id = m['id']\n                    # Add emoji indicators for popular models\n                    if 'gpt-4o' in model_id.lower():\n                        icon = '\u2b50'\n                    elif 'claude' in model_id.lower():\n                        icon = '\ud83c\udfad'\n                    elif 'gemini' in model_id.lower():\n                        icon = '\ud83d\udc8e'\n                    elif 'deepseek' in model_id.lower():\n                        icon = '\ud83e\udde0'\n                    else:\n                        icon = '\ud83e\udd16'\n                    \n                    self.console.print(f\"  [dim]{i:3d}.[/dim] {icon} [cyan]{model_id}[/cyan]\")\n                \n                # Try to find a good default\n                default_model = next((m['id'] for m in models if 'gpt-4o-mini' in m['id'].lower()), \n                                   next((m['id'] for m in models if 'gpt-4o' in m['id'].lower()), models[0]['id']))\n                \n                self.console.print(f\"\\n[dim]\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501[/dim]\")\n                try:\n                    choice = input(f\"[bold cyan]\u27a4[/bold cyan] Select model [dim](number/name, or Enter for[/dim] [yellow]{default_model}[/yellow][dim])[/dim]: \").strip()\n                    if not choice:\n                        model = default_model\n                    elif choice.isdigit():\n                        idx = int(choice) - 1\n                        if 0 <= idx < len(models):\n                            model = models[idx][\"id\"]\n                        else:\n                            self.console.print(\"[yellow]Invalid choice. Using default.[/yellow]\")\n                            model = default_model\n                    else:\n                        model = choice\n                except (EOFError, KeyboardInterrupt):\n                    self.console.print(f\"\\n[yellow]\u26a0 Using default: {default_model}[/yellow]\")\n                    model = default_model\n        \n        self.console.print(f\"\\n[bold green]\u2713[/bold green] [dim]Starting chat with[/dim] [bold cyan]{model}[/bold cyan]\")\n\n        history: List[Dict[str, str]] = []\n        if system_prompt:\n            history.append({\"role\": \"system\", \"content\": system_prompt})\n\n        self.console.print(WELCOME_MSG)\n        \n        while True:\n            try:\n                prompt = self.get_user_input()\n                if not prompt: continue\n\n                if prompt.lower().startswith(':'):\n                    if not self.handle_command(prompt, history, model):\n                        break\n                    continue\n                \n                enhanced_prompt = self.enhance_prompt_with_context(prompt)\n                history.append({\"role\": \"user\", \"content\": enhanced_prompt})\n                \n                history, _ = self.chat_client.stream_chat(history, model, self.temperature, self.max_tokens)\n\n            except (KeyboardInterrupt, EOFError):\n                break\n        \n        self.console.print(\"\\n[bold]Chat session ended.[/bold]\")\n\n    def enhance_prompt_with_context(self, prompt: str) -> str:\n        \"\"\"Enhances the user prompt with relevant code context if in YOLO mode.\"\"\"\n        if not self.yolo_mode or not self.code_analyzer:\n            return prompt\n\n        if self.query_analyzer and self.symbol_resolver and self.context_engine:\n            analysis = self.query_analyzer.analyze(prompt)\n            \n            if self.query_analyzer.should_use_symbol_search(analysis) and analysis['entities']:\n                symbol_name = analysis['entities'][0]\n                definitions = self.symbol_resolver.find_definition(symbol_name)\n                if definitions:\n                    context = \"\\n\".join([f\"[{d['file']}:{d['line']}]\\n{self.code_analyzer.read_file_content(d['file'])}\" for d in definitions[:3]])\n                    return f\"{prompt}\\n\\n[DEFINITIONS FOUND]:\\n{context}\"\n            \n            elif self.query_analyzer.should_use_semantic_search(analysis):\n                relevant_chunks = self.context_engine.search(prompt, top_k=5)\n                if relevant_chunks:\n                    context = \"\\n\".join([f\"[{c['filepath']}:{c['start_line']}]\\n{c['content']}\" for c in relevant_chunks])\n                    return f\"{prompt}\\n\\n[RELEVANT CODE]:\\n{context}\"\n        \n        return prompt\n\n    def get_user_input(self) -> str:\n        \"\"\"Gets user input with support for multiline.\"\"\"\n        try:\n            self.console.print(\"\\n[bold white]\u250c\u2500[/bold white] [bold cyan]You[/bold cyan]\")\n            self.console.print(\"[bold white]\u2514\u2500\u27a4[/bold white] \", end=\"\")\n            user_input = input().strip()\n            \n            # Check for multiline input (ending with \\)\n            while user_input.endswith('\\\\'):\n                user_input = user_input[:-1] + '\\n'\n                self.console.print(\"   [bold white]\u2502[/bold white] \", end=\"\")\n                user_input += input().strip()\n            \n            return user_input\n        except (EOFError, KeyboardInterrupt):\n            return \":exit\"\n\n    def handle_command(self, prompt: str, history: List[Dict[str, str]], model: str) -> bool:\n        \"\"\"Handles special commands.\"\"\"\n        command, *args = prompt[1:].strip().split()\n        \n        if command in [\"exit\", \"q\"]:\n            return False\n        elif command == \"yolo\":\n            self.yolo_mode = not self.yolo_mode\n            self.console.print(f\"[green]YOLO mode {'ENABLED' if self.yolo_mode else 'DISABLED'}[/green]\")\n            if self.yolo_mode and not self.current_project:\n                self.current_project = os.getcwd()\n        elif command == \"analyze\":\n            if self.yolo_mode:\n                self.code_analyzer = CodeAnalyzer(self.current_project, self.console)\n                self.code_analyzer.scan_project()\n                if CONTEXT_ENGINE_AVAILABLE:\n                    self.context_engine = ContextEngine(self.current_project, self.console)\n                    self.context_engine.build_index(self.code_analyzer)\n                if SYMBOL_RESOLVER_AVAILABLE:\n                    self.symbol_resolver = SymbolResolver(self.current_project)\n                    # Simplified analysis loop\n                    for f in self.code_analyzer.files[:100]:\n                        content = self.code_analyzer.read_file_content(f)\n                        if content: self.symbol_resolver.analyze_file(f, content)\n                if QUERY_ANALYZER_AVAILABLE:\n                    self.query_analyzer = QueryAnalyzer()\n                self.console.print(\"[green]Intelligent analysis complete![/green]\")\n            else:\n                self.console.print(\"[yellow]Enable YOLO mode first with :yolo[/yellow]\")\n        elif command == \"clear\":\n            history.clear()\n            self.console.print(\"[green]Chat history cleared.[/green]\")\n        elif command == \"temp\":\n            if args:\n                try:\n                    self.temperature = float(args[0])\n                    self.console.print(f\"[green]Temperature set to {self.temperature}[/green]\")\n                except ValueError:\n                    self.console.print(\"[red]Invalid temperature value. Use a number between 0.0 and 2.0[/red]\")\n            else:\n                self.console.print(f\"[cyan]Current temperature: {self.temperature}[/cyan]\")\n        elif command == \"tokens\":\n            if args:\n                try:\n                    self.max_tokens = int(args[0])\n                    self.console.print(f\"[green]Max tokens set to {self.max_tokens}[/green]\")\n                except ValueError:\n                    self.console.print(\"[red]Invalid tokens value. Use an integer.[/red]\")\n            else:\n                self.console.print(f\"[cyan]Current max tokens: {self.max_tokens or 'unlimited'}[/cyan]\")\n        elif command == \"system\":\n            if args:\n                system_prompt = \" \".join(args)\n                history.insert(0, {\"role\": \"system\", \"content\": system_prompt})\n                self.console.print(f\"[green]System prompt set.[/green]\")\n            else:\n                self.console.print(\"[red]Please provide a system prompt.[/red]\")\n        elif command == \"help\":\n            self.console.print(HELP_MSG)\n        else:\n            self.console.print(f\"[red]Unknown command: {command}[/red]\")\n            self.console.print(\"[dim]Type :help for available commands.[/dim]\")\n        return True\n\n    def handle_image_generation(self, args):\n        # ... (implementation for image generation)\n        pass", "docstring": "Handles the command-line interface and chat loop.", "searchable_text": "CLI Handles the command-line interface and chat loop. class CLI:\n    \"\"\"Handles the command-line interface and chat loop.\"\"\"\n    def __init__(self):\n        self.console = Console()\n        self.config_manager = ConfigManager(self.console)\n        self.chat_client: Optional[ChatClient] = None\n        self.temperature = 0.7\n        self.max_tokens: Optional[int] = None\n        self.yolo_mode = False\n        self.current_project: Optional[str] = None\n        self.code_analyzer: Optional[CodeAnalyzer] = None\n        self.context_engine: Optional['ContextEngine'] = None\n        self.symbol_resolver: Optional['SymbolResolver'] = None\n        self.query_analyzer: Optional['QueryAnalyzer'] = None\n\n    def run(self) -> None:\n        \"\"\"Runs the main CLI logic.\"\"\"\n        parser = argparse.ArgumentParser(description=\"A CLI for OpenAI-compatible APIs.\")\n        subparsers = parser.add_subparsers(dest=\"command\")\n\n        chat_parser = subparsers.add_parser(\"chat\", help=\"Start an interactive chat session.\")\n        chat_parser.add_argument(\"--model\", help=\"The model to use for the chat.\")\n        chat_parser.add_argument(\"--system\", help=\"Set a system prompt for the chat session.\")\n        \n        image_parser = subparsers.add_parser(\"image\", help=\"Generate images from text prompts.\")\n        image_parser.add_argument(\"prompt\", nargs=\"?\", help=\"Text description of the image to generate\")\n\n        # ... (other parsers for video, tts, models)\n\n        args = parser.parse_args()\n\n        if args.command == \"chat\":\n            self.config_manager.load_config()\n            self.chat_client = ChatClient(self.config_manager.api_base, self.config_manager.api_key, self.console)\n            self.start_chat_session(args.model, args.system)\n        elif args.command == \"image\":\n            self.config_manager.load_config()\n            self.chat_client = ChatClient(self.config_manager.api_base, self.config_manager.api_key, self.console)\n            self.handle_image_generation(args)\n        else:\n            parser.print_help()\n\n    def start_chat_session(self, model: Optional[str], system_prompt: Optional[str]) -> None:\n        \"\"\"Starts and manages the interactive chat session.\"\"\"\n        if not model:\n            # Get available models and let user choose\n            models = self.chat_client.list_models()\n            if not models:\n                self.console.print(\"[bold red]Error: Could not fetch models. Using default 'gpt-3.5-turbo'[/bold red]\")\n                model = \"gpt-3.5-turbo\"\n            elif len(models) == 1:\n                model = models[0][\"id\"]\n                self.console.print(f\"[cyan]Using model: {model}[/cyan]\")\n            else:\n                self.console.print(\"\\n[bold magenta]\u2554\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2557[/bold magenta]\")\n                self.console.print(\"[bold magenta]\u2551[/bold magenta]  [bold white on magenta] \ud83e\udd16 Select Model [/bold white on magenta]                                       [bold magenta]\u2551[/bold magenta]\")\n                self.console.print(\"[bold magenta]\u255a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u255d[/bold magenta]\\n\")\n                \n                # Display models in a nice grid\n                for i, m in enumerate(models, 1):\n                    model_id = m['id']\n                    # Add emoji indicators for popular models\n                    if 'gpt-4o' in model_id.lower():\n                        icon = '\u2b50'\n                    elif 'claude' in model_id.lower():\n                        icon = '\ud83c\udfad'\n                    elif 'gemini' in model_id.lower():\n                        icon = '\ud83d\udc8e'\n                    elif 'deepseek' in model_id.lower():\n                        icon = '\ud83e\udde0'\n                    else:\n                        icon = '\ud83e\udd16'\n                    \n                    self.console.print(f\"  [dim]{i:3d}.[/dim] {icon} [cyan]{model_id}[/cyan]\")\n                \n                # Try to find a good default\n                default_model = next((m['id'] for m in models if 'gpt-4o-mini' in m['id'].lower()), \n                                   next((m['id'] for m in models if 'gpt-4o' in m['id'].lower()), models[0]['id']))\n                \n                self.console.print(f\"\\n[dim]\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501[/dim]\")\n                try:\n                    choice = input(f\"[bold cyan]\u27a4[/bold cyan] Select model [dim](number/name, or Enter for[/dim] [yellow]{default_model}[/yellow][dim])[/dim]: \").strip()\n                    if not choice:\n                        model = default_model\n                    elif choice.isdigit():\n                        idx = int(choice) - 1\n                        if 0 <= idx < len(models):\n                            model = models[idx][\"id\"]\n                        else:\n                            self.console.print(\"[yellow]Invalid choice. Using default.[/yellow]\")\n                            model = default_model\n                    else:\n                        model = choice\n                except (EOFError, KeyboardInterrupt):\n                    self.console.print(f\"\\n[yellow]\u26a0 Using default: {default_model}[/yellow]\")\n                    model = default_model\n        \n        self.console.print(f\"\\n[bold green]\u2713[/bold green] [dim]Starting chat with[/dim] [bold cyan]{model}[/bold cyan]\")\n\n        history: List[Dict[str, str]] = []\n        if system_prompt:\n            history.append({\"role\": \"system\", \"content\": system_prompt})\n\n        self.console.print(WELCOME_MSG)\n        \n        while True:\n            try:\n                prompt = self.get_user_input()\n                if not prompt: continue\n\n                if prompt.lower().startswith(':'):\n                    if not self.handle_command(prompt, history, model):\n                        break\n                    continue\n                \n                enhanced_prompt = self.enhance_prompt_with_context(prompt)\n                history.append({\"role\": \"user\", \"content\": enhanced_prompt})\n                \n                history, _ = self.chat_client.stream_chat(history, model, self.temperature, self.max_tokens)\n\n            except (KeyboardInterrupt, EOFError):\n                break\n        \n        self.console.print(\"\\n[bold]Chat session ended.[/bold]\")\n\n    def enhance_prompt_with_context(self, prompt: str) -> str:\n        \"\"\"Enhances the user prompt with relevant code context if in YOLO mode.\"\"\"\n        if not self.yolo_mode or not self.code_analyzer:\n            return prompt\n\n        if self.query_analyzer and self.symbol_resolver and self.context_engine:\n            analysis = self.query_analyzer.analyze(prompt)\n            \n            if self.query_analyzer.should_use_symbol_search(analysis) and analysis['entities']:\n                symbol_name = analysis['entities'][0]\n                definitions = self.symbol_resolver.find_definition(symbol_name)\n                if definitions:\n                    context = \"\\n\".join([f\"[{d['file']}:{d['line']}]\\n{self.code_analyzer.read_file_content(d['file'])}\" for d in definitions[:3]])\n                    return f\"{prompt}\\n\\n[DEFINITIONS FOUND]:\\n{context}\"\n            \n            elif self.query_analyzer.should_use_semantic_search(analysis):\n                relevant_chunks = self.context_engine.search(prompt, top_k=5)\n                if relevant_chunks:\n                    context = \"\\n\".join([f\"[{c['filepath']}:{c['start_line']}]\\n{c['content']}\" for c in relevant_chunks])\n                    return f\"{prompt}\\n\\n[RELEVANT CODE]:\\n{context}\"\n        \n        return prompt\n\n    def get_user_input(self) -> str:\n        \"\"\"Gets user input with support for multiline.\"\"\"\n        try:\n            self.console.print(\"\\n[bold white]\u250c\u2500[/bold white] [bold cyan]You[/bold cyan]\")\n            self.console.print(\"[bold white]\u2514\u2500\u27a4[/bold white] \", end=\"\")\n            user_input = input().strip()\n            \n            # Check for multiline input (ending with \\)\n            while user_input.endswith('\\\\'):\n                user_input = user_input[:-1] + '\\n'\n                self.console.print(\"   [bold white]\u2502[/bold white] \", end=\"\")\n                user_input += input().strip()\n            \n            return user_input\n        except (EOFError, KeyboardInterrupt):\n            return \":exit\"\n\n    def handle_command(self, prompt: str, history: List[Dict[str, str]], model: str) -> bool:\n        \"\"\"Handles special commands.\"\"\"\n        command, *args = prompt[1:].strip().split()\n        \n        if command in [\"exit\", \"q\"]:\n            return False\n        elif command == \"yolo\":\n            self.yolo_mode = not self.yolo_mode\n            self.console.print(f\"[green]YOLO mode {'ENABLED' if self.yolo_mode else 'DISABLED'}[/green]\")\n            if self.yolo_mode and not self.current_project:\n                self.current_project = os.getcwd()\n        elif command == \"analyze\":\n            if self.yolo_mode:\n                self.code_analyzer = CodeAnalyzer(self.current_project, self.console)\n                self.code_analyzer.scan_project()\n                if CONTEXT_ENGINE_AVAILABLE:\n                    self.context_engine = ContextEngine(self.current_project, self.console)\n                    self.context_engine.build_index(self.code_analyzer)\n                if SYMBOL_RESOLVER_AVAILABLE:\n                    self.symbol_resolver = SymbolResolver(self.current_project)\n                    # Simplified analysis loop\n                    for f in self.code_analyzer.files[:100]:\n                        content = self.code_analyzer.read_file_content(f)\n                        if content: self.symbol_resolver.analyze_file(f, content)\n                if QUERY_ANALYZER_AVAILABLE:\n                    self.query_analyzer = QueryAnalyzer()\n                self.console.print(\"[green]Intelligent analysis complete![/green]\")\n            else:\n                self.console.print(\"[yellow]Enable YOLO mode first with :yolo[/yellow]\")\n        elif command == \"clear\":\n            history.clear()\n            self.console.print(\"[green]Chat history cleared.[/green]\")\n        elif command == \"temp\":\n            if args:\n                try:\n                    self.temperature = float(args[0])\n                    self.console.print(f\"[green]Temperature set to {self.temperature}[/green]\")\n                except ValueError:\n                    self.console.print(\"[red]Invalid temperature value. Use a number between 0.0 and 2.0[/red]\")\n            else:\n                self.console.print(f\"[cyan]Current temperature: {self.temperature}[/cyan]\")\n        elif command == \"tokens\":\n            if args:\n                try:\n                    self.max_tokens = int(args[0])\n                    self.console.print(f\"[green]Max tokens set to {self.max_tokens}[/green]\")\n                except ValueError:\n                    self.console.print(\"[red]Invalid tokens value. Use an integer.[/red]\")\n            else:\n                self.console.print(f\"[cyan]Current max tokens: {self.max_tokens or 'unlimited'}[/cyan]\")\n        elif command == \"system\":\n            if args:\n                system_prompt = \" \".join(args)\n                history.insert(0, {\"role\": \"system\", \"content\": system_prompt})\n                self.console.print(f\"[green]System prompt set.[/green]\")\n            else:\n                self.console.print(\"[red]Please provide a system prompt.[/red]\")\n        elif command == \"help\":\n            self.console.print(HELP_MSG)\n        else:\n            self.console.print(f\"[red]Unknown command: {command}[/red]\")\n            self.console.print(\"[dim]Type :help for available commands.[/dim]\")\n        return True\n\n    def handle_image_generation(self, args):\n        # ... (implementation for image generation)\n        pass"}, {"filepath": "cli.py", "start_line": 78, "end_line": 89, "type": "function", "name": "__init__", "content": "    def __init__(self):\n        self.console = Console()\n        self.config_manager = ConfigManager(self.console)\n        self.chat_client: Optional[ChatClient] = None\n        self.temperature = 0.7\n        self.max_tokens: Optional[int] = None\n        self.yolo_mode = False\n        self.current_project: Optional[str] = None\n        self.code_analyzer: Optional[CodeAnalyzer] = None\n        self.context_engine: Optional['ContextEngine'] = None\n        self.symbol_resolver: Optional['SymbolResolver'] = None\n        self.query_analyzer: Optional['QueryAnalyzer'] = None", "docstring": "", "searchable_text": "__init__      def __init__(self):\n        self.console = Console()\n        self.config_manager = ConfigManager(self.console)\n        self.chat_client: Optional[ChatClient] = None\n        self.temperature = 0.7\n        self.max_tokens: Optional[int] = None\n        self.yolo_mode = False\n        self.current_project: Optional[str] = None\n        self.code_analyzer: Optional[CodeAnalyzer] = None\n        self.context_engine: Optional['ContextEngine'] = None\n        self.symbol_resolver: Optional['SymbolResolver'] = None\n        self.query_analyzer: Optional['QueryAnalyzer'] = None"}, {"filepath": "cli.py", "start_line": 91, "end_line": 116, "type": "function", "name": "run", "content": "    def run(self) -> None:\n        \"\"\"Runs the main CLI logic.\"\"\"\n        parser = argparse.ArgumentParser(description=\"A CLI for OpenAI-compatible APIs.\")\n        subparsers = parser.add_subparsers(dest=\"command\")\n\n        chat_parser = subparsers.add_parser(\"chat\", help=\"Start an interactive chat session.\")\n        chat_parser.add_argument(\"--model\", help=\"The model to use for the chat.\")\n        chat_parser.add_argument(\"--system\", help=\"Set a system prompt for the chat session.\")\n        \n        image_parser = subparsers.add_parser(\"image\", help=\"Generate images from text prompts.\")\n        image_parser.add_argument(\"prompt\", nargs=\"?\", help=\"Text description of the image to generate\")\n\n        # ... (other parsers for video, tts, models)\n\n        args = parser.parse_args()\n\n        if args.command == \"chat\":\n            self.config_manager.load_config()\n            self.chat_client = ChatClient(self.config_manager.api_base, self.config_manager.api_key, self.console)\n            self.start_chat_session(args.model, args.system)\n        elif args.command == \"image\":\n            self.config_manager.load_config()\n            self.chat_client = ChatClient(self.config_manager.api_base, self.config_manager.api_key, self.console)\n            self.handle_image_generation(args)\n        else:\n            parser.print_help()", "docstring": "Runs the main CLI logic.", "searchable_text": "run Runs the main CLI logic.     def run(self) -> None:\n        \"\"\"Runs the main CLI logic.\"\"\"\n        parser = argparse.ArgumentParser(description=\"A CLI for OpenAI-compatible APIs.\")\n        subparsers = parser.add_subparsers(dest=\"command\")\n\n        chat_parser = subparsers.add_parser(\"chat\", help=\"Start an interactive chat session.\")\n        chat_parser.add_argument(\"--model\", help=\"The model to use for the chat.\")\n        chat_parser.add_argument(\"--system\", help=\"Set a system prompt for the chat session.\")\n        \n        image_parser = subparsers.add_parser(\"image\", help=\"Generate images from text prompts.\")\n        image_parser.add_argument(\"prompt\", nargs=\"?\", help=\"Text description of the image to generate\")\n\n        # ... (other parsers for video, tts, models)\n\n        args = parser.parse_args()\n\n        if args.command == \"chat\":\n            self.config_manager.load_config()\n            self.chat_client = ChatClient(self.config_manager.api_base, self.config_manager.api_key, self.console)\n            self.start_chat_session(args.model, args.system)\n        elif args.command == \"image\":\n            self.config_manager.load_config()\n            self.chat_client = ChatClient(self.config_manager.api_base, self.config_manager.api_key, self.console)\n            self.handle_image_generation(args)\n        else:\n            parser.print_help()"}, {"filepath": "cli.py", "start_line": 118, "end_line": 199, "type": "function", "name": "start_chat_session", "content": "    def start_chat_session(self, model: Optional[str], system_prompt: Optional[str]) -> None:\n        \"\"\"Starts and manages the interactive chat session.\"\"\"\n        if not model:\n            # Get available models and let user choose\n            models = self.chat_client.list_models()\n            if not models:\n                self.console.print(\"[bold red]Error: Could not fetch models. Using default 'gpt-3.5-turbo'[/bold red]\")\n                model = \"gpt-3.5-turbo\"\n            elif len(models) == 1:\n                model = models[0][\"id\"]\n                self.console.print(f\"[cyan]Using model: {model}[/cyan]\")\n            else:\n                self.console.print(\"\\n[bold magenta]\u2554\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2557[/bold magenta]\")\n                self.console.print(\"[bold magenta]\u2551[/bold magenta]  [bold white on magenta] \ud83e\udd16 Select Model [/bold white on magenta]                                       [bold magenta]\u2551[/bold magenta]\")\n                self.console.print(\"[bold magenta]\u255a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u255d[/bold magenta]\\n\")\n                \n                # Display models in a nice grid\n                for i, m in enumerate(models, 1):\n                    model_id = m['id']\n                    # Add emoji indicators for popular models\n                    if 'gpt-4o' in model_id.lower():\n                        icon = '\u2b50'\n                    elif 'claude' in model_id.lower():\n                        icon = '\ud83c\udfad'\n                    elif 'gemini' in model_id.lower():\n                        icon = '\ud83d\udc8e'\n                    elif 'deepseek' in model_id.lower():\n                        icon = '\ud83e\udde0'\n                    else:\n                        icon = '\ud83e\udd16'\n                    \n                    self.console.print(f\"  [dim]{i:3d}.[/dim] {icon} [cyan]{model_id}[/cyan]\")\n                \n                # Try to find a good default\n                default_model = next((m['id'] for m in models if 'gpt-4o-mini' in m['id'].lower()), \n                                   next((m['id'] for m in models if 'gpt-4o' in m['id'].lower()), models[0]['id']))\n                \n                self.console.print(f\"\\n[dim]\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501[/dim]\")\n                try:\n                    choice = input(f\"[bold cyan]\u27a4[/bold cyan] Select model [dim](number/name, or Enter for[/dim] [yellow]{default_model}[/yellow][dim])[/dim]: \").strip()\n                    if not choice:\n                        model = default_model\n                    elif choice.isdigit():\n                        idx = int(choice) - 1\n                        if 0 <= idx < len(models):\n                            model = models[idx][\"id\"]\n                        else:\n                            self.console.print(\"[yellow]Invalid choice. Using default.[/yellow]\")\n                            model = default_model\n                    else:\n                        model = choice\n                except (EOFError, KeyboardInterrupt):\n                    self.console.print(f\"\\n[yellow]\u26a0 Using default: {default_model}[/yellow]\")\n                    model = default_model\n        \n        self.console.print(f\"\\n[bold green]\u2713[/bold green] [dim]Starting chat with[/dim] [bold cyan]{model}[/bold cyan]\")\n\n        history: List[Dict[str, str]] = []\n        if system_prompt:\n            history.append({\"role\": \"system\", \"content\": system_prompt})\n\n        self.console.print(WELCOME_MSG)\n        \n        while True:\n            try:\n                prompt = self.get_user_input()\n                if not prompt: continue\n\n                if prompt.lower().startswith(':'):\n                    if not self.handle_command(prompt, history, model):\n                        break\n                    continue\n                \n                enhanced_prompt = self.enhance_prompt_with_context(prompt)\n                history.append({\"role\": \"user\", \"content\": enhanced_prompt})\n                \n                history, _ = self.chat_client.stream_chat(history, model, self.temperature, self.max_tokens)\n\n            except (KeyboardInterrupt, EOFError):\n                break\n        \n        self.console.print(\"\\n[bold]Chat session ended.[/bold]\")", "docstring": "Starts and manages the interactive chat session.", "searchable_text": "start_chat_session Starts and manages the interactive chat session.     def start_chat_session(self, model: Optional[str], system_prompt: Optional[str]) -> None:\n        \"\"\"Starts and manages the interactive chat session.\"\"\"\n        if not model:\n            # Get available models and let user choose\n            models = self.chat_client.list_models()\n            if not models:\n                self.console.print(\"[bold red]Error: Could not fetch models. Using default 'gpt-3.5-turbo'[/bold red]\")\n                model = \"gpt-3.5-turbo\"\n            elif len(models) == 1:\n                model = models[0][\"id\"]\n                self.console.print(f\"[cyan]Using model: {model}[/cyan]\")\n            else:\n                self.console.print(\"\\n[bold magenta]\u2554\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2557[/bold magenta]\")\n                self.console.print(\"[bold magenta]\u2551[/bold magenta]  [bold white on magenta] \ud83e\udd16 Select Model [/bold white on magenta]                                       [bold magenta]\u2551[/bold magenta]\")\n                self.console.print(\"[bold magenta]\u255a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u255d[/bold magenta]\\n\")\n                \n                # Display models in a nice grid\n                for i, m in enumerate(models, 1):\n                    model_id = m['id']\n                    # Add emoji indicators for popular models\n                    if 'gpt-4o' in model_id.lower():\n                        icon = '\u2b50'\n                    elif 'claude' in model_id.lower():\n                        icon = '\ud83c\udfad'\n                    elif 'gemini' in model_id.lower():\n                        icon = '\ud83d\udc8e'\n                    elif 'deepseek' in model_id.lower():\n                        icon = '\ud83e\udde0'\n                    else:\n                        icon = '\ud83e\udd16'\n                    \n                    self.console.print(f\"  [dim]{i:3d}.[/dim] {icon} [cyan]{model_id}[/cyan]\")\n                \n                # Try to find a good default\n                default_model = next((m['id'] for m in models if 'gpt-4o-mini' in m['id'].lower()), \n                                   next((m['id'] for m in models if 'gpt-4o' in m['id'].lower()), models[0]['id']))\n                \n                self.console.print(f\"\\n[dim]\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501[/dim]\")\n                try:\n                    choice = input(f\"[bold cyan]\u27a4[/bold cyan] Select model [dim](number/name, or Enter for[/dim] [yellow]{default_model}[/yellow][dim])[/dim]: \").strip()\n                    if not choice:\n                        model = default_model\n                    elif choice.isdigit():\n                        idx = int(choice) - 1\n                        if 0 <= idx < len(models):\n                            model = models[idx][\"id\"]\n                        else:\n                            self.console.print(\"[yellow]Invalid choice. Using default.[/yellow]\")\n                            model = default_model\n                    else:\n                        model = choice\n                except (EOFError, KeyboardInterrupt):\n                    self.console.print(f\"\\n[yellow]\u26a0 Using default: {default_model}[/yellow]\")\n                    model = default_model\n        \n        self.console.print(f\"\\n[bold green]\u2713[/bold green] [dim]Starting chat with[/dim] [bold cyan]{model}[/bold cyan]\")\n\n        history: List[Dict[str, str]] = []\n        if system_prompt:\n            history.append({\"role\": \"system\", \"content\": system_prompt})\n\n        self.console.print(WELCOME_MSG)\n        \n        while True:\n            try:\n                prompt = self.get_user_input()\n                if not prompt: continue\n\n                if prompt.lower().startswith(':'):\n                    if not self.handle_command(prompt, history, model):\n                        break\n                    continue\n                \n                enhanced_prompt = self.enhance_prompt_with_context(prompt)\n                history.append({\"role\": \"user\", \"content\": enhanced_prompt})\n                \n                history, _ = self.chat_client.stream_chat(history, model, self.temperature, self.max_tokens)\n\n            except (KeyboardInterrupt, EOFError):\n                break\n        \n        self.console.print(\"\\n[bold]Chat session ended.[/bold]\")"}, {"filepath": "cli.py", "start_line": 201, "end_line": 222, "type": "function", "name": "enhance_prompt_with_context", "content": "    def enhance_prompt_with_context(self, prompt: str) -> str:\n        \"\"\"Enhances the user prompt with relevant code context if in YOLO mode.\"\"\"\n        if not self.yolo_mode or not self.code_analyzer:\n            return prompt\n\n        if self.query_analyzer and self.symbol_resolver and self.context_engine:\n            analysis = self.query_analyzer.analyze(prompt)\n            \n            if self.query_analyzer.should_use_symbol_search(analysis) and analysis['entities']:\n                symbol_name = analysis['entities'][0]\n                definitions = self.symbol_resolver.find_definition(symbol_name)\n                if definitions:\n                    context = \"\\n\".join([f\"[{d['file']}:{d['line']}]\\n{self.code_analyzer.read_file_content(d['file'])}\" for d in definitions[:3]])\n                    return f\"{prompt}\\n\\n[DEFINITIONS FOUND]:\\n{context}\"\n            \n            elif self.query_analyzer.should_use_semantic_search(analysis):\n                relevant_chunks = self.context_engine.search(prompt, top_k=5)\n                if relevant_chunks:\n                    context = \"\\n\".join([f\"[{c['filepath']}:{c['start_line']}]\\n{c['content']}\" for c in relevant_chunks])\n                    return f\"{prompt}\\n\\n[RELEVANT CODE]:\\n{context}\"\n        \n        return prompt", "docstring": "Enhances the user prompt with relevant code context if in YOLO mode.", "searchable_text": "enhance_prompt_with_context Enhances the user prompt with relevant code context if in YOLO mode.     def enhance_prompt_with_context(self, prompt: str) -> str:\n        \"\"\"Enhances the user prompt with relevant code context if in YOLO mode.\"\"\"\n        if not self.yolo_mode or not self.code_analyzer:\n            return prompt\n\n        if self.query_analyzer and self.symbol_resolver and self.context_engine:\n            analysis = self.query_analyzer.analyze(prompt)\n            \n            if self.query_analyzer.should_use_symbol_search(analysis) and analysis['entities']:\n                symbol_name = analysis['entities'][0]\n                definitions = self.symbol_resolver.find_definition(symbol_name)\n                if definitions:\n                    context = \"\\n\".join([f\"[{d['file']}:{d['line']}]\\n{self.code_analyzer.read_file_content(d['file'])}\" for d in definitions[:3]])\n                    return f\"{prompt}\\n\\n[DEFINITIONS FOUND]:\\n{context}\"\n            \n            elif self.query_analyzer.should_use_semantic_search(analysis):\n                relevant_chunks = self.context_engine.search(prompt, top_k=5)\n                if relevant_chunks:\n                    context = \"\\n\".join([f\"[{c['filepath']}:{c['start_line']}]\\n{c['content']}\" for c in relevant_chunks])\n                    return f\"{prompt}\\n\\n[RELEVANT CODE]:\\n{context}\"\n        \n        return prompt"}, {"filepath": "cli.py", "start_line": 224, "end_line": 239, "type": "function", "name": "get_user_input", "content": "    def get_user_input(self) -> str:\n        \"\"\"Gets user input with support for multiline.\"\"\"\n        try:\n            self.console.print(\"\\n[bold white]\u250c\u2500[/bold white] [bold cyan]You[/bold cyan]\")\n            self.console.print(\"[bold white]\u2514\u2500\u27a4[/bold white] \", end=\"\")\n            user_input = input().strip()\n            \n            # Check for multiline input (ending with \\)\n            while user_input.endswith('\\\\'):\n                user_input = user_input[:-1] + '\\n'\n                self.console.print(\"   [bold white]\u2502[/bold white] \", end=\"\")\n                user_input += input().strip()\n            \n            return user_input\n        except (EOFError, KeyboardInterrupt):\n            return \":exit\"", "docstring": "Gets user input with support for multiline.", "searchable_text": "get_user_input Gets user input with support for multiline.     def get_user_input(self) -> str:\n        \"\"\"Gets user input with support for multiline.\"\"\"\n        try:\n            self.console.print(\"\\n[bold white]\u250c\u2500[/bold white] [bold cyan]You[/bold cyan]\")\n            self.console.print(\"[bold white]\u2514\u2500\u27a4[/bold white] \", end=\"\")\n            user_input = input().strip()\n            \n            # Check for multiline input (ending with \\)\n            while user_input.endswith('\\\\'):\n                user_input = user_input[:-1] + '\\n'\n                self.console.print(\"   [bold white]\u2502[/bold white] \", end=\"\")\n                user_input += input().strip()\n            \n            return user_input\n        except (EOFError, KeyboardInterrupt):\n            return \":exit\""}, {"filepath": "cli.py", "start_line": 241, "end_line": 303, "type": "function", "name": "handle_command", "content": "    def handle_command(self, prompt: str, history: List[Dict[str, str]], model: str) -> bool:\n        \"\"\"Handles special commands.\"\"\"\n        command, *args = prompt[1:].strip().split()\n        \n        if command in [\"exit\", \"q\"]:\n            return False\n        elif command == \"yolo\":\n            self.yolo_mode = not self.yolo_mode\n            self.console.print(f\"[green]YOLO mode {'ENABLED' if self.yolo_mode else 'DISABLED'}[/green]\")\n            if self.yolo_mode and not self.current_project:\n                self.current_project = os.getcwd()\n        elif command == \"analyze\":\n            if self.yolo_mode:\n                self.code_analyzer = CodeAnalyzer(self.current_project, self.console)\n                self.code_analyzer.scan_project()\n                if CONTEXT_ENGINE_AVAILABLE:\n                    self.context_engine = ContextEngine(self.current_project, self.console)\n                    self.context_engine.build_index(self.code_analyzer)\n                if SYMBOL_RESOLVER_AVAILABLE:\n                    self.symbol_resolver = SymbolResolver(self.current_project)\n                    # Simplified analysis loop\n                    for f in self.code_analyzer.files[:100]:\n                        content = self.code_analyzer.read_file_content(f)\n                        if content: self.symbol_resolver.analyze_file(f, content)\n                if QUERY_ANALYZER_AVAILABLE:\n                    self.query_analyzer = QueryAnalyzer()\n                self.console.print(\"[green]Intelligent analysis complete![/green]\")\n            else:\n                self.console.print(\"[yellow]Enable YOLO mode first with :yolo[/yellow]\")\n        elif command == \"clear\":\n            history.clear()\n            self.console.print(\"[green]Chat history cleared.[/green]\")\n        elif command == \"temp\":\n            if args:\n                try:\n                    self.temperature = float(args[0])\n                    self.console.print(f\"[green]Temperature set to {self.temperature}[/green]\")\n                except ValueError:\n                    self.console.print(\"[red]Invalid temperature value. Use a number between 0.0 and 2.0[/red]\")\n            else:\n                self.console.print(f\"[cyan]Current temperature: {self.temperature}[/cyan]\")\n        elif command == \"tokens\":\n            if args:\n                try:\n                    self.max_tokens = int(args[0])\n                    self.console.print(f\"[green]Max tokens set to {self.max_tokens}[/green]\")\n                except ValueError:\n                    self.console.print(\"[red]Invalid tokens value. Use an integer.[/red]\")\n            else:\n                self.console.print(f\"[cyan]Current max tokens: {self.max_tokens or 'unlimited'}[/cyan]\")\n        elif command == \"system\":\n            if args:\n                system_prompt = \" \".join(args)\n                history.insert(0, {\"role\": \"system\", \"content\": system_prompt})\n                self.console.print(f\"[green]System prompt set.[/green]\")\n            else:\n                self.console.print(\"[red]Please provide a system prompt.[/red]\")\n        elif command == \"help\":\n            self.console.print(HELP_MSG)\n        else:\n            self.console.print(f\"[red]Unknown command: {command}[/red]\")\n            self.console.print(\"[dim]Type :help for available commands.[/dim]\")\n        return True", "docstring": "Handles special commands.", "searchable_text": "handle_command Handles special commands.     def handle_command(self, prompt: str, history: List[Dict[str, str]], model: str) -> bool:\n        \"\"\"Handles special commands.\"\"\"\n        command, *args = prompt[1:].strip().split()\n        \n        if command in [\"exit\", \"q\"]:\n            return False\n        elif command == \"yolo\":\n            self.yolo_mode = not self.yolo_mode\n            self.console.print(f\"[green]YOLO mode {'ENABLED' if self.yolo_mode else 'DISABLED'}[/green]\")\n            if self.yolo_mode and not self.current_project:\n                self.current_project = os.getcwd()\n        elif command == \"analyze\":\n            if self.yolo_mode:\n                self.code_analyzer = CodeAnalyzer(self.current_project, self.console)\n                self.code_analyzer.scan_project()\n                if CONTEXT_ENGINE_AVAILABLE:\n                    self.context_engine = ContextEngine(self.current_project, self.console)\n                    self.context_engine.build_index(self.code_analyzer)\n                if SYMBOL_RESOLVER_AVAILABLE:\n                    self.symbol_resolver = SymbolResolver(self.current_project)\n                    # Simplified analysis loop\n                    for f in self.code_analyzer.files[:100]:\n                        content = self.code_analyzer.read_file_content(f)\n                        if content: self.symbol_resolver.analyze_file(f, content)\n                if QUERY_ANALYZER_AVAILABLE:\n                    self.query_analyzer = QueryAnalyzer()\n                self.console.print(\"[green]Intelligent analysis complete![/green]\")\n            else:\n                self.console.print(\"[yellow]Enable YOLO mode first with :yolo[/yellow]\")\n        elif command == \"clear\":\n            history.clear()\n            self.console.print(\"[green]Chat history cleared.[/green]\")\n        elif command == \"temp\":\n            if args:\n                try:\n                    self.temperature = float(args[0])\n                    self.console.print(f\"[green]Temperature set to {self.temperature}[/green]\")\n                except ValueError:\n                    self.console.print(\"[red]Invalid temperature value. Use a number between 0.0 and 2.0[/red]\")\n            else:\n                self.console.print(f\"[cyan]Current temperature: {self.temperature}[/cyan]\")\n        elif command == \"tokens\":\n            if args:\n                try:\n                    self.max_tokens = int(args[0])\n                    self.console.print(f\"[green]Max tokens set to {self.max_tokens}[/green]\")\n                except ValueError:\n                    self.console.print(\"[red]Invalid tokens value. Use an integer.[/red]\")\n            else:\n                self.console.print(f\"[cyan]Current max tokens: {self.max_tokens or 'unlimited'}[/cyan]\")\n        elif command == \"system\":\n            if args:\n                system_prompt = \" \".join(args)\n                history.insert(0, {\"role\": \"system\", \"content\": system_prompt})\n                self.console.print(f\"[green]System prompt set.[/green]\")\n            else:\n                self.console.print(\"[red]Please provide a system prompt.[/red]\")\n        elif command == \"help\":\n            self.console.print(HELP_MSG)\n        else:\n            self.console.print(f\"[red]Unknown command: {command}[/red]\")\n            self.console.print(\"[dim]Type :help for available commands.[/dim]\")\n        return True"}, {"filepath": "cli.py", "start_line": 305, "end_line": 307, "type": "function", "name": "handle_image_generation", "content": "    def handle_image_generation(self, args):\n        # ... (implementation for image generation)\n        pass", "docstring": "", "searchable_text": "handle_image_generation      def handle_image_generation(self, args):\n        # ... (implementation for image generation)\n        pass"}, {"filepath": "code_analyzer.py", "start_line": 15, "end_line": 217, "type": "class", "name": "CodeAnalyzer", "content": "class CodeAnalyzer:\n    \"\"\"Analyzes source code in a project directory.\"\"\"\n    \n    CODE_EXTENSIONS = {\n        '.py', '.js', '.ts', '.jsx', '.tsx', '.java', '.cpp', '.c', '.h', '.hpp',\n        '.cs', '.go', '.rs', '.rb', '.php', '.swift', '.kt', '.scala', '.r',\n        '.m', '.mm', '.sh', '.bash', '.zsh', '.bat', '.cmd', '.ps1', '.sql', '.html', '.css', '.scss',\n        '.sass', '.vue', '.svelte', '.json', '.yaml', '.yml', '.xml', '.md',\n        '.toml', '.ini', '.cfg', '.conf', '.dockerfile', '.lock', '.txt'\n    }\n    \n    IGNORE_DIRS = {\n        'node_modules', '.git', '.venv', 'venv', 'env', '__pycache__',\n        '.pytest_cache', 'dist', 'build', 'target', '.idea', '.vscode',\n        'vendor', 'tmp', 'temp', '.cache', 'coverage', '.next', 'out',\n        '.angular', '.svelte-kit', '.nuxt', '.output', 'bower_components',\n        '.dart_tool', 'buck-out', '.gradle', '.mvn'\n    }\n    \n    PROJECT_MARKERS = {\n        'python': ['requirements.txt', 'setup.py', 'pyproject.toml', 'Pipfile'],\n        'node': ['package.json', 'package-lock.json', 'yarn.lock', 'pnpm-lock.yaml'],\n        'java': ['pom.xml', 'build.gradle', 'build.gradle.kts'],\n        'rust': ['Cargo.toml', 'Cargo.lock'],\n        'go': ['go.mod', 'go.sum'],\n        'php': ['composer.json', 'composer.lock'],\n        'ruby': ['Gemfile', 'Gemfile.lock'],\n        '.net': ['*.csproj', '*.sln', 'packages.config']\n    }\n    \n    def __init__(self, project_path: str, console: Console):\n        self.project_path = os.path.abspath(project_path)\n        self.console = console\n        self.files: List[str] = []\n        self.file_stats: Dict[str, int] = {}\n        self.total_lines = 0\n        self.project_type: Optional[str] = None\n        self.key_files: List[str] = []\n        \n        self.max_file_size = int(os.environ.get('MAPLECLI_MAX_FILE_SIZE', 10 * 1024 * 1024))\n        self.max_total_size = int(os.environ.get('MAPLECLI_MAX_TOTAL_SIZE', 100 * 1024 * 1024))\n        self.max_depth = int(os.environ.get('MAPLECLI_MAX_DEPTH', 10))\n        self.processed_size = 0\n        \n    def _should_ignore_file(self, filepath: str) -> bool:\n        \"\"\"Check if file should be ignored.\"\"\"\n        basename = os.path.basename(filepath)\n        if basename.startswith('.') and basename not in {'.env.example', '.gitignore', '.dockerignore'}:\n            return True\n        \n        ignore_patterns = ['*.lock', '*.log', '*.tmp', '*.swp', '*.bak', '*.pyc', '*.class']\n        for pattern in ignore_patterns:\n            if pattern.replace('*', '') in basename:\n                return True\n        return False\n    \n    def _detect_project_type(self):\n        \"\"\"Detect project type based on marker files.\"\"\"\n        for project_type, markers in self.PROJECT_MARKERS.items():\n            for marker in markers:\n                if os.path.exists(os.path.join(self.project_path, marker)):\n                    self.project_type = project_type\n                    return\n        self.project_type = 'unknown'\n    \n    async def scan_project_async(self) -> Dict[str, any]:\n        \"\"\"Asynchronously scans the project directory.\"\"\"\n        if not os.path.isdir(self.project_path):\n            self.console.print(f\"[bold red]Error: Path is not a directory: {self.project_path}[/bold red]\")\n            return {}\n        \n        self.files, self.file_stats, self.total_lines, self.key_files, self.processed_size = [], {}, 0, [], 0\n        skipped_files = []\n        \n        with Progress(SpinnerColumn(), TextColumn(\"[progress.description]{task.description}\"), BarColumn(), TaskProgressColumn(), console=self.console) as progress:\n            task = progress.add_task(\"[cyan]Scanning project...\", total=None)\n            self._detect_project_type()\n            \n            for root, dirs, files in os.walk(self.project_path):\n                dirs[:] = [d for d in dirs if d not in self.IGNORE_DIRS]\n                \n                rel_root = os.path.relpath(root, self.project_path)\n                depth = len(rel_root.split(os.sep)) if rel_root != '.' else 0\n                if depth > self.max_depth:\n                    dirs.clear()\n                    continue\n                \n                for file in files:\n                    progress.update(task, description=f\"[cyan]Scanning... {len(self.files)} files\")\n                    if self._should_ignore_file(file):\n                        continue\n                    \n                    filepath = os.path.join(root, file)\n                    rel_path = os.path.relpath(filepath, self.project_path)\n                    \n                    try:\n                        file_size = os.path.getsize(filepath)\n                        if file_size > self.max_file_size or self.processed_size + file_size > self.max_total_size:\n                            skipped_files.append(f\"{rel_path} (size limit)\")\n                            continue\n                        self.processed_size += file_size\n                        \n                        if any(keyword in file.upper() for keyword in ['README', 'LICENSE', 'CONTRIBUTING']) or \\\n                           file in ['package.json', 'setup.py', 'main.py', 'index.js']:\n                            self.key_files.append(rel_path)\n                        \n                        ext = os.path.splitext(file)[1].lower()\n                        if ext in self.CODE_EXTENSIONS:\n                            self.files.append(rel_path)\n                            with open(filepath, 'r', encoding='utf-8', errors='ignore') as f:\n                                lines = sum(1 for _ in f)\n                                self.total_lines += lines\n                                self.file_stats[ext] = self.file_stats.get(ext, 0) + lines\n                    except Exception as e:\n                        maple_logger.log_error(e, \"scan_project\", rel_path)\n                        skipped_files.append(f\"{rel_path} (read error)\")\n        \n        return {\n            'project_path': self.project_path, 'project_type': self.project_type,\n            'total_files': len(self.files), 'total_lines': self.total_lines,\n            'file_stats': self.file_stats, 'key_files': self.key_files,\n            'files': self.files[:100], 'skipped_files': skipped_files,\n            'processed_size_mb': round(self.processed_size / (1024 * 1024), 2)\n        }\n\n    def scan_project(self) -> Dict[str, any]:\n        \"\"\"Synchronous wrapper for scan_project_async.\"\"\"\n        return asyncio.run(self.scan_project_async())\n\n    def get_project_tree(self, max_depth: int = 3) -> Tree:\n        \"\"\"Creates a visual tree representation of the project structure.\"\"\"\n        tree = Tree(f\"[bold cyan]{os.path.basename(self.project_path)}[/bold cyan]\")\n        \n        def add_to_tree(parent_tree: Tree, path: str, current_depth: int):\n            if current_depth >= max_depth:\n                return\n            try:\n                items = sorted(os.listdir(path))\n                dirs = [item for item in items if os.path.isdir(os.path.join(path, item)) and item not in self.IGNORE_DIRS][:10]\n                files = [item for item in items if os.path.isfile(os.path.join(path, item))][:10]\n                \n                for d in dirs:\n                    branch = parent_tree.add(f\"[blue]{d}/[/blue]\")\n                    add_to_tree(branch, os.path.join(path, d), current_depth + 1)\n                for f in files:\n                    parent_tree.add(f\"[green]{f}[/green]\" if os.path.splitext(f)[1] in self.CODE_EXTENSIONS else f\"[dim]{f}[/dim]\")\n            except PermissionError:\n                pass\n        \n        add_to_tree(tree, self.project_path, 0)\n        return tree\n\n    def read_file_content(self, relative_path: str, max_lines: int = 100) -> Optional[str]:\n        \"\"\"Reads the content of a specific file with security validation.\"\"\"\n        try:\n            filepath = self._safe_join_path(self.project_path, relative_path)\n            if not filepath or not os.path.isfile(filepath):\n                return f\"Error: Not a regular file: {relative_path}\"\n            \n            if os.path.getsize(filepath) > self.max_file_size:\n                return f\"Error: File too large\"\n            \n            with open(filepath, 'r', encoding='utf-8', errors='ignore') as f:\n                lines = f.readlines()[:max_lines]\n                content = ''.join(lines)\n                if sum(1 for _ in f) > 0:\n                    content += \"\\n... (more lines)\"\n                return content\n        except (SecurityError, Exception) as e:\n            maple_logger.log_error(e, \"read_file_content\", relative_path)\n            return f\"Error reading file: {e}\"\n\n    def _safe_join_path(self, base_path: str, relative_path: str) -> Optional[str]:\n        \"\"\"Prevent path traversal attacks.\"\"\"\n        full_path = os.path.abspath(os.path.join(base_path, os.path.normpath(relative_path)))\n        if not full_path.startswith(base_path):\n            raise SecurityError(\"Path traversal detected\")\n        return full_path\n\n    def generate_summary(self) -> str:\n        \"\"\"Generates a comprehensive text summary for AI context.\"\"\"\n        summary_parts = [\n            \"=== WORKSPACE CONTEXT ===\",\n            f\"Project: {os.path.basename(self.project_path)} ({self.project_type or 'unknown'})\",\n            f\"Stats: {len(self.files)} files, {self.total_lines:,} lines\",\n            \"\\n=== LANGUAGES & FILE TYPES ===\"\n        ]\n        for ext, lines in sorted(self.file_stats.items(), key=lambda x: x[1], reverse=True)[:5]:\n            percentage = (lines / self.total_lines * 100) if self.total_lines > 0 else 0\n            summary_parts.append(f\"  {ext:10s}: {lines:7,} lines ({percentage:5.1f}%)\")\n        \n        if self.key_files:\n            summary_parts.append(\"\\n=== KEY FILES ===\")\n            for file in self.key_files[:10]:\n                summary_parts.append(f\"  - {file}\")\n        \n        summary_parts.append(\"\\n=== PROJECT STRUCTURE (First 40 files) ===\")\n        for file in self.files[:40]:\n            summary_parts.append(f\"  - {file}\")\n        if len(self.files) > 40:\n            summary_parts.append(f\"  ... and {len(self.files) - 40} more files\")\n        \n        return \"\\n\".join(summary_parts)", "docstring": "Analyzes source code in a project directory.", "searchable_text": "CodeAnalyzer Analyzes source code in a project directory. class CodeAnalyzer:\n    \"\"\"Analyzes source code in a project directory.\"\"\"\n    \n    CODE_EXTENSIONS = {\n        '.py', '.js', '.ts', '.jsx', '.tsx', '.java', '.cpp', '.c', '.h', '.hpp',\n        '.cs', '.go', '.rs', '.rb', '.php', '.swift', '.kt', '.scala', '.r',\n        '.m', '.mm', '.sh', '.bash', '.zsh', '.bat', '.cmd', '.ps1', '.sql', '.html', '.css', '.scss',\n        '.sass', '.vue', '.svelte', '.json', '.yaml', '.yml', '.xml', '.md',\n        '.toml', '.ini', '.cfg', '.conf', '.dockerfile', '.lock', '.txt'\n    }\n    \n    IGNORE_DIRS = {\n        'node_modules', '.git', '.venv', 'venv', 'env', '__pycache__',\n        '.pytest_cache', 'dist', 'build', 'target', '.idea', '.vscode',\n        'vendor', 'tmp', 'temp', '.cache', 'coverage', '.next', 'out',\n        '.angular', '.svelte-kit', '.nuxt', '.output', 'bower_components',\n        '.dart_tool', 'buck-out', '.gradle', '.mvn'\n    }\n    \n    PROJECT_MARKERS = {\n        'python': ['requirements.txt', 'setup.py', 'pyproject.toml', 'Pipfile'],\n        'node': ['package.json', 'package-lock.json', 'yarn.lock', 'pnpm-lock.yaml'],\n        'java': ['pom.xml', 'build.gradle', 'build.gradle.kts'],\n        'rust': ['Cargo.toml', 'Cargo.lock'],\n        'go': ['go.mod', 'go.sum'],\n        'php': ['composer.json', 'composer.lock'],\n        'ruby': ['Gemfile', 'Gemfile.lock'],\n        '.net': ['*.csproj', '*.sln', 'packages.config']\n    }\n    \n    def __init__(self, project_path: str, console: Console):\n        self.project_path = os.path.abspath(project_path)\n        self.console = console\n        self.files: List[str] = []\n        self.file_stats: Dict[str, int] = {}\n        self.total_lines = 0\n        self.project_type: Optional[str] = None\n        self.key_files: List[str] = []\n        \n        self.max_file_size = int(os.environ.get('MAPLECLI_MAX_FILE_SIZE', 10 * 1024 * 1024))\n        self.max_total_size = int(os.environ.get('MAPLECLI_MAX_TOTAL_SIZE', 100 * 1024 * 1024))\n        self.max_depth = int(os.environ.get('MAPLECLI_MAX_DEPTH', 10))\n        self.processed_size = 0\n        \n    def _should_ignore_file(self, filepath: str) -> bool:\n        \"\"\"Check if file should be ignored.\"\"\"\n        basename = os.path.basename(filepath)\n        if basename.startswith('.') and basename not in {'.env.example', '.gitignore', '.dockerignore'}:\n            return True\n        \n        ignore_patterns = ['*.lock', '*.log', '*.tmp', '*.swp', '*.bak', '*.pyc', '*.class']\n        for pattern in ignore_patterns:\n            if pattern.replace('*', '') in basename:\n                return True\n        return False\n    \n    def _detect_project_type(self):\n        \"\"\"Detect project type based on marker files.\"\"\"\n        for project_type, markers in self.PROJECT_MARKERS.items():\n            for marker in markers:\n                if os.path.exists(os.path.join(self.project_path, marker)):\n                    self.project_type = project_type\n                    return\n        self.project_type = 'unknown'\n    \n    async def scan_project_async(self) -> Dict[str, any]:\n        \"\"\"Asynchronously scans the project directory.\"\"\"\n        if not os.path.isdir(self.project_path):\n            self.console.print(f\"[bold red]Error: Path is not a directory: {self.project_path}[/bold red]\")\n            return {}\n        \n        self.files, self.file_stats, self.total_lines, self.key_files, self.processed_size = [], {}, 0, [], 0\n        skipped_files = []\n        \n        with Progress(SpinnerColumn(), TextColumn(\"[progress.description]{task.description}\"), BarColumn(), TaskProgressColumn(), console=self.console) as progress:\n            task = progress.add_task(\"[cyan]Scanning project...\", total=None)\n            self._detect_project_type()\n            \n            for root, dirs, files in os.walk(self.project_path):\n                dirs[:] = [d for d in dirs if d not in self.IGNORE_DIRS]\n                \n                rel_root = os.path.relpath(root, self.project_path)\n                depth = len(rel_root.split(os.sep)) if rel_root != '.' else 0\n                if depth > self.max_depth:\n                    dirs.clear()\n                    continue\n                \n                for file in files:\n                    progress.update(task, description=f\"[cyan]Scanning... {len(self.files)} files\")\n                    if self._should_ignore_file(file):\n                        continue\n                    \n                    filepath = os.path.join(root, file)\n                    rel_path = os.path.relpath(filepath, self.project_path)\n                    \n                    try:\n                        file_size = os.path.getsize(filepath)\n                        if file_size > self.max_file_size or self.processed_size + file_size > self.max_total_size:\n                            skipped_files.append(f\"{rel_path} (size limit)\")\n                            continue\n                        self.processed_size += file_size\n                        \n                        if any(keyword in file.upper() for keyword in ['README', 'LICENSE', 'CONTRIBUTING']) or \\\n                           file in ['package.json', 'setup.py', 'main.py', 'index.js']:\n                            self.key_files.append(rel_path)\n                        \n                        ext = os.path.splitext(file)[1].lower()\n                        if ext in self.CODE_EXTENSIONS:\n                            self.files.append(rel_path)\n                            with open(filepath, 'r', encoding='utf-8', errors='ignore') as f:\n                                lines = sum(1 for _ in f)\n                                self.total_lines += lines\n                                self.file_stats[ext] = self.file_stats.get(ext, 0) + lines\n                    except Exception as e:\n                        maple_logger.log_error(e, \"scan_project\", rel_path)\n                        skipped_files.append(f\"{rel_path} (read error)\")\n        \n        return {\n            'project_path': self.project_path, 'project_type': self.project_type,\n            'total_files': len(self.files), 'total_lines': self.total_lines,\n            'file_stats': self.file_stats, 'key_files': self.key_files,\n            'files': self.files[:100], 'skipped_files': skipped_files,\n            'processed_size_mb': round(self.processed_size / (1024 * 1024), 2)\n        }\n\n    def scan_project(self) -> Dict[str, any]:\n        \"\"\"Synchronous wrapper for scan_project_async.\"\"\"\n        return asyncio.run(self.scan_project_async())\n\n    def get_project_tree(self, max_depth: int = 3) -> Tree:\n        \"\"\"Creates a visual tree representation of the project structure.\"\"\"\n        tree = Tree(f\"[bold cyan]{os.path.basename(self.project_path)}[/bold cyan]\")\n        \n        def add_to_tree(parent_tree: Tree, path: str, current_depth: int):\n            if current_depth >= max_depth:\n                return\n            try:\n                items = sorted(os.listdir(path))\n                dirs = [item for item in items if os.path.isdir(os.path.join(path, item)) and item not in self.IGNORE_DIRS][:10]\n                files = [item for item in items if os.path.isfile(os.path.join(path, item))][:10]\n                \n                for d in dirs:\n                    branch = parent_tree.add(f\"[blue]{d}/[/blue]\")\n                    add_to_tree(branch, os.path.join(path, d), current_depth + 1)\n                for f in files:\n                    parent_tree.add(f\"[green]{f}[/green]\" if os.path.splitext(f)[1] in self.CODE_EXTENSIONS else f\"[dim]{f}[/dim]\")\n            except PermissionError:\n                pass\n        \n        add_to_tree(tree, self.project_path, 0)\n        return tree\n\n    def read_file_content(self, relative_path: str, max_lines: int = 100) -> Optional[str]:\n        \"\"\"Reads the content of a specific file with security validation.\"\"\"\n        try:\n            filepath = self._safe_join_path(self.project_path, relative_path)\n            if not filepath or not os.path.isfile(filepath):\n                return f\"Error: Not a regular file: {relative_path}\"\n            \n            if os.path.getsize(filepath) > self.max_file_size:\n                return f\"Error: File too large\"\n            \n            with open(filepath, 'r', encoding='utf-8', errors='ignore') as f:\n                lines = f.readlines()[:max_lines]\n                content = ''.join(lines)\n                if sum(1 for _ in f) > 0:\n                    content += \"\\n... (more lines)\"\n                return content\n        except (SecurityError, Exception) as e:\n            maple_logger.log_error(e, \"read_file_content\", relative_path)\n            return f\"Error reading file: {e}\"\n\n    def _safe_join_path(self, base_path: str, relative_path: str) -> Optional[str]:\n        \"\"\"Prevent path traversal attacks.\"\"\"\n        full_path = os.path.abspath(os.path.join(base_path, os.path.normpath(relative_path)))\n        if not full_path.startswith(base_path):\n            raise SecurityError(\"Path traversal detected\")\n        return full_path\n\n    def generate_summary(self) -> str:\n        \"\"\"Generates a comprehensive text summary for AI context.\"\"\"\n        summary_parts = [\n            \"=== WORKSPACE CONTEXT ===\",\n            f\"Project: {os.path.basename(self.project_path)} ({self.project_type or 'unknown'})\",\n            f\"Stats: {len(self.files)} files, {self.total_lines:,} lines\",\n            \"\\n=== LANGUAGES & FILE TYPES ===\"\n        ]\n        for ext, lines in sorted(self.file_stats.items(), key=lambda x: x[1], reverse=True)[:5]:\n            percentage = (lines / self.total_lines * 100) if self.total_lines > 0 else 0\n            summary_parts.append(f\"  {ext:10s}: {lines:7,} lines ({percentage:5.1f}%)\")\n        \n        if self.key_files:\n            summary_parts.append(\"\\n=== KEY FILES ===\")\n            for file in self.key_files[:10]:\n                summary_parts.append(f\"  - {file}\")\n        \n        summary_parts.append(\"\\n=== PROJECT STRUCTURE (First 40 files) ===\")\n        for file in self.files[:40]:\n            summary_parts.append(f\"  - {file}\")\n        if len(self.files) > 40:\n            summary_parts.append(f\"  ... and {len(self.files) - 40} more files\")\n        \n        return \"\\n\".join(summary_parts)"}, {"filepath": "code_analyzer.py", "start_line": 45, "end_line": 57, "type": "function", "name": "__init__", "content": "    def __init__(self, project_path: str, console: Console):\n        self.project_path = os.path.abspath(project_path)\n        self.console = console\n        self.files: List[str] = []\n        self.file_stats: Dict[str, int] = {}\n        self.total_lines = 0\n        self.project_type: Optional[str] = None\n        self.key_files: List[str] = []\n        \n        self.max_file_size = int(os.environ.get('MAPLECLI_MAX_FILE_SIZE', 10 * 1024 * 1024))\n        self.max_total_size = int(os.environ.get('MAPLECLI_MAX_TOTAL_SIZE', 100 * 1024 * 1024))\n        self.max_depth = int(os.environ.get('MAPLECLI_MAX_DEPTH', 10))\n        self.processed_size = 0", "docstring": "", "searchable_text": "__init__      def __init__(self, project_path: str, console: Console):\n        self.project_path = os.path.abspath(project_path)\n        self.console = console\n        self.files: List[str] = []\n        self.file_stats: Dict[str, int] = {}\n        self.total_lines = 0\n        self.project_type: Optional[str] = None\n        self.key_files: List[str] = []\n        \n        self.max_file_size = int(os.environ.get('MAPLECLI_MAX_FILE_SIZE', 10 * 1024 * 1024))\n        self.max_total_size = int(os.environ.get('MAPLECLI_MAX_TOTAL_SIZE', 100 * 1024 * 1024))\n        self.max_depth = int(os.environ.get('MAPLECLI_MAX_DEPTH', 10))\n        self.processed_size = 0"}, {"filepath": "code_analyzer.py", "start_line": 59, "end_line": 69, "type": "function", "name": "_should_ignore_file", "content": "    def _should_ignore_file(self, filepath: str) -> bool:\n        \"\"\"Check if file should be ignored.\"\"\"\n        basename = os.path.basename(filepath)\n        if basename.startswith('.') and basename not in {'.env.example', '.gitignore', '.dockerignore'}:\n            return True\n        \n        ignore_patterns = ['*.lock', '*.log', '*.tmp', '*.swp', '*.bak', '*.pyc', '*.class']\n        for pattern in ignore_patterns:\n            if pattern.replace('*', '') in basename:\n                return True\n        return False", "docstring": "Check if file should be ignored.", "searchable_text": "_should_ignore_file Check if file should be ignored.     def _should_ignore_file(self, filepath: str) -> bool:\n        \"\"\"Check if file should be ignored.\"\"\"\n        basename = os.path.basename(filepath)\n        if basename.startswith('.') and basename not in {'.env.example', '.gitignore', '.dockerignore'}:\n            return True\n        \n        ignore_patterns = ['*.lock', '*.log', '*.tmp', '*.swp', '*.bak', '*.pyc', '*.class']\n        for pattern in ignore_patterns:\n            if pattern.replace('*', '') in basename:\n                return True\n        return False"}, {"filepath": "code_analyzer.py", "start_line": 71, "end_line": 78, "type": "function", "name": "_detect_project_type", "content": "    def _detect_project_type(self):\n        \"\"\"Detect project type based on marker files.\"\"\"\n        for project_type, markers in self.PROJECT_MARKERS.items():\n            for marker in markers:\n                if os.path.exists(os.path.join(self.project_path, marker)):\n                    self.project_type = project_type\n                    return\n        self.project_type = 'unknown'", "docstring": "Detect project type based on marker files.", "searchable_text": "_detect_project_type Detect project type based on marker files.     def _detect_project_type(self):\n        \"\"\"Detect project type based on marker files.\"\"\"\n        for project_type, markers in self.PROJECT_MARKERS.items():\n            for marker in markers:\n                if os.path.exists(os.path.join(self.project_path, marker)):\n                    self.project_type = project_type\n                    return\n        self.project_type = 'unknown'"}, {"filepath": "code_analyzer.py", "start_line": 80, "end_line": 138, "type": "function", "name": "scan_project_async", "content": "    async def scan_project_async(self) -> Dict[str, any]:\n        \"\"\"Asynchronously scans the project directory.\"\"\"\n        if not os.path.isdir(self.project_path):\n            self.console.print(f\"[bold red]Error: Path is not a directory: {self.project_path}[/bold red]\")\n            return {}\n        \n        self.files, self.file_stats, self.total_lines, self.key_files, self.processed_size = [], {}, 0, [], 0\n        skipped_files = []\n        \n        with Progress(SpinnerColumn(), TextColumn(\"[progress.description]{task.description}\"), BarColumn(), TaskProgressColumn(), console=self.console) as progress:\n            task = progress.add_task(\"[cyan]Scanning project...\", total=None)\n            self._detect_project_type()\n            \n            for root, dirs, files in os.walk(self.project_path):\n                dirs[:] = [d for d in dirs if d not in self.IGNORE_DIRS]\n                \n                rel_root = os.path.relpath(root, self.project_path)\n                depth = len(rel_root.split(os.sep)) if rel_root != '.' else 0\n                if depth > self.max_depth:\n                    dirs.clear()\n                    continue\n                \n                for file in files:\n                    progress.update(task, description=f\"[cyan]Scanning... {len(self.files)} files\")\n                    if self._should_ignore_file(file):\n                        continue\n                    \n                    filepath = os.path.join(root, file)\n                    rel_path = os.path.relpath(filepath, self.project_path)\n                    \n                    try:\n                        file_size = os.path.getsize(filepath)\n                        if file_size > self.max_file_size or self.processed_size + file_size > self.max_total_size:\n                            skipped_files.append(f\"{rel_path} (size limit)\")\n                            continue\n                        self.processed_size += file_size\n                        \n                        if any(keyword in file.upper() for keyword in ['README', 'LICENSE', 'CONTRIBUTING']) or \\\n                           file in ['package.json', 'setup.py', 'main.py', 'index.js']:\n                            self.key_files.append(rel_path)\n                        \n                        ext = os.path.splitext(file)[1].lower()\n                        if ext in self.CODE_EXTENSIONS:\n                            self.files.append(rel_path)\n                            with open(filepath, 'r', encoding='utf-8', errors='ignore') as f:\n                                lines = sum(1 for _ in f)\n                                self.total_lines += lines\n                                self.file_stats[ext] = self.file_stats.get(ext, 0) + lines\n                    except Exception as e:\n                        maple_logger.log_error(e, \"scan_project\", rel_path)\n                        skipped_files.append(f\"{rel_path} (read error)\")\n        \n        return {\n            'project_path': self.project_path, 'project_type': self.project_type,\n            'total_files': len(self.files), 'total_lines': self.total_lines,\n            'file_stats': self.file_stats, 'key_files': self.key_files,\n            'files': self.files[:100], 'skipped_files': skipped_files,\n            'processed_size_mb': round(self.processed_size / (1024 * 1024), 2)\n        }", "docstring": "Asynchronously scans the project directory.", "searchable_text": "scan_project_async Asynchronously scans the project directory.     async def scan_project_async(self) -> Dict[str, any]:\n        \"\"\"Asynchronously scans the project directory.\"\"\"\n        if not os.path.isdir(self.project_path):\n            self.console.print(f\"[bold red]Error: Path is not a directory: {self.project_path}[/bold red]\")\n            return {}\n        \n        self.files, self.file_stats, self.total_lines, self.key_files, self.processed_size = [], {}, 0, [], 0\n        skipped_files = []\n        \n        with Progress(SpinnerColumn(), TextColumn(\"[progress.description]{task.description}\"), BarColumn(), TaskProgressColumn(), console=self.console) as progress:\n            task = progress.add_task(\"[cyan]Scanning project...\", total=None)\n            self._detect_project_type()\n            \n            for root, dirs, files in os.walk(self.project_path):\n                dirs[:] = [d for d in dirs if d not in self.IGNORE_DIRS]\n                \n                rel_root = os.path.relpath(root, self.project_path)\n                depth = len(rel_root.split(os.sep)) if rel_root != '.' else 0\n                if depth > self.max_depth:\n                    dirs.clear()\n                    continue\n                \n                for file in files:\n                    progress.update(task, description=f\"[cyan]Scanning... {len(self.files)} files\")\n                    if self._should_ignore_file(file):\n                        continue\n                    \n                    filepath = os.path.join(root, file)\n                    rel_path = os.path.relpath(filepath, self.project_path)\n                    \n                    try:\n                        file_size = os.path.getsize(filepath)\n                        if file_size > self.max_file_size or self.processed_size + file_size > self.max_total_size:\n                            skipped_files.append(f\"{rel_path} (size limit)\")\n                            continue\n                        self.processed_size += file_size\n                        \n                        if any(keyword in file.upper() for keyword in ['README', 'LICENSE', 'CONTRIBUTING']) or \\\n                           file in ['package.json', 'setup.py', 'main.py', 'index.js']:\n                            self.key_files.append(rel_path)\n                        \n                        ext = os.path.splitext(file)[1].lower()\n                        if ext in self.CODE_EXTENSIONS:\n                            self.files.append(rel_path)\n                            with open(filepath, 'r', encoding='utf-8', errors='ignore') as f:\n                                lines = sum(1 for _ in f)\n                                self.total_lines += lines\n                                self.file_stats[ext] = self.file_stats.get(ext, 0) + lines\n                    except Exception as e:\n                        maple_logger.log_error(e, \"scan_project\", rel_path)\n                        skipped_files.append(f\"{rel_path} (read error)\")\n        \n        return {\n            'project_path': self.project_path, 'project_type': self.project_type,\n            'total_files': len(self.files), 'total_lines': self.total_lines,\n            'file_stats': self.file_stats, 'key_files': self.key_files,\n            'files': self.files[:100], 'skipped_files': skipped_files,\n            'processed_size_mb': round(self.processed_size / (1024 * 1024), 2)\n        }"}, {"filepath": "code_analyzer.py", "start_line": 140, "end_line": 142, "type": "function", "name": "scan_project", "content": "    def scan_project(self) -> Dict[str, any]:\n        \"\"\"Synchronous wrapper for scan_project_async.\"\"\"\n        return asyncio.run(self.scan_project_async())", "docstring": "Synchronous wrapper for scan_project_async.", "searchable_text": "scan_project Synchronous wrapper for scan_project_async.     def scan_project(self) -> Dict[str, any]:\n        \"\"\"Synchronous wrapper for scan_project_async.\"\"\"\n        return asyncio.run(self.scan_project_async())"}, {"filepath": "code_analyzer.py", "start_line": 144, "end_line": 165, "type": "function", "name": "get_project_tree", "content": "    def get_project_tree(self, max_depth: int = 3) -> Tree:\n        \"\"\"Creates a visual tree representation of the project structure.\"\"\"\n        tree = Tree(f\"[bold cyan]{os.path.basename(self.project_path)}[/bold cyan]\")\n        \n        def add_to_tree(parent_tree: Tree, path: str, current_depth: int):\n            if current_depth >= max_depth:\n                return\n            try:\n                items = sorted(os.listdir(path))\n                dirs = [item for item in items if os.path.isdir(os.path.join(path, item)) and item not in self.IGNORE_DIRS][:10]\n                files = [item for item in items if os.path.isfile(os.path.join(path, item))][:10]\n                \n                for d in dirs:\n                    branch = parent_tree.add(f\"[blue]{d}/[/blue]\")\n                    add_to_tree(branch, os.path.join(path, d), current_depth + 1)\n                for f in files:\n                    parent_tree.add(f\"[green]{f}[/green]\" if os.path.splitext(f)[1] in self.CODE_EXTENSIONS else f\"[dim]{f}[/dim]\")\n            except PermissionError:\n                pass\n        \n        add_to_tree(tree, self.project_path, 0)\n        return tree", "docstring": "Creates a visual tree representation of the project structure.", "searchable_text": "get_project_tree Creates a visual tree representation of the project structure.     def get_project_tree(self, max_depth: int = 3) -> Tree:\n        \"\"\"Creates a visual tree representation of the project structure.\"\"\"\n        tree = Tree(f\"[bold cyan]{os.path.basename(self.project_path)}[/bold cyan]\")\n        \n        def add_to_tree(parent_tree: Tree, path: str, current_depth: int):\n            if current_depth >= max_depth:\n                return\n            try:\n                items = sorted(os.listdir(path))\n                dirs = [item for item in items if os.path.isdir(os.path.join(path, item)) and item not in self.IGNORE_DIRS][:10]\n                files = [item for item in items if os.path.isfile(os.path.join(path, item))][:10]\n                \n                for d in dirs:\n                    branch = parent_tree.add(f\"[blue]{d}/[/blue]\")\n                    add_to_tree(branch, os.path.join(path, d), current_depth + 1)\n                for f in files:\n                    parent_tree.add(f\"[green]{f}[/green]\" if os.path.splitext(f)[1] in self.CODE_EXTENSIONS else f\"[dim]{f}[/dim]\")\n            except PermissionError:\n                pass\n        \n        add_to_tree(tree, self.project_path, 0)\n        return tree"}, {"filepath": "code_analyzer.py", "start_line": 167, "end_line": 185, "type": "function", "name": "read_file_content", "content": "    def read_file_content(self, relative_path: str, max_lines: int = 100) -> Optional[str]:\n        \"\"\"Reads the content of a specific file with security validation.\"\"\"\n        try:\n            filepath = self._safe_join_path(self.project_path, relative_path)\n            if not filepath or not os.path.isfile(filepath):\n                return f\"Error: Not a regular file: {relative_path}\"\n            \n            if os.path.getsize(filepath) > self.max_file_size:\n                return f\"Error: File too large\"\n            \n            with open(filepath, 'r', encoding='utf-8', errors='ignore') as f:\n                lines = f.readlines()[:max_lines]\n                content = ''.join(lines)\n                if sum(1 for _ in f) > 0:\n                    content += \"\\n... (more lines)\"\n                return content\n        except (SecurityError, Exception) as e:\n            maple_logger.log_error(e, \"read_file_content\", relative_path)\n            return f\"Error reading file: {e}\"", "docstring": "Reads the content of a specific file with security validation.", "searchable_text": "read_file_content Reads the content of a specific file with security validation.     def read_file_content(self, relative_path: str, max_lines: int = 100) -> Optional[str]:\n        \"\"\"Reads the content of a specific file with security validation.\"\"\"\n        try:\n            filepath = self._safe_join_path(self.project_path, relative_path)\n            if not filepath or not os.path.isfile(filepath):\n                return f\"Error: Not a regular file: {relative_path}\"\n            \n            if os.path.getsize(filepath) > self.max_file_size:\n                return f\"Error: File too large\"\n            \n            with open(filepath, 'r', encoding='utf-8', errors='ignore') as f:\n                lines = f.readlines()[:max_lines]\n                content = ''.join(lines)\n                if sum(1 for _ in f) > 0:\n                    content += \"\\n... (more lines)\"\n                return content\n        except (SecurityError, Exception) as e:\n            maple_logger.log_error(e, \"read_file_content\", relative_path)\n            return f\"Error reading file: {e}\""}, {"filepath": "code_analyzer.py", "start_line": 187, "end_line": 192, "type": "function", "name": "_safe_join_path", "content": "    def _safe_join_path(self, base_path: str, relative_path: str) -> Optional[str]:\n        \"\"\"Prevent path traversal attacks.\"\"\"\n        full_path = os.path.abspath(os.path.join(base_path, os.path.normpath(relative_path)))\n        if not full_path.startswith(base_path):\n            raise SecurityError(\"Path traversal detected\")\n        return full_path", "docstring": "Prevent path traversal attacks.", "searchable_text": "_safe_join_path Prevent path traversal attacks.     def _safe_join_path(self, base_path: str, relative_path: str) -> Optional[str]:\n        \"\"\"Prevent path traversal attacks.\"\"\"\n        full_path = os.path.abspath(os.path.join(base_path, os.path.normpath(relative_path)))\n        if not full_path.startswith(base_path):\n            raise SecurityError(\"Path traversal detected\")\n        return full_path"}, {"filepath": "code_analyzer.py", "start_line": 194, "end_line": 217, "type": "function", "name": "generate_summary", "content": "    def generate_summary(self) -> str:\n        \"\"\"Generates a comprehensive text summary for AI context.\"\"\"\n        summary_parts = [\n            \"=== WORKSPACE CONTEXT ===\",\n            f\"Project: {os.path.basename(self.project_path)} ({self.project_type or 'unknown'})\",\n            f\"Stats: {len(self.files)} files, {self.total_lines:,} lines\",\n            \"\\n=== LANGUAGES & FILE TYPES ===\"\n        ]\n        for ext, lines in sorted(self.file_stats.items(), key=lambda x: x[1], reverse=True)[:5]:\n            percentage = (lines / self.total_lines * 100) if self.total_lines > 0 else 0\n            summary_parts.append(f\"  {ext:10s}: {lines:7,} lines ({percentage:5.1f}%)\")\n        \n        if self.key_files:\n            summary_parts.append(\"\\n=== KEY FILES ===\")\n            for file in self.key_files[:10]:\n                summary_parts.append(f\"  - {file}\")\n        \n        summary_parts.append(\"\\n=== PROJECT STRUCTURE (First 40 files) ===\")\n        for file in self.files[:40]:\n            summary_parts.append(f\"  - {file}\")\n        if len(self.files) > 40:\n            summary_parts.append(f\"  ... and {len(self.files) - 40} more files\")\n        \n        return \"\\n\".join(summary_parts)", "docstring": "Generates a comprehensive text summary for AI context.", "searchable_text": "generate_summary Generates a comprehensive text summary for AI context.     def generate_summary(self) -> str:\n        \"\"\"Generates a comprehensive text summary for AI context.\"\"\"\n        summary_parts = [\n            \"=== WORKSPACE CONTEXT ===\",\n            f\"Project: {os.path.basename(self.project_path)} ({self.project_type or 'unknown'})\",\n            f\"Stats: {len(self.files)} files, {self.total_lines:,} lines\",\n            \"\\n=== LANGUAGES & FILE TYPES ===\"\n        ]\n        for ext, lines in sorted(self.file_stats.items(), key=lambda x: x[1], reverse=True)[:5]:\n            percentage = (lines / self.total_lines * 100) if self.total_lines > 0 else 0\n            summary_parts.append(f\"  {ext:10s}: {lines:7,} lines ({percentage:5.1f}%)\")\n        \n        if self.key_files:\n            summary_parts.append(\"\\n=== KEY FILES ===\")\n            for file in self.key_files[:10]:\n                summary_parts.append(f\"  - {file}\")\n        \n        summary_parts.append(\"\\n=== PROJECT STRUCTURE (First 40 files) ===\")\n        for file in self.files[:40]:\n            summary_parts.append(f\"  - {file}\")\n        if len(self.files) > 40:\n            summary_parts.append(f\"  ... and {len(self.files) - 40} more files\")\n        \n        return \"\\n\".join(summary_parts)"}, {"filepath": "code_analyzer.py", "start_line": 148, "end_line": 162, "type": "function", "name": "add_to_tree", "content": "        def add_to_tree(parent_tree: Tree, path: str, current_depth: int):\n            if current_depth >= max_depth:\n                return\n            try:\n                items = sorted(os.listdir(path))\n                dirs = [item for item in items if os.path.isdir(os.path.join(path, item)) and item not in self.IGNORE_DIRS][:10]\n                files = [item for item in items if os.path.isfile(os.path.join(path, item))][:10]\n                \n                for d in dirs:\n                    branch = parent_tree.add(f\"[blue]{d}/[/blue]\")\n                    add_to_tree(branch, os.path.join(path, d), current_depth + 1)\n                for f in files:\n                    parent_tree.add(f\"[green]{f}[/green]\" if os.path.splitext(f)[1] in self.CODE_EXTENSIONS else f\"[dim]{f}[/dim]\")\n            except PermissionError:\n                pass", "docstring": "", "searchable_text": "add_to_tree          def add_to_tree(parent_tree: Tree, path: str, current_depth: int):\n            if current_depth >= max_depth:\n                return\n            try:\n                items = sorted(os.listdir(path))\n                dirs = [item for item in items if os.path.isdir(os.path.join(path, item)) and item not in self.IGNORE_DIRS][:10]\n                files = [item for item in items if os.path.isfile(os.path.join(path, item))][:10]\n                \n                for d in dirs:\n                    branch = parent_tree.add(f\"[blue]{d}/[/blue]\")\n                    add_to_tree(branch, os.path.join(path, d), current_depth + 1)\n                for f in files:\n                    parent_tree.add(f\"[green]{f}[/green]\" if os.path.splitext(f)[1] in self.CODE_EXTENSIONS else f\"[dim]{f}[/dim]\")\n            except PermissionError:\n                pass"}, {"filepath": "config_manager.py", "start_line": 14, "end_line": 100, "type": "class", "name": "ConfigManager", "content": "class ConfigManager:\n    \"\"\"Manages the configuration for the CLI.\"\"\"\n    def __init__(self, console: Console):\n        self.console = console\n        self.config_dir = os.path.expanduser(\"~/.config/openaicli\")\n        self.config_file = os.path.join(self.config_dir, \"config.json\")\n        self.api_base: Optional[str] = None\n        self.api_key: Optional[str] = None\n        self.recent_projects: List[str] = []\n\n    def load_config(self) -> None:\n        \"\"\"Loads configuration from file or environment variables.\"\"\"\n        if os.path.exists(self.config_file):\n            with open(self.config_file, 'r') as f:\n                try:\n                    config = json.load(f)\n                    self.api_base = config.get(\"OPENAI_API_BASE\")\n                    self.api_key = config.get(\"OPENAI_API_KEY\")\n                    self.recent_projects = config.get(\"recent_projects\", [])\n                except json.JSONDecodeError as e:\n                    self.console.print(f\"[bold yellow]Warning: Config file corrupted ({e}). Using environment variables or prompting.[/bold yellow]\")\n\n        if not self.api_base:\n            self.api_base = os.environ.get(\"OPENAI_API_BASE\")\n            self.api_key = os.environ.get(\"OPENAI_API_KEY\")\n\n        if not self.api_base:\n            self.console.print(\"[bold red]OpenAI API base URL not found.[/bold red]\")\n            self.api_base = input(\"Please enter your OpenAI API base URL: \")\n            try:\n                self.api_key = getpass.getpass(\"Please enter your OpenAI API key (input hidden, optional): \")\n            except Exception as e:\n                maple_logger.log_error(e, \"API key input\", severity=\"high\")\n                self.console.print(\"[yellow]Warning: Could not hide input. API key will be visible.[/yellow]\")\n                self.api_key = input(\"Please enter your OpenAI API key (optional): \")\n            self.save_config()\n\n    def save_config(self) -> None:\n        \"\"\"Saves API configuration to the config file with enhanced security.\"\"\"\n        try:\n            os.makedirs(self.config_dir, exist_ok=True)\n            \n            config_data = {\n                \"OPENAI_API_BASE\": self.api_base,\n                \"OPENAI_API_KEY\": self.api_key,\n                \"recent_projects\": self.recent_projects[-10:]\n            }\n            \n            temp_file = self.config_file + '.tmp'\n            with open(temp_file, 'w') as f:\n                json.dump(config_data, f, indent=2)\n            \n            if os.path.exists(self.config_file):\n                os.replace(temp_file, self.config_file)\n            else:\n                os.rename(temp_file, self.config_file)\n            \n            self._secure_config_file(self.config_file)\n            \n            self.console.print(f\"Configuration saved to [green]{self.config_file}[/green]\")\n            maple_logger.logger.info(f\"Configuration saved to {self.config_file}\")\n            \n        except Exception as e:\n            maple_logger.log_error(e, \"save_config\", self.config_file, severity=\"high\")\n            self.console.print(f\"[bold red]Error saving configuration: {e}[/bold red]\")\n    \n    def _secure_config_file(self, filepath: str) -> None:\n        \"\"\"Set secure permissions across platforms\"\"\"\n        try:\n            if platform.system() != \"Windows\":\n                os.chmod(filepath, stat.S_IRUSR | stat.S_IWUSR)\n            else:\n                import ctypes\n                try:\n                    ctypes.windll.kernel32.SetFileAttributesW(filepath, 0x2)\n                except:\n                    pass\n        except Exception as e:\n            maple_logger.log_error(e, \"secure_config_file\", filepath, severity=\"medium\")\n    \n    def add_recent_project(self, project_path: str) -> None:\n        \"\"\"Adds a project to the recent projects list.\"\"\"\n        abs_path = os.path.abspath(project_path)\n        if abs_path in self.recent_projects:\n            self.recent_projects.remove(abs_path)\n        self.recent_projects.append(abs_path)\n        self.save_config()", "docstring": "Manages the configuration for the CLI.", "searchable_text": "ConfigManager Manages the configuration for the CLI. class ConfigManager:\n    \"\"\"Manages the configuration for the CLI.\"\"\"\n    def __init__(self, console: Console):\n        self.console = console\n        self.config_dir = os.path.expanduser(\"~/.config/openaicli\")\n        self.config_file = os.path.join(self.config_dir, \"config.json\")\n        self.api_base: Optional[str] = None\n        self.api_key: Optional[str] = None\n        self.recent_projects: List[str] = []\n\n    def load_config(self) -> None:\n        \"\"\"Loads configuration from file or environment variables.\"\"\"\n        if os.path.exists(self.config_file):\n            with open(self.config_file, 'r') as f:\n                try:\n                    config = json.load(f)\n                    self.api_base = config.get(\"OPENAI_API_BASE\")\n                    self.api_key = config.get(\"OPENAI_API_KEY\")\n                    self.recent_projects = config.get(\"recent_projects\", [])\n                except json.JSONDecodeError as e:\n                    self.console.print(f\"[bold yellow]Warning: Config file corrupted ({e}). Using environment variables or prompting.[/bold yellow]\")\n\n        if not self.api_base:\n            self.api_base = os.environ.get(\"OPENAI_API_BASE\")\n            self.api_key = os.environ.get(\"OPENAI_API_KEY\")\n\n        if not self.api_base:\n            self.console.print(\"[bold red]OpenAI API base URL not found.[/bold red]\")\n            self.api_base = input(\"Please enter your OpenAI API base URL: \")\n            try:\n                self.api_key = getpass.getpass(\"Please enter your OpenAI API key (input hidden, optional): \")\n            except Exception as e:\n                maple_logger.log_error(e, \"API key input\", severity=\"high\")\n                self.console.print(\"[yellow]Warning: Could not hide input. API key will be visible.[/yellow]\")\n                self.api_key = input(\"Please enter your OpenAI API key (optional): \")\n            self.save_config()\n\n    def save_config(self) -> None:\n        \"\"\"Saves API configuration to the config file with enhanced security.\"\"\"\n        try:\n            os.makedirs(self.config_dir, exist_ok=True)\n            \n            config_data = {\n                \"OPENAI_API_BASE\": self.api_base,\n                \"OPENAI_API_KEY\": self.api_key,\n                \"recent_projects\": self.recent_projects[-10:]\n            }\n            \n            temp_file = self.config_file + '.tmp'\n            with open(temp_file, 'w') as f:\n                json.dump(config_data, f, indent=2)\n            \n            if os.path.exists(self.config_file):\n                os.replace(temp_file, self.config_file)\n            else:\n                os.rename(temp_file, self.config_file)\n            \n            self._secure_config_file(self.config_file)\n            \n            self.console.print(f\"Configuration saved to [green]{self.config_file}[/green]\")\n            maple_logger.logger.info(f\"Configuration saved to {self.config_file}\")\n            \n        except Exception as e:\n            maple_logger.log_error(e, \"save_config\", self.config_file, severity=\"high\")\n            self.console.print(f\"[bold red]Error saving configuration: {e}[/bold red]\")\n    \n    def _secure_config_file(self, filepath: str) -> None:\n        \"\"\"Set secure permissions across platforms\"\"\"\n        try:\n            if platform.system() != \"Windows\":\n                os.chmod(filepath, stat.S_IRUSR | stat.S_IWUSR)\n            else:\n                import ctypes\n                try:\n                    ctypes.windll.kernel32.SetFileAttributesW(filepath, 0x2)\n                except:\n                    pass\n        except Exception as e:\n            maple_logger.log_error(e, \"secure_config_file\", filepath, severity=\"medium\")\n    \n    def add_recent_project(self, project_path: str) -> None:\n        \"\"\"Adds a project to the recent projects list.\"\"\"\n        abs_path = os.path.abspath(project_path)\n        if abs_path in self.recent_projects:\n            self.recent_projects.remove(abs_path)\n        self.recent_projects.append(abs_path)\n        self.save_config()"}, {"filepath": "config_manager.py", "start_line": 16, "end_line": 22, "type": "function", "name": "__init__", "content": "    def __init__(self, console: Console):\n        self.console = console\n        self.config_dir = os.path.expanduser(\"~/.config/openaicli\")\n        self.config_file = os.path.join(self.config_dir, \"config.json\")\n        self.api_base: Optional[str] = None\n        self.api_key: Optional[str] = None\n        self.recent_projects: List[str] = []", "docstring": "", "searchable_text": "__init__      def __init__(self, console: Console):\n        self.console = console\n        self.config_dir = os.path.expanduser(\"~/.config/openaicli\")\n        self.config_file = os.path.join(self.config_dir, \"config.json\")\n        self.api_base: Optional[str] = None\n        self.api_key: Optional[str] = None\n        self.recent_projects: List[str] = []"}, {"filepath": "config_manager.py", "start_line": 24, "end_line": 49, "type": "function", "name": "load_config", "content": "    def load_config(self) -> None:\n        \"\"\"Loads configuration from file or environment variables.\"\"\"\n        if os.path.exists(self.config_file):\n            with open(self.config_file, 'r') as f:\n                try:\n                    config = json.load(f)\n                    self.api_base = config.get(\"OPENAI_API_BASE\")\n                    self.api_key = config.get(\"OPENAI_API_KEY\")\n                    self.recent_projects = config.get(\"recent_projects\", [])\n                except json.JSONDecodeError as e:\n                    self.console.print(f\"[bold yellow]Warning: Config file corrupted ({e}). Using environment variables or prompting.[/bold yellow]\")\n\n        if not self.api_base:\n            self.api_base = os.environ.get(\"OPENAI_API_BASE\")\n            self.api_key = os.environ.get(\"OPENAI_API_KEY\")\n\n        if not self.api_base:\n            self.console.print(\"[bold red]OpenAI API base URL not found.[/bold red]\")\n            self.api_base = input(\"Please enter your OpenAI API base URL: \")\n            try:\n                self.api_key = getpass.getpass(\"Please enter your OpenAI API key (input hidden, optional): \")\n            except Exception as e:\n                maple_logger.log_error(e, \"API key input\", severity=\"high\")\n                self.console.print(\"[yellow]Warning: Could not hide input. API key will be visible.[/yellow]\")\n                self.api_key = input(\"Please enter your OpenAI API key (optional): \")\n            self.save_config()", "docstring": "Loads configuration from file or environment variables.", "searchable_text": "load_config Loads configuration from file or environment variables.     def load_config(self) -> None:\n        \"\"\"Loads configuration from file or environment variables.\"\"\"\n        if os.path.exists(self.config_file):\n            with open(self.config_file, 'r') as f:\n                try:\n                    config = json.load(f)\n                    self.api_base = config.get(\"OPENAI_API_BASE\")\n                    self.api_key = config.get(\"OPENAI_API_KEY\")\n                    self.recent_projects = config.get(\"recent_projects\", [])\n                except json.JSONDecodeError as e:\n                    self.console.print(f\"[bold yellow]Warning: Config file corrupted ({e}). Using environment variables or prompting.[/bold yellow]\")\n\n        if not self.api_base:\n            self.api_base = os.environ.get(\"OPENAI_API_BASE\")\n            self.api_key = os.environ.get(\"OPENAI_API_KEY\")\n\n        if not self.api_base:\n            self.console.print(\"[bold red]OpenAI API base URL not found.[/bold red]\")\n            self.api_base = input(\"Please enter your OpenAI API base URL: \")\n            try:\n                self.api_key = getpass.getpass(\"Please enter your OpenAI API key (input hidden, optional): \")\n            except Exception as e:\n                maple_logger.log_error(e, \"API key input\", severity=\"high\")\n                self.console.print(\"[yellow]Warning: Could not hide input. API key will be visible.[/yellow]\")\n                self.api_key = input(\"Please enter your OpenAI API key (optional): \")\n            self.save_config()"}, {"filepath": "config_manager.py", "start_line": 51, "end_line": 78, "type": "function", "name": "save_config", "content": "    def save_config(self) -> None:\n        \"\"\"Saves API configuration to the config file with enhanced security.\"\"\"\n        try:\n            os.makedirs(self.config_dir, exist_ok=True)\n            \n            config_data = {\n                \"OPENAI_API_BASE\": self.api_base,\n                \"OPENAI_API_KEY\": self.api_key,\n                \"recent_projects\": self.recent_projects[-10:]\n            }\n            \n            temp_file = self.config_file + '.tmp'\n            with open(temp_file, 'w') as f:\n                json.dump(config_data, f, indent=2)\n            \n            if os.path.exists(self.config_file):\n                os.replace(temp_file, self.config_file)\n            else:\n                os.rename(temp_file, self.config_file)\n            \n            self._secure_config_file(self.config_file)\n            \n            self.console.print(f\"Configuration saved to [green]{self.config_file}[/green]\")\n            maple_logger.logger.info(f\"Configuration saved to {self.config_file}\")\n            \n        except Exception as e:\n            maple_logger.log_error(e, \"save_config\", self.config_file, severity=\"high\")\n            self.console.print(f\"[bold red]Error saving configuration: {e}[/bold red]\")", "docstring": "Saves API configuration to the config file with enhanced security.", "searchable_text": "save_config Saves API configuration to the config file with enhanced security.     def save_config(self) -> None:\n        \"\"\"Saves API configuration to the config file with enhanced security.\"\"\"\n        try:\n            os.makedirs(self.config_dir, exist_ok=True)\n            \n            config_data = {\n                \"OPENAI_API_BASE\": self.api_base,\n                \"OPENAI_API_KEY\": self.api_key,\n                \"recent_projects\": self.recent_projects[-10:]\n            }\n            \n            temp_file = self.config_file + '.tmp'\n            with open(temp_file, 'w') as f:\n                json.dump(config_data, f, indent=2)\n            \n            if os.path.exists(self.config_file):\n                os.replace(temp_file, self.config_file)\n            else:\n                os.rename(temp_file, self.config_file)\n            \n            self._secure_config_file(self.config_file)\n            \n            self.console.print(f\"Configuration saved to [green]{self.config_file}[/green]\")\n            maple_logger.logger.info(f\"Configuration saved to {self.config_file}\")\n            \n        except Exception as e:\n            maple_logger.log_error(e, \"save_config\", self.config_file, severity=\"high\")\n            self.console.print(f\"[bold red]Error saving configuration: {e}[/bold red]\")"}, {"filepath": "config_manager.py", "start_line": 80, "end_line": 92, "type": "function", "name": "_secure_config_file", "content": "    def _secure_config_file(self, filepath: str) -> None:\n        \"\"\"Set secure permissions across platforms\"\"\"\n        try:\n            if platform.system() != \"Windows\":\n                os.chmod(filepath, stat.S_IRUSR | stat.S_IWUSR)\n            else:\n                import ctypes\n                try:\n                    ctypes.windll.kernel32.SetFileAttributesW(filepath, 0x2)\n                except:\n                    pass\n        except Exception as e:\n            maple_logger.log_error(e, \"secure_config_file\", filepath, severity=\"medium\")", "docstring": "Set secure permissions across platforms", "searchable_text": "_secure_config_file Set secure permissions across platforms     def _secure_config_file(self, filepath: str) -> None:\n        \"\"\"Set secure permissions across platforms\"\"\"\n        try:\n            if platform.system() != \"Windows\":\n                os.chmod(filepath, stat.S_IRUSR | stat.S_IWUSR)\n            else:\n                import ctypes\n                try:\n                    ctypes.windll.kernel32.SetFileAttributesW(filepath, 0x2)\n                except:\n                    pass\n        except Exception as e:\n            maple_logger.log_error(e, \"secure_config_file\", filepath, severity=\"medium\")"}, {"filepath": "config_manager.py", "start_line": 94, "end_line": 100, "type": "function", "name": "add_recent_project", "content": "    def add_recent_project(self, project_path: str) -> None:\n        \"\"\"Adds a project to the recent projects list.\"\"\"\n        abs_path = os.path.abspath(project_path)\n        if abs_path in self.recent_projects:\n            self.recent_projects.remove(abs_path)\n        self.recent_projects.append(abs_path)\n        self.save_config()", "docstring": "Adds a project to the recent projects list.", "searchable_text": "add_recent_project Adds a project to the recent projects list.     def add_recent_project(self, project_path: str) -> None:\n        \"\"\"Adds a project to the recent projects list.\"\"\"\n        abs_path = os.path.abspath(project_path)\n        if abs_path in self.recent_projects:\n            self.recent_projects.remove(abs_path)\n        self.recent_projects.append(abs_path)\n        self.save_config()"}, {"filepath": "context_engine.py", "start_line": 25, "end_line": 394, "type": "class", "name": "ContextEngine", "content": "class ContextEngine:\n    \"\"\"\n    Intelligent code retrieval using semantic search.\n    Similar to Augment's context engine.\n    \"\"\"\n    \n    def __init__(self, project_path: str, console=None):\n        self.project_path = project_path\n        self.console = console\n        self.model = None\n        self.index = None\n        self.chunks = []  # Store code chunks\n        self.metadata = []  # Store file paths, line numbers, etc.\n        self.cache_dir = os.path.join(project_path, '.maplecli_cache')\n        self.index_cache_file = os.path.join(self.cache_dir, 'index.faiss')\n        self.metadata_cache_file = os.path.join(self.cache_dir, 'metadata.json')\n        self.state_cache_file = os.path.join(self.cache_dir, 'state.json')\n        \n        if EMBEDDINGS_AVAILABLE:\n            try:\n                self.model = SentenceTransformer('all-MiniLM-L6-v2')  # Fast, good quality\n            except Exception as e:\n                logger.error(f\"Failed to load embedding model: {e}\")\n                self.model = None\n        \n    def chunk_code(self, filepath: str, content: str) -> List[Dict]:\n        \"\"\"\n        Chunk code into semantic units (functions, classes, blocks).\n        \"\"\"\n        chunks = []\n        ext = os.path.splitext(filepath)[1].lower()\n        \n        if ext == '.py':\n            chunks = self._chunk_python(filepath, content)\n        elif ext in ['.js', '.ts', '.jsx', '.tsx']:\n            chunks = self._chunk_javascript(filepath, content)\n        elif ext == '.java':\n            chunks = self._chunk_java(filepath, content)\n        else:\n            # Fallback: chunk by lines (every 50 lines)\n            chunks = self._chunk_by_lines(filepath, content, chunk_size=50)\n        \n        return chunks\n    \n    def _chunk_python(self, filepath: str, content: str) -> List[Dict]:\n        \"\"\"Extract Python functions and classes as chunks.\"\"\"\n        import ast\n        chunks = []\n        \n        try:\n            tree = ast.parse(content)\n            lines = content.split('\\n')\n            \n            for node in ast.walk(tree):\n                if isinstance(node, (ast.FunctionDef, ast.ClassDef, ast.AsyncFunctionDef)):\n                    start_line = node.lineno - 1\n                    end_line = node.end_lineno if hasattr(node, 'end_lineno') else start_line + 20\n                    \n                    chunk_content = '\\n'.join(lines[start_line:end_line])\n                    \n                    # Get docstring if available\n                    docstring = ast.get_docstring(node) or \"\"\n                    \n                    chunks.append({\n                        'filepath': filepath,\n                        'start_line': start_line + 1,\n                        'end_line': end_line,\n                        'type': 'function' if isinstance(node, (ast.FunctionDef, ast.AsyncFunctionDef)) else 'class',\n                        'name': node.name,\n                        'content': chunk_content,\n                        'docstring': docstring,\n                        'searchable_text': f\"{node.name} {docstring} {chunk_content}\"\n                    })\n        except Exception as e:\n            logger.debug(f\"Failed to parse Python file {filepath}: {e}\")\n            # Fallback to line-based chunking\n            return self._chunk_by_lines(filepath, content)\n        \n        if not chunks:\n            return self._chunk_by_lines(filepath, content)\n        \n        return chunks\n    \n    def _chunk_javascript(self, filepath: str, content: str) -> List[Dict]:\n        \"\"\"Extract JavaScript/TypeScript functions and classes using AST if available.\"\"\"\n        try:\n            import esprima\n            tree = esprima.parseModule(content, {'loc': True})\n            lines = content.split('\\n')\n            chunks = []\n\n            for node in tree.body:\n                if node.type in ['FunctionDeclaration', 'ClassDeclaration'] and node.id:\n                    start_line = node.loc.start.line - 1\n                    end_line = node.loc.end.line\n                    chunk_content = '\\n'.join(lines[start_line:end_line])\n                    chunks.append({\n                        'filepath': filepath, 'start_line': start_line + 1, 'end_line': end_line,\n                        'type': 'function' if node.type == 'FunctionDeclaration' else 'class',\n                        'name': node.id.name, 'content': chunk_content,\n                        'searchable_text': f\"{node.id.name} {chunk_content}\"\n                    })\n            if chunks: return chunks\n        except (ImportError, Exception):\n            pass  # Fallback to regex\n        \n        # Fallback regex implementation\n        import re\n        chunks = []\n        lines = content.split('\\n')\n        patterns = [\n            (r'function\\s+(\\w+)\\s*\\([^)]*\\)\\s*{', 'function'),\n            (r'const\\s+(\\w+)\\s*=\\s*\\([^)]*\\)\\s*=>', 'function'),\n            (r'class\\s+(\\w+)\\s*{', 'class'),\n        ]\n        for i, line in enumerate(lines):\n            for pattern, chunk_type in patterns:\n                match = re.search(pattern, line)\n                if match:\n                    name = match.group(1) if match.groups() else 'anonymous'\n                    chunk_lines = lines[i:min(i + 40, len(lines))]\n                    chunks.append({\n                        'filepath': filepath, 'start_line': i + 1, 'end_line': min(i + 40, len(lines)),\n                        'type': chunk_type, 'name': name, 'content': '\\n'.join(chunk_lines),\n                        'searchable_text': f\"{name} {' '.join(chunk_lines)}\"\n                    })\n                    break\n        return chunks if chunks else self._chunk_by_lines(filepath, content)\n    \n    def _chunk_java(self, filepath: str, content: str) -> List[Dict]:\n        \"\"\"Extract Java methods and classes using AST if available.\"\"\"\n        try:\n            import javalang\n            tree = javalang.parse.parse(content)\n            lines = content.split('\\n')\n            chunks = []\n\n            for path, node in tree:\n                if isinstance(node, (javalang.tree.ClassDeclaration, javalang.tree.MethodDeclaration)):\n                    start_line = node.position.line - 1\n                    # Estimate end line as javalang doesn't provide it\n                    end_line = start_line + len(str(node).split('\\n')) + 5\n                    chunk_content = '\\n'.join(lines[start_line:min(end_line, len(lines))])\n                    chunks.append({\n                        'filepath': filepath, 'start_line': start_line + 1, 'end_line': min(end_line, len(lines)),\n                        'type': 'class' if isinstance(node, javalang.tree.ClassDeclaration) else 'method',\n                        'name': node.name, 'content': chunk_content,\n                        'searchable_text': f\"{node.name} {chunk_content}\"\n                    })\n            if chunks: return chunks\n        except (ImportError, Exception):\n            pass # Fallback to regex\n\n        # Fallback regex implementation\n        import re\n        chunks = []\n        lines = content.split('\\n')\n        patterns = [\n            (r'class\\s+(\\w+)', 'class'),\n            (r'(?:public|private|protected)?\\s*(?:static\\s+)?[\\w<>\\[\\]]+\\s+(\\w+)\\s*\\([^)]*\\)\\s*{', 'method'),\n        ]\n        for i, line in enumerate(lines):\n            for pattern, chunk_type in patterns:\n                match = re.search(pattern, line)\n                if match:\n                    name = match.group(1)\n                    chunk_lines = lines[i:min(i + 40, len(lines))]\n                    chunks.append({\n                        'filepath': filepath, 'start_line': i + 1, 'end_line': min(i + 40, len(lines)),\n                        'type': chunk_type, 'name': name, 'content': '\\n'.join(chunk_lines),\n                        'searchable_text': f\"{name} {' '.join(chunk_lines)}\"\n                    })\n                    break\n        return chunks if chunks else self._chunk_by_lines(filepath, content)\n    \n    def _chunk_by_lines(self, filepath: str, content: str, chunk_size: int = 50) -> List[Dict]:\n        \"\"\"Fallback: chunk by fixed line count.\"\"\"\n        lines = content.split('\\n')\n        chunks = []\n        \n        for i in range(0, len(lines), chunk_size):\n            chunk_lines = lines[i:i+chunk_size]\n            chunks.append({\n                'filepath': filepath,\n                'start_line': i + 1,\n                'end_line': min(i + chunk_size, len(lines)),\n                'type': 'block',\n                'name': f'block_{i}',\n                'content': '\\n'.join(chunk_lines),\n                'searchable_text': '\\n'.join(chunk_lines)\n            })\n        \n        return chunks\n    \n    def build_index(self, code_analyzer) -> bool:\n        \"\"\"\n        Build FAISS index from all code chunks.\n        Returns True if successful, False otherwise.\n        \"\"\"\n        if not EMBEDDINGS_AVAILABLE or not self.model:\n            if self.console:\n                self.console.print(\"[yellow]\u26a0\ufe0f  Semantic search not available. Install with: pip install sentence-transformers faiss-cpu[/yellow]\")\n            return False\n        \n        # Check if cached index exists and is fresh\n        if self._load_cached_index(code_analyzer):\n            if self.console:\n                self.console.print(f\"[green]\u2713 Loaded fresh cache ({len(self.chunks)} chunks)[/green]\")\n            return True\n        \n        all_chunks = []\n        all_texts = []\n        \n        if self.console:\n            from rich.progress import Progress, SpinnerColumn, TextColumn, BarColumn\n            \n            with Progress(\n                SpinnerColumn(),\n                TextColumn(\"[progress.description]{task.description}\"),\n                BarColumn(),\n                console=self.console\n            ) as progress:\n                task = progress.add_task(\"[cyan]Building semantic index...\", total=len(code_analyzer.files))\n                \n                for filepath in code_analyzer.files:\n                    try:\n                        content = code_analyzer.read_file_content(filepath, max_lines=10000)\n                        if content and not content.startswith(\"Error\"):\n                            chunks = self.chunk_code(filepath, content)\n                            all_chunks.extend(chunks)\n                            all_texts.extend([c['searchable_text'] for c in chunks])\n                    except Exception as e:\n                        logger.debug(f\"Error chunking {filepath}: {e}\")\n                    progress.advance(task)\n        else:\n            for filepath in code_analyzer.files:\n                try:\n                    content = code_analyzer.read_file_content(filepath, max_lines=10000)\n                    if content and not content.startswith(\"Error\"):\n                        chunks = self.chunk_code(filepath, content)\n                        all_chunks.extend(chunks)\n                        all_texts.extend([c['searchable_text'] for c in chunks])\n                except Exception as e:\n                    logger.debug(f\"Error chunking {filepath}: {e}\")\n        \n        if not all_texts:\n            if self.console:\n                self.console.print(\"[yellow]\u26a0\ufe0f  No code chunks to index[/yellow]\")\n            return False\n        \n        # Generate embeddings\n        if self.console:\n            self.console.print(f\"[cyan]Generating embeddings for {len(all_texts)} code chunks...[/cyan]\")\n        \n        try:\n            embeddings = self.model.encode(all_texts, show_progress_bar=False, batch_size=32)\n        except Exception as e:\n            logger.error(f\"Failed to generate embeddings: {e}\")\n            return False\n        \n        # Build FAISS index\n        dimension = embeddings.shape[1]\n        self.index = faiss.IndexFlatL2(dimension)\n        self.index.add(embeddings.astype('float32'))\n        \n        self.chunks = all_chunks\n        self.metadata = [{'filepath': c['filepath'], 'start_line': c['start_line']} for c in all_chunks]\n        \n        # Cache the index\n        self._save_index(code_analyzer)\n        \n        if self.console:\n            self.console.print(f\"[green]\u2713 Index built: {len(all_chunks)} chunks indexed[/green]\")\n        \n        return True\n    \n    def search(self, query: str, top_k: int = 5) -> List[Dict]:\n        \"\"\"\n        Search for relevant code chunks using semantic similarity.\n        \"\"\"\n        if self.index is None or not EMBEDDINGS_AVAILABLE or not self.model:\n            return []\n        \n        try:\n            # Encode query\n            query_embedding = self.model.encode([query])\n            \n            # Search\n            distances, indices = self.index.search(query_embedding.astype('float32'), min(top_k, len(self.chunks)))\n            \n            # Return results with scores\n            results = []\n            for i, idx in enumerate(indices[0]):\n                if idx < len(self.chunks) and idx >= 0:\n                    chunk = self.chunks[idx].copy()\n                    chunk['relevance_score'] = float(1 / (1 + distances[0][i]))  # Convert distance to similarity\n                    results.append(chunk)\n            \n            return results\n        except Exception as e:\n            logger.error(f\"Search failed: {e}\")\n            return []\n    \n    def _get_project_state_hash(self, code_analyzer) -> str:\n        \"\"\"Generates a hash representing the current state of project files.\"\"\"\n        try:\n            file_metadata = []\n            for f in code_analyzer.files:\n                full_path = os.path.join(self.project_path, f)\n                if os.path.exists(full_path):\n                    mtime = os.path.getmtime(full_path)\n                    file_metadata.append(f\"{f}:{mtime}\")\n            \n            state_string = \"\".join(sorted(file_metadata))\n            return hashlib.md5(state_string.encode()).hexdigest()\n        except Exception as e:\n            logger.debug(f\"Could not generate project state hash: {e}\")\n            return \"\"\n\n    def _save_index(self, code_analyzer):\n        \"\"\"Save index and metadata to disk using a secure format.\"\"\"\n        try:\n            os.makedirs(self.cache_dir, exist_ok=True)\n            \n            # Save FAISS index\n            if self.index:\n                faiss.write_index(self.index, self.index_cache_file)\n\n            # Save metadata as JSON\n            import json\n            with open(self.metadata_cache_file, 'w') as f:\n                json.dump({'chunks': self.chunks, 'metadata': self.metadata}, f)\n\n            # Save project state hash\n            state_hash = self._get_project_state_hash(code_analyzer)\n            with open(self.state_cache_file, 'w') as f:\n                json.dump({'hash': state_hash}, f)\n\n        except Exception as e:\n            logger.debug(f\"Failed to save index cache: {e}\")\n    \n    def _load_cached_index(self, code_analyzer) -> bool:\n        \"\"\"Load cached index if available and fresh.\"\"\"\n        if not all(os.path.exists(p) for p in [self.index_cache_file, self.metadata_cache_file, self.state_cache_file]):\n            return False\n        \n        try:\n            # Check if cache is stale\n            import json\n            with open(self.state_cache_file, 'r') as f:\n                stored_state = json.load(f)\n            \n            current_hash = self._get_project_state_hash(code_analyzer)\n            \n            if stored_state.get('hash') != current_hash:\n                logger.info(\"Project has changed, rebuilding index.\")\n                return False\n\n            # Load index and metadata\n            self.index = faiss.read_index(self.index_cache_file)\n            with open(self.metadata_cache_file, 'r') as f:\n                metadata = json.load(f)\n            \n            self.chunks = metadata['chunks']\n            self.metadata = metadata['metadata']\n            \n            return True\n        except Exception as e:\n            logger.debug(f\"Failed to load cached index: {e}\")\n            return False", "docstring": "Intelligent code retrieval using semantic search.\nSimilar to Augment's context engine.", "searchable_text": "ContextEngine Intelligent code retrieval using semantic search.\nSimilar to Augment's context engine. class ContextEngine:\n    \"\"\"\n    Intelligent code retrieval using semantic search.\n    Similar to Augment's context engine.\n    \"\"\"\n    \n    def __init__(self, project_path: str, console=None):\n        self.project_path = project_path\n        self.console = console\n        self.model = None\n        self.index = None\n        self.chunks = []  # Store code chunks\n        self.metadata = []  # Store file paths, line numbers, etc.\n        self.cache_dir = os.path.join(project_path, '.maplecli_cache')\n        self.index_cache_file = os.path.join(self.cache_dir, 'index.faiss')\n        self.metadata_cache_file = os.path.join(self.cache_dir, 'metadata.json')\n        self.state_cache_file = os.path.join(self.cache_dir, 'state.json')\n        \n        if EMBEDDINGS_AVAILABLE:\n            try:\n                self.model = SentenceTransformer('all-MiniLM-L6-v2')  # Fast, good quality\n            except Exception as e:\n                logger.error(f\"Failed to load embedding model: {e}\")\n                self.model = None\n        \n    def chunk_code(self, filepath: str, content: str) -> List[Dict]:\n        \"\"\"\n        Chunk code into semantic units (functions, classes, blocks).\n        \"\"\"\n        chunks = []\n        ext = os.path.splitext(filepath)[1].lower()\n        \n        if ext == '.py':\n            chunks = self._chunk_python(filepath, content)\n        elif ext in ['.js', '.ts', '.jsx', '.tsx']:\n            chunks = self._chunk_javascript(filepath, content)\n        elif ext == '.java':\n            chunks = self._chunk_java(filepath, content)\n        else:\n            # Fallback: chunk by lines (every 50 lines)\n            chunks = self._chunk_by_lines(filepath, content, chunk_size=50)\n        \n        return chunks\n    \n    def _chunk_python(self, filepath: str, content: str) -> List[Dict]:\n        \"\"\"Extract Python functions and classes as chunks.\"\"\"\n        import ast\n        chunks = []\n        \n        try:\n            tree = ast.parse(content)\n            lines = content.split('\\n')\n            \n            for node in ast.walk(tree):\n                if isinstance(node, (ast.FunctionDef, ast.ClassDef, ast.AsyncFunctionDef)):\n                    start_line = node.lineno - 1\n                    end_line = node.end_lineno if hasattr(node, 'end_lineno') else start_line + 20\n                    \n                    chunk_content = '\\n'.join(lines[start_line:end_line])\n                    \n                    # Get docstring if available\n                    docstring = ast.get_docstring(node) or \"\"\n                    \n                    chunks.append({\n                        'filepath': filepath,\n                        'start_line': start_line + 1,\n                        'end_line': end_line,\n                        'type': 'function' if isinstance(node, (ast.FunctionDef, ast.AsyncFunctionDef)) else 'class',\n                        'name': node.name,\n                        'content': chunk_content,\n                        'docstring': docstring,\n                        'searchable_text': f\"{node.name} {docstring} {chunk_content}\"\n                    })\n        except Exception as e:\n            logger.debug(f\"Failed to parse Python file {filepath}: {e}\")\n            # Fallback to line-based chunking\n            return self._chunk_by_lines(filepath, content)\n        \n        if not chunks:\n            return self._chunk_by_lines(filepath, content)\n        \n        return chunks\n    \n    def _chunk_javascript(self, filepath: str, content: str) -> List[Dict]:\n        \"\"\"Extract JavaScript/TypeScript functions and classes using AST if available.\"\"\"\n        try:\n            import esprima\n            tree = esprima.parseModule(content, {'loc': True})\n            lines = content.split('\\n')\n            chunks = []\n\n            for node in tree.body:\n                if node.type in ['FunctionDeclaration', 'ClassDeclaration'] and node.id:\n                    start_line = node.loc.start.line - 1\n                    end_line = node.loc.end.line\n                    chunk_content = '\\n'.join(lines[start_line:end_line])\n                    chunks.append({\n                        'filepath': filepath, 'start_line': start_line + 1, 'end_line': end_line,\n                        'type': 'function' if node.type == 'FunctionDeclaration' else 'class',\n                        'name': node.id.name, 'content': chunk_content,\n                        'searchable_text': f\"{node.id.name} {chunk_content}\"\n                    })\n            if chunks: return chunks\n        except (ImportError, Exception):\n            pass  # Fallback to regex\n        \n        # Fallback regex implementation\n        import re\n        chunks = []\n        lines = content.split('\\n')\n        patterns = [\n            (r'function\\s+(\\w+)\\s*\\([^)]*\\)\\s*{', 'function'),\n            (r'const\\s+(\\w+)\\s*=\\s*\\([^)]*\\)\\s*=>', 'function'),\n            (r'class\\s+(\\w+)\\s*{', 'class'),\n        ]\n        for i, line in enumerate(lines):\n            for pattern, chunk_type in patterns:\n                match = re.search(pattern, line)\n                if match:\n                    name = match.group(1) if match.groups() else 'anonymous'\n                    chunk_lines = lines[i:min(i + 40, len(lines))]\n                    chunks.append({\n                        'filepath': filepath, 'start_line': i + 1, 'end_line': min(i + 40, len(lines)),\n                        'type': chunk_type, 'name': name, 'content': '\\n'.join(chunk_lines),\n                        'searchable_text': f\"{name} {' '.join(chunk_lines)}\"\n                    })\n                    break\n        return chunks if chunks else self._chunk_by_lines(filepath, content)\n    \n    def _chunk_java(self, filepath: str, content: str) -> List[Dict]:\n        \"\"\"Extract Java methods and classes using AST if available.\"\"\"\n        try:\n            import javalang\n            tree = javalang.parse.parse(content)\n            lines = content.split('\\n')\n            chunks = []\n\n            for path, node in tree:\n                if isinstance(node, (javalang.tree.ClassDeclaration, javalang.tree.MethodDeclaration)):\n                    start_line = node.position.line - 1\n                    # Estimate end line as javalang doesn't provide it\n                    end_line = start_line + len(str(node).split('\\n')) + 5\n                    chunk_content = '\\n'.join(lines[start_line:min(end_line, len(lines))])\n                    chunks.append({\n                        'filepath': filepath, 'start_line': start_line + 1, 'end_line': min(end_line, len(lines)),\n                        'type': 'class' if isinstance(node, javalang.tree.ClassDeclaration) else 'method',\n                        'name': node.name, 'content': chunk_content,\n                        'searchable_text': f\"{node.name} {chunk_content}\"\n                    })\n            if chunks: return chunks\n        except (ImportError, Exception):\n            pass # Fallback to regex\n\n        # Fallback regex implementation\n        import re\n        chunks = []\n        lines = content.split('\\n')\n        patterns = [\n            (r'class\\s+(\\w+)', 'class'),\n            (r'(?:public|private|protected)?\\s*(?:static\\s+)?[\\w<>\\[\\]]+\\s+(\\w+)\\s*\\([^)]*\\)\\s*{', 'method'),\n        ]\n        for i, line in enumerate(lines):\n            for pattern, chunk_type in patterns:\n                match = re.search(pattern, line)\n                if match:\n                    name = match.group(1)\n                    chunk_lines = lines[i:min(i + 40, len(lines))]\n                    chunks.append({\n                        'filepath': filepath, 'start_line': i + 1, 'end_line': min(i + 40, len(lines)),\n                        'type': chunk_type, 'name': name, 'content': '\\n'.join(chunk_lines),\n                        'searchable_text': f\"{name} {' '.join(chunk_lines)}\"\n                    })\n                    break\n        return chunks if chunks else self._chunk_by_lines(filepath, content)\n    \n    def _chunk_by_lines(self, filepath: str, content: str, chunk_size: int = 50) -> List[Dict]:\n        \"\"\"Fallback: chunk by fixed line count.\"\"\"\n        lines = content.split('\\n')\n        chunks = []\n        \n        for i in range(0, len(lines), chunk_size):\n            chunk_lines = lines[i:i+chunk_size]\n            chunks.append({\n                'filepath': filepath,\n                'start_line': i + 1,\n                'end_line': min(i + chunk_size, len(lines)),\n                'type': 'block',\n                'name': f'block_{i}',\n                'content': '\\n'.join(chunk_lines),\n                'searchable_text': '\\n'.join(chunk_lines)\n            })\n        \n        return chunks\n    \n    def build_index(self, code_analyzer) -> bool:\n        \"\"\"\n        Build FAISS index from all code chunks.\n        Returns True if successful, False otherwise.\n        \"\"\"\n        if not EMBEDDINGS_AVAILABLE or not self.model:\n            if self.console:\n                self.console.print(\"[yellow]\u26a0\ufe0f  Semantic search not available. Install with: pip install sentence-transformers faiss-cpu[/yellow]\")\n            return False\n        \n        # Check if cached index exists and is fresh\n        if self._load_cached_index(code_analyzer):\n            if self.console:\n                self.console.print(f\"[green]\u2713 Loaded fresh cache ({len(self.chunks)} chunks)[/green]\")\n            return True\n        \n        all_chunks = []\n        all_texts = []\n        \n        if self.console:\n            from rich.progress import Progress, SpinnerColumn, TextColumn, BarColumn\n            \n            with Progress(\n                SpinnerColumn(),\n                TextColumn(\"[progress.description]{task.description}\"),\n                BarColumn(),\n                console=self.console\n            ) as progress:\n                task = progress.add_task(\"[cyan]Building semantic index...\", total=len(code_analyzer.files))\n                \n                for filepath in code_analyzer.files:\n                    try:\n                        content = code_analyzer.read_file_content(filepath, max_lines=10000)\n                        if content and not content.startswith(\"Error\"):\n                            chunks = self.chunk_code(filepath, content)\n                            all_chunks.extend(chunks)\n                            all_texts.extend([c['searchable_text'] for c in chunks])\n                    except Exception as e:\n                        logger.debug(f\"Error chunking {filepath}: {e}\")\n                    progress.advance(task)\n        else:\n            for filepath in code_analyzer.files:\n                try:\n                    content = code_analyzer.read_file_content(filepath, max_lines=10000)\n                    if content and not content.startswith(\"Error\"):\n                        chunks = self.chunk_code(filepath, content)\n                        all_chunks.extend(chunks)\n                        all_texts.extend([c['searchable_text'] for c in chunks])\n                except Exception as e:\n                    logger.debug(f\"Error chunking {filepath}: {e}\")\n        \n        if not all_texts:\n            if self.console:\n                self.console.print(\"[yellow]\u26a0\ufe0f  No code chunks to index[/yellow]\")\n            return False\n        \n        # Generate embeddings\n        if self.console:\n            self.console.print(f\"[cyan]Generating embeddings for {len(all_texts)} code chunks...[/cyan]\")\n        \n        try:\n            embeddings = self.model.encode(all_texts, show_progress_bar=False, batch_size=32)\n        except Exception as e:\n            logger.error(f\"Failed to generate embeddings: {e}\")\n            return False\n        \n        # Build FAISS index\n        dimension = embeddings.shape[1]\n        self.index = faiss.IndexFlatL2(dimension)\n        self.index.add(embeddings.astype('float32'))\n        \n        self.chunks = all_chunks\n        self.metadata = [{'filepath': c['filepath'], 'start_line': c['start_line']} for c in all_chunks]\n        \n        # Cache the index\n        self._save_index(code_analyzer)\n        \n        if self.console:\n            self.console.print(f\"[green]\u2713 Index built: {len(all_chunks)} chunks indexed[/green]\")\n        \n        return True\n    \n    def search(self, query: str, top_k: int = 5) -> List[Dict]:\n        \"\"\"\n        Search for relevant code chunks using semantic similarity.\n        \"\"\"\n        if self.index is None or not EMBEDDINGS_AVAILABLE or not self.model:\n            return []\n        \n        try:\n            # Encode query\n            query_embedding = self.model.encode([query])\n            \n            # Search\n            distances, indices = self.index.search(query_embedding.astype('float32'), min(top_k, len(self.chunks)))\n            \n            # Return results with scores\n            results = []\n            for i, idx in enumerate(indices[0]):\n                if idx < len(self.chunks) and idx >= 0:\n                    chunk = self.chunks[idx].copy()\n                    chunk['relevance_score'] = float(1 / (1 + distances[0][i]))  # Convert distance to similarity\n                    results.append(chunk)\n            \n            return results\n        except Exception as e:\n            logger.error(f\"Search failed: {e}\")\n            return []\n    \n    def _get_project_state_hash(self, code_analyzer) -> str:\n        \"\"\"Generates a hash representing the current state of project files.\"\"\"\n        try:\n            file_metadata = []\n            for f in code_analyzer.files:\n                full_path = os.path.join(self.project_path, f)\n                if os.path.exists(full_path):\n                    mtime = os.path.getmtime(full_path)\n                    file_metadata.append(f\"{f}:{mtime}\")\n            \n            state_string = \"\".join(sorted(file_metadata))\n            return hashlib.md5(state_string.encode()).hexdigest()\n        except Exception as e:\n            logger.debug(f\"Could not generate project state hash: {e}\")\n            return \"\"\n\n    def _save_index(self, code_analyzer):\n        \"\"\"Save index and metadata to disk using a secure format.\"\"\"\n        try:\n            os.makedirs(self.cache_dir, exist_ok=True)\n            \n            # Save FAISS index\n            if self.index:\n                faiss.write_index(self.index, self.index_cache_file)\n\n            # Save metadata as JSON\n            import json\n            with open(self.metadata_cache_file, 'w') as f:\n                json.dump({'chunks': self.chunks, 'metadata': self.metadata}, f)\n\n            # Save project state hash\n            state_hash = self._get_project_state_hash(code_analyzer)\n            with open(self.state_cache_file, 'w') as f:\n                json.dump({'hash': state_hash}, f)\n\n        except Exception as e:\n            logger.debug(f\"Failed to save index cache: {e}\")\n    \n    def _load_cached_index(self, code_analyzer) -> bool:\n        \"\"\"Load cached index if available and fresh.\"\"\"\n        if not all(os.path.exists(p) for p in [self.index_cache_file, self.metadata_cache_file, self.state_cache_file]):\n            return False\n        \n        try:\n            # Check if cache is stale\n            import json\n            with open(self.state_cache_file, 'r') as f:\n                stored_state = json.load(f)\n            \n            current_hash = self._get_project_state_hash(code_analyzer)\n            \n            if stored_state.get('hash') != current_hash:\n                logger.info(\"Project has changed, rebuilding index.\")\n                return False\n\n            # Load index and metadata\n            self.index = faiss.read_index(self.index_cache_file)\n            with open(self.metadata_cache_file, 'r') as f:\n                metadata = json.load(f)\n            \n            self.chunks = metadata['chunks']\n            self.metadata = metadata['metadata']\n            \n            return True\n        except Exception as e:\n            logger.debug(f\"Failed to load cached index: {e}\")\n            return False"}, {"filepath": "context_engine.py", "start_line": 31, "end_line": 48, "type": "function", "name": "__init__", "content": "    def __init__(self, project_path: str, console=None):\n        self.project_path = project_path\n        self.console = console\n        self.model = None\n        self.index = None\n        self.chunks = []  # Store code chunks\n        self.metadata = []  # Store file paths, line numbers, etc.\n        self.cache_dir = os.path.join(project_path, '.maplecli_cache')\n        self.index_cache_file = os.path.join(self.cache_dir, 'index.faiss')\n        self.metadata_cache_file = os.path.join(self.cache_dir, 'metadata.json')\n        self.state_cache_file = os.path.join(self.cache_dir, 'state.json')\n        \n        if EMBEDDINGS_AVAILABLE:\n            try:\n                self.model = SentenceTransformer('all-MiniLM-L6-v2')  # Fast, good quality\n            except Exception as e:\n                logger.error(f\"Failed to load embedding model: {e}\")\n                self.model = None", "docstring": "", "searchable_text": "__init__      def __init__(self, project_path: str, console=None):\n        self.project_path = project_path\n        self.console = console\n        self.model = None\n        self.index = None\n        self.chunks = []  # Store code chunks\n        self.metadata = []  # Store file paths, line numbers, etc.\n        self.cache_dir = os.path.join(project_path, '.maplecli_cache')\n        self.index_cache_file = os.path.join(self.cache_dir, 'index.faiss')\n        self.metadata_cache_file = os.path.join(self.cache_dir, 'metadata.json')\n        self.state_cache_file = os.path.join(self.cache_dir, 'state.json')\n        \n        if EMBEDDINGS_AVAILABLE:\n            try:\n                self.model = SentenceTransformer('all-MiniLM-L6-v2')  # Fast, good quality\n            except Exception as e:\n                logger.error(f\"Failed to load embedding model: {e}\")\n                self.model = None"}, {"filepath": "context_engine.py", "start_line": 50, "end_line": 67, "type": "function", "name": "chunk_code", "content": "    def chunk_code(self, filepath: str, content: str) -> List[Dict]:\n        \"\"\"\n        Chunk code into semantic units (functions, classes, blocks).\n        \"\"\"\n        chunks = []\n        ext = os.path.splitext(filepath)[1].lower()\n        \n        if ext == '.py':\n            chunks = self._chunk_python(filepath, content)\n        elif ext in ['.js', '.ts', '.jsx', '.tsx']:\n            chunks = self._chunk_javascript(filepath, content)\n        elif ext == '.java':\n            chunks = self._chunk_java(filepath, content)\n        else:\n            # Fallback: chunk by lines (every 50 lines)\n            chunks = self._chunk_by_lines(filepath, content, chunk_size=50)\n        \n        return chunks", "docstring": "Chunk code into semantic units (functions, classes, blocks).", "searchable_text": "chunk_code Chunk code into semantic units (functions, classes, blocks).     def chunk_code(self, filepath: str, content: str) -> List[Dict]:\n        \"\"\"\n        Chunk code into semantic units (functions, classes, blocks).\n        \"\"\"\n        chunks = []\n        ext = os.path.splitext(filepath)[1].lower()\n        \n        if ext == '.py':\n            chunks = self._chunk_python(filepath, content)\n        elif ext in ['.js', '.ts', '.jsx', '.tsx']:\n            chunks = self._chunk_javascript(filepath, content)\n        elif ext == '.java':\n            chunks = self._chunk_java(filepath, content)\n        else:\n            # Fallback: chunk by lines (every 50 lines)\n            chunks = self._chunk_by_lines(filepath, content, chunk_size=50)\n        \n        return chunks"}, {"filepath": "context_engine.py", "start_line": 69, "end_line": 106, "type": "function", "name": "_chunk_python", "content": "    def _chunk_python(self, filepath: str, content: str) -> List[Dict]:\n        \"\"\"Extract Python functions and classes as chunks.\"\"\"\n        import ast\n        chunks = []\n        \n        try:\n            tree = ast.parse(content)\n            lines = content.split('\\n')\n            \n            for node in ast.walk(tree):\n                if isinstance(node, (ast.FunctionDef, ast.ClassDef, ast.AsyncFunctionDef)):\n                    start_line = node.lineno - 1\n                    end_line = node.end_lineno if hasattr(node, 'end_lineno') else start_line + 20\n                    \n                    chunk_content = '\\n'.join(lines[start_line:end_line])\n                    \n                    # Get docstring if available\n                    docstring = ast.get_docstring(node) or \"\"\n                    \n                    chunks.append({\n                        'filepath': filepath,\n                        'start_line': start_line + 1,\n                        'end_line': end_line,\n                        'type': 'function' if isinstance(node, (ast.FunctionDef, ast.AsyncFunctionDef)) else 'class',\n                        'name': node.name,\n                        'content': chunk_content,\n                        'docstring': docstring,\n                        'searchable_text': f\"{node.name} {docstring} {chunk_content}\"\n                    })\n        except Exception as e:\n            logger.debug(f\"Failed to parse Python file {filepath}: {e}\")\n            # Fallback to line-based chunking\n            return self._chunk_by_lines(filepath, content)\n        \n        if not chunks:\n            return self._chunk_by_lines(filepath, content)\n        \n        return chunks", "docstring": "Extract Python functions and classes as chunks.", "searchable_text": "_chunk_python Extract Python functions and classes as chunks.     def _chunk_python(self, filepath: str, content: str) -> List[Dict]:\n        \"\"\"Extract Python functions and classes as chunks.\"\"\"\n        import ast\n        chunks = []\n        \n        try:\n            tree = ast.parse(content)\n            lines = content.split('\\n')\n            \n            for node in ast.walk(tree):\n                if isinstance(node, (ast.FunctionDef, ast.ClassDef, ast.AsyncFunctionDef)):\n                    start_line = node.lineno - 1\n                    end_line = node.end_lineno if hasattr(node, 'end_lineno') else start_line + 20\n                    \n                    chunk_content = '\\n'.join(lines[start_line:end_line])\n                    \n                    # Get docstring if available\n                    docstring = ast.get_docstring(node) or \"\"\n                    \n                    chunks.append({\n                        'filepath': filepath,\n                        'start_line': start_line + 1,\n                        'end_line': end_line,\n                        'type': 'function' if isinstance(node, (ast.FunctionDef, ast.AsyncFunctionDef)) else 'class',\n                        'name': node.name,\n                        'content': chunk_content,\n                        'docstring': docstring,\n                        'searchable_text': f\"{node.name} {docstring} {chunk_content}\"\n                    })\n        except Exception as e:\n            logger.debug(f\"Failed to parse Python file {filepath}: {e}\")\n            # Fallback to line-based chunking\n            return self._chunk_by_lines(filepath, content)\n        \n        if not chunks:\n            return self._chunk_by_lines(filepath, content)\n        \n        return chunks"}, {"filepath": "context_engine.py", "start_line": 108, "end_line": 152, "type": "function", "name": "_chunk_javascript", "content": "    def _chunk_javascript(self, filepath: str, content: str) -> List[Dict]:\n        \"\"\"Extract JavaScript/TypeScript functions and classes using AST if available.\"\"\"\n        try:\n            import esprima\n            tree = esprima.parseModule(content, {'loc': True})\n            lines = content.split('\\n')\n            chunks = []\n\n            for node in tree.body:\n                if node.type in ['FunctionDeclaration', 'ClassDeclaration'] and node.id:\n                    start_line = node.loc.start.line - 1\n                    end_line = node.loc.end.line\n                    chunk_content = '\\n'.join(lines[start_line:end_line])\n                    chunks.append({\n                        'filepath': filepath, 'start_line': start_line + 1, 'end_line': end_line,\n                        'type': 'function' if node.type == 'FunctionDeclaration' else 'class',\n                        'name': node.id.name, 'content': chunk_content,\n                        'searchable_text': f\"{node.id.name} {chunk_content}\"\n                    })\n            if chunks: return chunks\n        except (ImportError, Exception):\n            pass  # Fallback to regex\n        \n        # Fallback regex implementation\n        import re\n        chunks = []\n        lines = content.split('\\n')\n        patterns = [\n            (r'function\\s+(\\w+)\\s*\\([^)]*\\)\\s*{', 'function'),\n            (r'const\\s+(\\w+)\\s*=\\s*\\([^)]*\\)\\s*=>', 'function'),\n            (r'class\\s+(\\w+)\\s*{', 'class'),\n        ]\n        for i, line in enumerate(lines):\n            for pattern, chunk_type in patterns:\n                match = re.search(pattern, line)\n                if match:\n                    name = match.group(1) if match.groups() else 'anonymous'\n                    chunk_lines = lines[i:min(i + 40, len(lines))]\n                    chunks.append({\n                        'filepath': filepath, 'start_line': i + 1, 'end_line': min(i + 40, len(lines)),\n                        'type': chunk_type, 'name': name, 'content': '\\n'.join(chunk_lines),\n                        'searchable_text': f\"{name} {' '.join(chunk_lines)}\"\n                    })\n                    break\n        return chunks if chunks else self._chunk_by_lines(filepath, content)", "docstring": "Extract JavaScript/TypeScript functions and classes using AST if available.", "searchable_text": "_chunk_javascript Extract JavaScript/TypeScript functions and classes using AST if available.     def _chunk_javascript(self, filepath: str, content: str) -> List[Dict]:\n        \"\"\"Extract JavaScript/TypeScript functions and classes using AST if available.\"\"\"\n        try:\n            import esprima\n            tree = esprima.parseModule(content, {'loc': True})\n            lines = content.split('\\n')\n            chunks = []\n\n            for node in tree.body:\n                if node.type in ['FunctionDeclaration', 'ClassDeclaration'] and node.id:\n                    start_line = node.loc.start.line - 1\n                    end_line = node.loc.end.line\n                    chunk_content = '\\n'.join(lines[start_line:end_line])\n                    chunks.append({\n                        'filepath': filepath, 'start_line': start_line + 1, 'end_line': end_line,\n                        'type': 'function' if node.type == 'FunctionDeclaration' else 'class',\n                        'name': node.id.name, 'content': chunk_content,\n                        'searchable_text': f\"{node.id.name} {chunk_content}\"\n                    })\n            if chunks: return chunks\n        except (ImportError, Exception):\n            pass  # Fallback to regex\n        \n        # Fallback regex implementation\n        import re\n        chunks = []\n        lines = content.split('\\n')\n        patterns = [\n            (r'function\\s+(\\w+)\\s*\\([^)]*\\)\\s*{', 'function'),\n            (r'const\\s+(\\w+)\\s*=\\s*\\([^)]*\\)\\s*=>', 'function'),\n            (r'class\\s+(\\w+)\\s*{', 'class'),\n        ]\n        for i, line in enumerate(lines):\n            for pattern, chunk_type in patterns:\n                match = re.search(pattern, line)\n                if match:\n                    name = match.group(1) if match.groups() else 'anonymous'\n                    chunk_lines = lines[i:min(i + 40, len(lines))]\n                    chunks.append({\n                        'filepath': filepath, 'start_line': i + 1, 'end_line': min(i + 40, len(lines)),\n                        'type': chunk_type, 'name': name, 'content': '\\n'.join(chunk_lines),\n                        'searchable_text': f\"{name} {' '.join(chunk_lines)}\"\n                    })\n                    break\n        return chunks if chunks else self._chunk_by_lines(filepath, content)"}, {"filepath": "context_engine.py", "start_line": 154, "end_line": 198, "type": "function", "name": "_chunk_java", "content": "    def _chunk_java(self, filepath: str, content: str) -> List[Dict]:\n        \"\"\"Extract Java methods and classes using AST if available.\"\"\"\n        try:\n            import javalang\n            tree = javalang.parse.parse(content)\n            lines = content.split('\\n')\n            chunks = []\n\n            for path, node in tree:\n                if isinstance(node, (javalang.tree.ClassDeclaration, javalang.tree.MethodDeclaration)):\n                    start_line = node.position.line - 1\n                    # Estimate end line as javalang doesn't provide it\n                    end_line = start_line + len(str(node).split('\\n')) + 5\n                    chunk_content = '\\n'.join(lines[start_line:min(end_line, len(lines))])\n                    chunks.append({\n                        'filepath': filepath, 'start_line': start_line + 1, 'end_line': min(end_line, len(lines)),\n                        'type': 'class' if isinstance(node, javalang.tree.ClassDeclaration) else 'method',\n                        'name': node.name, 'content': chunk_content,\n                        'searchable_text': f\"{node.name} {chunk_content}\"\n                    })\n            if chunks: return chunks\n        except (ImportError, Exception):\n            pass # Fallback to regex\n\n        # Fallback regex implementation\n        import re\n        chunks = []\n        lines = content.split('\\n')\n        patterns = [\n            (r'class\\s+(\\w+)', 'class'),\n            (r'(?:public|private|protected)?\\s*(?:static\\s+)?[\\w<>\\[\\]]+\\s+(\\w+)\\s*\\([^)]*\\)\\s*{', 'method'),\n        ]\n        for i, line in enumerate(lines):\n            for pattern, chunk_type in patterns:\n                match = re.search(pattern, line)\n                if match:\n                    name = match.group(1)\n                    chunk_lines = lines[i:min(i + 40, len(lines))]\n                    chunks.append({\n                        'filepath': filepath, 'start_line': i + 1, 'end_line': min(i + 40, len(lines)),\n                        'type': chunk_type, 'name': name, 'content': '\\n'.join(chunk_lines),\n                        'searchable_text': f\"{name} {' '.join(chunk_lines)}\"\n                    })\n                    break\n        return chunks if chunks else self._chunk_by_lines(filepath, content)", "docstring": "Extract Java methods and classes using AST if available.", "searchable_text": "_chunk_java Extract Java methods and classes using AST if available.     def _chunk_java(self, filepath: str, content: str) -> List[Dict]:\n        \"\"\"Extract Java methods and classes using AST if available.\"\"\"\n        try:\n            import javalang\n            tree = javalang.parse.parse(content)\n            lines = content.split('\\n')\n            chunks = []\n\n            for path, node in tree:\n                if isinstance(node, (javalang.tree.ClassDeclaration, javalang.tree.MethodDeclaration)):\n                    start_line = node.position.line - 1\n                    # Estimate end line as javalang doesn't provide it\n                    end_line = start_line + len(str(node).split('\\n')) + 5\n                    chunk_content = '\\n'.join(lines[start_line:min(end_line, len(lines))])\n                    chunks.append({\n                        'filepath': filepath, 'start_line': start_line + 1, 'end_line': min(end_line, len(lines)),\n                        'type': 'class' if isinstance(node, javalang.tree.ClassDeclaration) else 'method',\n                        'name': node.name, 'content': chunk_content,\n                        'searchable_text': f\"{node.name} {chunk_content}\"\n                    })\n            if chunks: return chunks\n        except (ImportError, Exception):\n            pass # Fallback to regex\n\n        # Fallback regex implementation\n        import re\n        chunks = []\n        lines = content.split('\\n')\n        patterns = [\n            (r'class\\s+(\\w+)', 'class'),\n            (r'(?:public|private|protected)?\\s*(?:static\\s+)?[\\w<>\\[\\]]+\\s+(\\w+)\\s*\\([^)]*\\)\\s*{', 'method'),\n        ]\n        for i, line in enumerate(lines):\n            for pattern, chunk_type in patterns:\n                match = re.search(pattern, line)\n                if match:\n                    name = match.group(1)\n                    chunk_lines = lines[i:min(i + 40, len(lines))]\n                    chunks.append({\n                        'filepath': filepath, 'start_line': i + 1, 'end_line': min(i + 40, len(lines)),\n                        'type': chunk_type, 'name': name, 'content': '\\n'.join(chunk_lines),\n                        'searchable_text': f\"{name} {' '.join(chunk_lines)}\"\n                    })\n                    break\n        return chunks if chunks else self._chunk_by_lines(filepath, content)"}, {"filepath": "context_engine.py", "start_line": 200, "end_line": 217, "type": "function", "name": "_chunk_by_lines", "content": "    def _chunk_by_lines(self, filepath: str, content: str, chunk_size: int = 50) -> List[Dict]:\n        \"\"\"Fallback: chunk by fixed line count.\"\"\"\n        lines = content.split('\\n')\n        chunks = []\n        \n        for i in range(0, len(lines), chunk_size):\n            chunk_lines = lines[i:i+chunk_size]\n            chunks.append({\n                'filepath': filepath,\n                'start_line': i + 1,\n                'end_line': min(i + chunk_size, len(lines)),\n                'type': 'block',\n                'name': f'block_{i}',\n                'content': '\\n'.join(chunk_lines),\n                'searchable_text': '\\n'.join(chunk_lines)\n            })\n        \n        return chunks", "docstring": "Fallback: chunk by fixed line count.", "searchable_text": "_chunk_by_lines Fallback: chunk by fixed line count.     def _chunk_by_lines(self, filepath: str, content: str, chunk_size: int = 50) -> List[Dict]:\n        \"\"\"Fallback: chunk by fixed line count.\"\"\"\n        lines = content.split('\\n')\n        chunks = []\n        \n        for i in range(0, len(lines), chunk_size):\n            chunk_lines = lines[i:i+chunk_size]\n            chunks.append({\n                'filepath': filepath,\n                'start_line': i + 1,\n                'end_line': min(i + chunk_size, len(lines)),\n                'type': 'block',\n                'name': f'block_{i}',\n                'content': '\\n'.join(chunk_lines),\n                'searchable_text': '\\n'.join(chunk_lines)\n            })\n        \n        return chunks"}, {"filepath": "context_engine.py", "start_line": 219, "end_line": 299, "type": "function", "name": "build_index", "content": "    def build_index(self, code_analyzer) -> bool:\n        \"\"\"\n        Build FAISS index from all code chunks.\n        Returns True if successful, False otherwise.\n        \"\"\"\n        if not EMBEDDINGS_AVAILABLE or not self.model:\n            if self.console:\n                self.console.print(\"[yellow]\u26a0\ufe0f  Semantic search not available. Install with: pip install sentence-transformers faiss-cpu[/yellow]\")\n            return False\n        \n        # Check if cached index exists and is fresh\n        if self._load_cached_index(code_analyzer):\n            if self.console:\n                self.console.print(f\"[green]\u2713 Loaded fresh cache ({len(self.chunks)} chunks)[/green]\")\n            return True\n        \n        all_chunks = []\n        all_texts = []\n        \n        if self.console:\n            from rich.progress import Progress, SpinnerColumn, TextColumn, BarColumn\n            \n            with Progress(\n                SpinnerColumn(),\n                TextColumn(\"[progress.description]{task.description}\"),\n                BarColumn(),\n                console=self.console\n            ) as progress:\n                task = progress.add_task(\"[cyan]Building semantic index...\", total=len(code_analyzer.files))\n                \n                for filepath in code_analyzer.files:\n                    try:\n                        content = code_analyzer.read_file_content(filepath, max_lines=10000)\n                        if content and not content.startswith(\"Error\"):\n                            chunks = self.chunk_code(filepath, content)\n                            all_chunks.extend(chunks)\n                            all_texts.extend([c['searchable_text'] for c in chunks])\n                    except Exception as e:\n                        logger.debug(f\"Error chunking {filepath}: {e}\")\n                    progress.advance(task)\n        else:\n            for filepath in code_analyzer.files:\n                try:\n                    content = code_analyzer.read_file_content(filepath, max_lines=10000)\n                    if content and not content.startswith(\"Error\"):\n                        chunks = self.chunk_code(filepath, content)\n                        all_chunks.extend(chunks)\n                        all_texts.extend([c['searchable_text'] for c in chunks])\n                except Exception as e:\n                    logger.debug(f\"Error chunking {filepath}: {e}\")\n        \n        if not all_texts:\n            if self.console:\n                self.console.print(\"[yellow]\u26a0\ufe0f  No code chunks to index[/yellow]\")\n            return False\n        \n        # Generate embeddings\n        if self.console:\n            self.console.print(f\"[cyan]Generating embeddings for {len(all_texts)} code chunks...[/cyan]\")\n        \n        try:\n            embeddings = self.model.encode(all_texts, show_progress_bar=False, batch_size=32)\n        except Exception as e:\n            logger.error(f\"Failed to generate embeddings: {e}\")\n            return False\n        \n        # Build FAISS index\n        dimension = embeddings.shape[1]\n        self.index = faiss.IndexFlatL2(dimension)\n        self.index.add(embeddings.astype('float32'))\n        \n        self.chunks = all_chunks\n        self.metadata = [{'filepath': c['filepath'], 'start_line': c['start_line']} for c in all_chunks]\n        \n        # Cache the index\n        self._save_index(code_analyzer)\n        \n        if self.console:\n            self.console.print(f\"[green]\u2713 Index built: {len(all_chunks)} chunks indexed[/green]\")\n        \n        return True", "docstring": "Build FAISS index from all code chunks.\nReturns True if successful, False otherwise.", "searchable_text": "build_index Build FAISS index from all code chunks.\nReturns True if successful, False otherwise.     def build_index(self, code_analyzer) -> bool:\n        \"\"\"\n        Build FAISS index from all code chunks.\n        Returns True if successful, False otherwise.\n        \"\"\"\n        if not EMBEDDINGS_AVAILABLE or not self.model:\n            if self.console:\n                self.console.print(\"[yellow]\u26a0\ufe0f  Semantic search not available. Install with: pip install sentence-transformers faiss-cpu[/yellow]\")\n            return False\n        \n        # Check if cached index exists and is fresh\n        if self._load_cached_index(code_analyzer):\n            if self.console:\n                self.console.print(f\"[green]\u2713 Loaded fresh cache ({len(self.chunks)} chunks)[/green]\")\n            return True\n        \n        all_chunks = []\n        all_texts = []\n        \n        if self.console:\n            from rich.progress import Progress, SpinnerColumn, TextColumn, BarColumn\n            \n            with Progress(\n                SpinnerColumn(),\n                TextColumn(\"[progress.description]{task.description}\"),\n                BarColumn(),\n                console=self.console\n            ) as progress:\n                task = progress.add_task(\"[cyan]Building semantic index...\", total=len(code_analyzer.files))\n                \n                for filepath in code_analyzer.files:\n                    try:\n                        content = code_analyzer.read_file_content(filepath, max_lines=10000)\n                        if content and not content.startswith(\"Error\"):\n                            chunks = self.chunk_code(filepath, content)\n                            all_chunks.extend(chunks)\n                            all_texts.extend([c['searchable_text'] for c in chunks])\n                    except Exception as e:\n                        logger.debug(f\"Error chunking {filepath}: {e}\")\n                    progress.advance(task)\n        else:\n            for filepath in code_analyzer.files:\n                try:\n                    content = code_analyzer.read_file_content(filepath, max_lines=10000)\n                    if content and not content.startswith(\"Error\"):\n                        chunks = self.chunk_code(filepath, content)\n                        all_chunks.extend(chunks)\n                        all_texts.extend([c['searchable_text'] for c in chunks])\n                except Exception as e:\n                    logger.debug(f\"Error chunking {filepath}: {e}\")\n        \n        if not all_texts:\n            if self.console:\n                self.console.print(\"[yellow]\u26a0\ufe0f  No code chunks to index[/yellow]\")\n            return False\n        \n        # Generate embeddings\n        if self.console:\n            self.console.print(f\"[cyan]Generating embeddings for {len(all_texts)} code chunks...[/cyan]\")\n        \n        try:\n            embeddings = self.model.encode(all_texts, show_progress_bar=False, batch_size=32)\n        except Exception as e:\n            logger.error(f\"Failed to generate embeddings: {e}\")\n            return False\n        \n        # Build FAISS index\n        dimension = embeddings.shape[1]\n        self.index = faiss.IndexFlatL2(dimension)\n        self.index.add(embeddings.astype('float32'))\n        \n        self.chunks = all_chunks\n        self.metadata = [{'filepath': c['filepath'], 'start_line': c['start_line']} for c in all_chunks]\n        \n        # Cache the index\n        self._save_index(code_analyzer)\n        \n        if self.console:\n            self.console.print(f\"[green]\u2713 Index built: {len(all_chunks)} chunks indexed[/green]\")\n        \n        return True"}, {"filepath": "context_engine.py", "start_line": 301, "end_line": 326, "type": "function", "name": "search", "content": "    def search(self, query: str, top_k: int = 5) -> List[Dict]:\n        \"\"\"\n        Search for relevant code chunks using semantic similarity.\n        \"\"\"\n        if self.index is None or not EMBEDDINGS_AVAILABLE or not self.model:\n            return []\n        \n        try:\n            # Encode query\n            query_embedding = self.model.encode([query])\n            \n            # Search\n            distances, indices = self.index.search(query_embedding.astype('float32'), min(top_k, len(self.chunks)))\n            \n            # Return results with scores\n            results = []\n            for i, idx in enumerate(indices[0]):\n                if idx < len(self.chunks) and idx >= 0:\n                    chunk = self.chunks[idx].copy()\n                    chunk['relevance_score'] = float(1 / (1 + distances[0][i]))  # Convert distance to similarity\n                    results.append(chunk)\n            \n            return results\n        except Exception as e:\n            logger.error(f\"Search failed: {e}\")\n            return []", "docstring": "Search for relevant code chunks using semantic similarity.", "searchable_text": "search Search for relevant code chunks using semantic similarity.     def search(self, query: str, top_k: int = 5) -> List[Dict]:\n        \"\"\"\n        Search for relevant code chunks using semantic similarity.\n        \"\"\"\n        if self.index is None or not EMBEDDINGS_AVAILABLE or not self.model:\n            return []\n        \n        try:\n            # Encode query\n            query_embedding = self.model.encode([query])\n            \n            # Search\n            distances, indices = self.index.search(query_embedding.astype('float32'), min(top_k, len(self.chunks)))\n            \n            # Return results with scores\n            results = []\n            for i, idx in enumerate(indices[0]):\n                if idx < len(self.chunks) and idx >= 0:\n                    chunk = self.chunks[idx].copy()\n                    chunk['relevance_score'] = float(1 / (1 + distances[0][i]))  # Convert distance to similarity\n                    results.append(chunk)\n            \n            return results\n        except Exception as e:\n            logger.error(f\"Search failed: {e}\")\n            return []"}, {"filepath": "context_engine.py", "start_line": 328, "end_line": 342, "type": "function", "name": "_get_project_state_hash", "content": "    def _get_project_state_hash(self, code_analyzer) -> str:\n        \"\"\"Generates a hash representing the current state of project files.\"\"\"\n        try:\n            file_metadata = []\n            for f in code_analyzer.files:\n                full_path = os.path.join(self.project_path, f)\n                if os.path.exists(full_path):\n                    mtime = os.path.getmtime(full_path)\n                    file_metadata.append(f\"{f}:{mtime}\")\n            \n            state_string = \"\".join(sorted(file_metadata))\n            return hashlib.md5(state_string.encode()).hexdigest()\n        except Exception as e:\n            logger.debug(f\"Could not generate project state hash: {e}\")\n            return \"\"", "docstring": "Generates a hash representing the current state of project files.", "searchable_text": "_get_project_state_hash Generates a hash representing the current state of project files.     def _get_project_state_hash(self, code_analyzer) -> str:\n        \"\"\"Generates a hash representing the current state of project files.\"\"\"\n        try:\n            file_metadata = []\n            for f in code_analyzer.files:\n                full_path = os.path.join(self.project_path, f)\n                if os.path.exists(full_path):\n                    mtime = os.path.getmtime(full_path)\n                    file_metadata.append(f\"{f}:{mtime}\")\n            \n            state_string = \"\".join(sorted(file_metadata))\n            return hashlib.md5(state_string.encode()).hexdigest()\n        except Exception as e:\n            logger.debug(f\"Could not generate project state hash: {e}\")\n            return \"\""}, {"filepath": "context_engine.py", "start_line": 344, "end_line": 364, "type": "function", "name": "_save_index", "content": "    def _save_index(self, code_analyzer):\n        \"\"\"Save index and metadata to disk using a secure format.\"\"\"\n        try:\n            os.makedirs(self.cache_dir, exist_ok=True)\n            \n            # Save FAISS index\n            if self.index:\n                faiss.write_index(self.index, self.index_cache_file)\n\n            # Save metadata as JSON\n            import json\n            with open(self.metadata_cache_file, 'w') as f:\n                json.dump({'chunks': self.chunks, 'metadata': self.metadata}, f)\n\n            # Save project state hash\n            state_hash = self._get_project_state_hash(code_analyzer)\n            with open(self.state_cache_file, 'w') as f:\n                json.dump({'hash': state_hash}, f)\n\n        except Exception as e:\n            logger.debug(f\"Failed to save index cache: {e}\")", "docstring": "Save index and metadata to disk using a secure format.", "searchable_text": "_save_index Save index and metadata to disk using a secure format.     def _save_index(self, code_analyzer):\n        \"\"\"Save index and metadata to disk using a secure format.\"\"\"\n        try:\n            os.makedirs(self.cache_dir, exist_ok=True)\n            \n            # Save FAISS index\n            if self.index:\n                faiss.write_index(self.index, self.index_cache_file)\n\n            # Save metadata as JSON\n            import json\n            with open(self.metadata_cache_file, 'w') as f:\n                json.dump({'chunks': self.chunks, 'metadata': self.metadata}, f)\n\n            # Save project state hash\n            state_hash = self._get_project_state_hash(code_analyzer)\n            with open(self.state_cache_file, 'w') as f:\n                json.dump({'hash': state_hash}, f)\n\n        except Exception as e:\n            logger.debug(f\"Failed to save index cache: {e}\")"}, {"filepath": "context_engine.py", "start_line": 366, "end_line": 394, "type": "function", "name": "_load_cached_index", "content": "    def _load_cached_index(self, code_analyzer) -> bool:\n        \"\"\"Load cached index if available and fresh.\"\"\"\n        if not all(os.path.exists(p) for p in [self.index_cache_file, self.metadata_cache_file, self.state_cache_file]):\n            return False\n        \n        try:\n            # Check if cache is stale\n            import json\n            with open(self.state_cache_file, 'r') as f:\n                stored_state = json.load(f)\n            \n            current_hash = self._get_project_state_hash(code_analyzer)\n            \n            if stored_state.get('hash') != current_hash:\n                logger.info(\"Project has changed, rebuilding index.\")\n                return False\n\n            # Load index and metadata\n            self.index = faiss.read_index(self.index_cache_file)\n            with open(self.metadata_cache_file, 'r') as f:\n                metadata = json.load(f)\n            \n            self.chunks = metadata['chunks']\n            self.metadata = metadata['metadata']\n            \n            return True\n        except Exception as e:\n            logger.debug(f\"Failed to load cached index: {e}\")\n            return False", "docstring": "Load cached index if available and fresh.", "searchable_text": "_load_cached_index Load cached index if available and fresh.     def _load_cached_index(self, code_analyzer) -> bool:\n        \"\"\"Load cached index if available and fresh.\"\"\"\n        if not all(os.path.exists(p) for p in [self.index_cache_file, self.metadata_cache_file, self.state_cache_file]):\n            return False\n        \n        try:\n            # Check if cache is stale\n            import json\n            with open(self.state_cache_file, 'r') as f:\n                stored_state = json.load(f)\n            \n            current_hash = self._get_project_state_hash(code_analyzer)\n            \n            if stored_state.get('hash') != current_hash:\n                logger.info(\"Project has changed, rebuilding index.\")\n                return False\n\n            # Load index and metadata\n            self.index = faiss.read_index(self.index_cache_file)\n            with open(self.metadata_cache_file, 'r') as f:\n                metadata = json.load(f)\n            \n            self.chunks = metadata['chunks']\n            self.metadata = metadata['metadata']\n            \n            return True\n        except Exception as e:\n            logger.debug(f\"Failed to load cached index: {e}\")\n            return False"}, {"filepath": "install_intelligent.bat", "start_line": 1, "end_line": 50, "type": "block", "name": "block_0", "content": "@echo off\nREM Installation script for MapleCLI with intelligent features (Windows)\n\necho ========================================\necho MapleCLI Intelligent Features Installer\necho ========================================\necho.\n\nREM Check Python\npython --version >nul 2>&1\nif errorlevel 1 (\n    echo [ERROR] Python not found. Please install Python first.\n    pause\n    exit /b 1\n)\n\necho [OK] Python is available\necho.\n\nREM Ask user what to install\necho Choose installation option:\necho 1) Basic installation (no intelligent features)\necho 2) Full installation (with intelligent features - recommended)\necho 3) Full installation with GPU support (requires CUDA)\necho.\nset /p choice=\"Enter choice [1-3]: \"\n\nif \"%choice%\"==\"1\" (\n    echo.\n    echo Installing basic MapleCLI...\n    pip install -e .\n    goto :done\n)\n\nif \"%choice%\"==\"2\" (\n    echo.\n    echo Installing MapleCLI with intelligent features...\n    echo This will download ~500MB of dependencies\n    echo.\n    set /p confirm=\"Continue? [y/N]: \"\n    if /i \"%confirm%\"==\"y\" (\n        pip install -e \".[intelligent]\"\n    ) else (\n        echo Installation cancelled.\n        pause\n        exit /b 0\n    )\n    goto :done\n)\n", "searchable_text": "@echo off\nREM Installation script for MapleCLI with intelligent features (Windows)\n\necho ========================================\necho MapleCLI Intelligent Features Installer\necho ========================================\necho.\n\nREM Check Python\npython --version >nul 2>&1\nif errorlevel 1 (\n    echo [ERROR] Python not found. Please install Python first.\n    pause\n    exit /b 1\n)\n\necho [OK] Python is available\necho.\n\nREM Ask user what to install\necho Choose installation option:\necho 1) Basic installation (no intelligent features)\necho 2) Full installation (with intelligent features - recommended)\necho 3) Full installation with GPU support (requires CUDA)\necho.\nset /p choice=\"Enter choice [1-3]: \"\n\nif \"%choice%\"==\"1\" (\n    echo.\n    echo Installing basic MapleCLI...\n    pip install -e .\n    goto :done\n)\n\nif \"%choice%\"==\"2\" (\n    echo.\n    echo Installing MapleCLI with intelligent features...\n    echo This will download ~500MB of dependencies\n    echo.\n    set /p confirm=\"Continue? [y/N]: \"\n    if /i \"%confirm%\"==\"y\" (\n        pip install -e \".[intelligent]\"\n    ) else (\n        echo Installation cancelled.\n        pause\n        exit /b 0\n    )\n    goto :done\n)\n"}, {"filepath": "install_intelligent.bat", "start_line": 51, "end_line": 95, "type": "block", "name": "block_50", "content": "if \"%choice%\"==\"3\" (\n    echo.\n    echo Installing MapleCLI with GPU support...\n    echo This requires CUDA to be installed on your system.\n    echo.\n    set /p confirm=\"Continue? [y/N]: \"\n    if /i \"%confirm%\"==\"y\" (\n        pip install -e \".[intelligent]\"\n        pip uninstall -y faiss-cpu\n        pip install faiss-gpu\n    ) else (\n        echo Installation cancelled.\n        pause\n        exit /b 0\n    )\n    goto :done\n)\n\necho Invalid choice. Exiting.\npause\nexit /b 1\n\n:done\necho.\necho ========================================\necho Installation complete!\necho ========================================\necho.\necho Quick Start:\necho   1. Run: maplecli chat\necho   2. Enable YOLO mode: :yolo\necho   3. Switch to project: :cd C:\\path\\to\\project\necho   4. Analyze: :analyze\necho   5. Ask questions!\necho.\necho Documentation:\necho   - Quick Start: QUICKSTART_INTELLIGENT.md\necho   - Full Guide: INTELLIGENT_FEATURES.md\necho   - Examples: YOLO_EXAMPLES.md\necho.\necho Happy coding!\necho.\npause\n\n", "searchable_text": "if \"%choice%\"==\"3\" (\n    echo.\n    echo Installing MapleCLI with GPU support...\n    echo This requires CUDA to be installed on your system.\n    echo.\n    set /p confirm=\"Continue? [y/N]: \"\n    if /i \"%confirm%\"==\"y\" (\n        pip install -e \".[intelligent]\"\n        pip uninstall -y faiss-cpu\n        pip install faiss-gpu\n    ) else (\n        echo Installation cancelled.\n        pause\n        exit /b 0\n    )\n    goto :done\n)\n\necho Invalid choice. Exiting.\npause\nexit /b 1\n\n:done\necho.\necho ========================================\necho Installation complete!\necho ========================================\necho.\necho Quick Start:\necho   1. Run: maplecli chat\necho   2. Enable YOLO mode: :yolo\necho   3. Switch to project: :cd C:\\path\\to\\project\necho   4. Analyze: :analyze\necho   5. Ask questions!\necho.\necho Documentation:\necho   - Quick Start: QUICKSTART_INTELLIGENT.md\necho   - Full Guide: INTELLIGENT_FEATURES.md\necho   - Examples: YOLO_EXAMPLES.md\necho.\necho Happy coding!\necho.\npause\n\n"}, {"filepath": "install_intelligent.sh", "start_line": 1, "end_line": 50, "type": "block", "name": "block_0", "content": "#!/bin/bash\n\n# Installation script for MapleCLI with intelligent features\n\necho \"\ud83d\ude80 MapleCLI Intelligent Features Installer\"\necho \"==========================================\"\necho \"\"\n\n# Check Python version\npython_version=$(python3 --version 2>&1 | awk '{print $2}')\necho \"\u2713 Python version: $python_version\"\n\n# Check if pip is available\nif ! command -v pip3 &> /dev/null; then\n    echo \"\u274c pip3 not found. Please install pip first.\"\n    exit 1\nfi\n\necho \"\u2713 pip3 is available\"\necho \"\"\n\n# Ask user what to install\necho \"Choose installation option:\"\necho \"1) Basic installation (no intelligent features)\"\necho \"2) Full installation (with intelligent features - recommended)\"\necho \"3) Full installation with GPU support (requires CUDA)\"\necho \"\"\nread -p \"Enter choice [1-3]: \" choice\n\ncase $choice in\n    1)\n        echo \"\"\n        echo \"\ud83d\udce6 Installing basic MapleCLI...\"\n        pip3 install -e .\n        ;;\n    2)\n        echo \"\"\n        echo \"\ud83d\udce6 Installing MapleCLI with intelligent features...\"\n        echo \"This will download ~500MB of dependencies (torch, sentence-transformers, etc.)\"\n        echo \"\"\n        read -p \"Continue? [y/N]: \" confirm\n        if [[ $confirm == [yY] ]]; then\n            pip3 install -e \".[intelligent]\"\n        else\n            echo \"Installation cancelled.\"\n            exit 0\n        fi\n        ;;\n    3)\n        echo \"\"", "searchable_text": "#!/bin/bash\n\n# Installation script for MapleCLI with intelligent features\n\necho \"\ud83d\ude80 MapleCLI Intelligent Features Installer\"\necho \"==========================================\"\necho \"\"\n\n# Check Python version\npython_version=$(python3 --version 2>&1 | awk '{print $2}')\necho \"\u2713 Python version: $python_version\"\n\n# Check if pip is available\nif ! command -v pip3 &> /dev/null; then\n    echo \"\u274c pip3 not found. Please install pip first.\"\n    exit 1\nfi\n\necho \"\u2713 pip3 is available\"\necho \"\"\n\n# Ask user what to install\necho \"Choose installation option:\"\necho \"1) Basic installation (no intelligent features)\"\necho \"2) Full installation (with intelligent features - recommended)\"\necho \"3) Full installation with GPU support (requires CUDA)\"\necho \"\"\nread -p \"Enter choice [1-3]: \" choice\n\ncase $choice in\n    1)\n        echo \"\"\n        echo \"\ud83d\udce6 Installing basic MapleCLI...\"\n        pip3 install -e .\n        ;;\n    2)\n        echo \"\"\n        echo \"\ud83d\udce6 Installing MapleCLI with intelligent features...\"\n        echo \"This will download ~500MB of dependencies (torch, sentence-transformers, etc.)\"\n        echo \"\"\n        read -p \"Continue? [y/N]: \" confirm\n        if [[ $confirm == [yY] ]]; then\n            pip3 install -e \".[intelligent]\"\n        else\n            echo \"Installation cancelled.\"\n            exit 0\n        fi\n        ;;\n    3)\n        echo \"\""}, {"filepath": "install_intelligent.sh", "start_line": 51, "end_line": 87, "type": "block", "name": "block_50", "content": "        echo \"\ud83d\udce6 Installing MapleCLI with GPU support...\"\n        echo \"This requires CUDA to be installed on your system.\"\n        echo \"\"\n        read -p \"Continue? [y/N]: \" confirm\n        if [[ $confirm == [yY] ]]; then\n            pip3 install -e \".[intelligent]\"\n            pip3 uninstall -y faiss-cpu\n            pip3 install faiss-gpu\n        else\n            echo \"Installation cancelled.\"\n            exit 0\n        fi\n        ;;\n    *)\n        echo \"Invalid choice. Exiting.\"\n        exit 1\n        ;;\nesac\n\necho \"\"\necho \"\u2705 Installation complete!\"\necho \"\"\necho \"\ud83c\udfaf Quick Start:\"\necho \"  1. Run: maplecli chat\"\necho \"  2. Enable YOLO mode: :yolo\"\necho \"  3. Switch to project: :cd /path/to/project\"\necho \"  4. Analyze: :analyze\"\necho \"  5. Ask questions!\"\necho \"\"\necho \"\ud83d\udcda Documentation:\"\necho \"  - Quick Start: QUICKSTART_INTELLIGENT.md\"\necho \"  - Full Guide: INTELLIGENT_FEATURES.md\"\necho \"  - Examples: YOLO_EXAMPLES.md\"\necho \"\"\necho \"Happy coding! \ud83d\ude80\"\n\n", "searchable_text": "        echo \"\ud83d\udce6 Installing MapleCLI with GPU support...\"\n        echo \"This requires CUDA to be installed on your system.\"\n        echo \"\"\n        read -p \"Continue? [y/N]: \" confirm\n        if [[ $confirm == [yY] ]]; then\n            pip3 install -e \".[intelligent]\"\n            pip3 uninstall -y faiss-cpu\n            pip3 install faiss-gpu\n        else\n            echo \"Installation cancelled.\"\n            exit 0\n        fi\n        ;;\n    *)\n        echo \"Invalid choice. Exiting.\"\n        exit 1\n        ;;\nesac\n\necho \"\"\necho \"\u2705 Installation complete!\"\necho \"\"\necho \"\ud83c\udfaf Quick Start:\"\necho \"  1. Run: maplecli chat\"\necho \"  2. Enable YOLO mode: :yolo\"\necho \"  3. Switch to project: :cd /path/to/project\"\necho \"  4. Analyze: :analyze\"\necho \"  5. Ask questions!\"\necho \"\"\necho \"\ud83d\udcda Documentation:\"\necho \"  - Quick Start: QUICKSTART_INTELLIGENT.md\"\necho \"  - Full Guide: INTELLIGENT_FEATURES.md\"\necho \"  - Examples: YOLO_EXAMPLES.md\"\necho \"\"\necho \"Happy coding! \ud83d\ude80\"\n\n"}, {"filepath": "logger.py", "start_line": 17, "end_line": 19, "type": "class", "name": "SecurityError", "content": "class SecurityError(Exception):\n    \"\"\"Custom exception for security violations\"\"\"\n    pass", "docstring": "Custom exception for security violations", "searchable_text": "SecurityError Custom exception for security violations class SecurityError(Exception):\n    \"\"\"Custom exception for security violations\"\"\"\n    pass"}, {"filepath": "logger.py", "start_line": 21, "end_line": 39, "type": "class", "name": "MapleLogger", "content": "class MapleLogger:\n    \"\"\"Enhanced logging with structured error context\"\"\"\n    def __init__(self, name: str = \"maplecli\"):\n        self.logger = logging.getLogger(name)\n        \n    def log_error(self, error: Exception, operation: str, filepath: Optional[str] = None, severity: str = \"medium\"):\n        \"\"\"Structured error logging\"\"\"\n        self.logger.error(\n            f\"Error in {operation}: {str(error)}\",\n            extra={\n                'severity': severity,\n                'filepath': filepath,\n                'error_type': type(error).__name__\n            }\n        )\n        \n    def log_security_event(self, event: str, details: str):\n        \"\"\"Log security-related events\"\"\"\n        self.logger.warning(f\"SECURITY: {event} - {details}\")", "docstring": "Enhanced logging with structured error context", "searchable_text": "MapleLogger Enhanced logging with structured error context class MapleLogger:\n    \"\"\"Enhanced logging with structured error context\"\"\"\n    def __init__(self, name: str = \"maplecli\"):\n        self.logger = logging.getLogger(name)\n        \n    def log_error(self, error: Exception, operation: str, filepath: Optional[str] = None, severity: str = \"medium\"):\n        \"\"\"Structured error logging\"\"\"\n        self.logger.error(\n            f\"Error in {operation}: {str(error)}\",\n            extra={\n                'severity': severity,\n                'filepath': filepath,\n                'error_type': type(error).__name__\n            }\n        )\n        \n    def log_security_event(self, event: str, details: str):\n        \"\"\"Log security-related events\"\"\"\n        self.logger.warning(f\"SECURITY: {event} - {details}\")"}, {"filepath": "logger.py", "start_line": 23, "end_line": 24, "type": "function", "name": "__init__", "content": "    def __init__(self, name: str = \"maplecli\"):\n        self.logger = logging.getLogger(name)", "docstring": "", "searchable_text": "__init__      def __init__(self, name: str = \"maplecli\"):\n        self.logger = logging.getLogger(name)"}, {"filepath": "logger.py", "start_line": 26, "end_line": 35, "type": "function", "name": "log_error", "content": "    def log_error(self, error: Exception, operation: str, filepath: Optional[str] = None, severity: str = \"medium\"):\n        \"\"\"Structured error logging\"\"\"\n        self.logger.error(\n            f\"Error in {operation}: {str(error)}\",\n            extra={\n                'severity': severity,\n                'filepath': filepath,\n                'error_type': type(error).__name__\n            }\n        )", "docstring": "Structured error logging", "searchable_text": "log_error Structured error logging     def log_error(self, error: Exception, operation: str, filepath: Optional[str] = None, severity: str = \"medium\"):\n        \"\"\"Structured error logging\"\"\"\n        self.logger.error(\n            f\"Error in {operation}: {str(error)}\",\n            extra={\n                'severity': severity,\n                'filepath': filepath,\n                'error_type': type(error).__name__\n            }\n        )"}, {"filepath": "logger.py", "start_line": 37, "end_line": 39, "type": "function", "name": "log_security_event", "content": "    def log_security_event(self, event: str, details: str):\n        \"\"\"Log security-related events\"\"\"\n        self.logger.warning(f\"SECURITY: {event} - {details}\")", "docstring": "Log security-related events", "searchable_text": "log_security_event Log security-related events     def log_security_event(self, event: str, details: str):\n        \"\"\"Log security-related events\"\"\"\n        self.logger.warning(f\"SECURITY: {event} - {details}\")"}, {"filepath": "main.py", "start_line": 7, "end_line": 19, "type": "function", "name": "main", "content": "def main() -> None:\n    \"\"\"Main function for the CLI.\"\"\"\n    try:\n        cli = CLI()\n        cli.run()\n    except ImportError as e:\n        print(f\"Error: Missing required library - {e}\")\n        print(\"Please install required libraries: pip install -e .\")\n    except KeyboardInterrupt:\n        print(\"\\n\\nExiting...\")\n    except Exception as e:\n        print(f\"Fatal error: {e}\")\n        sys.exit(1)", "docstring": "Main function for the CLI.", "searchable_text": "main Main function for the CLI. def main() -> None:\n    \"\"\"Main function for the CLI.\"\"\"\n    try:\n        cli = CLI()\n        cli.run()\n    except ImportError as e:\n        print(f\"Error: Missing required library - {e}\")\n        print(\"Please install required libraries: pip install -e .\")\n    except KeyboardInterrupt:\n        print(\"\\n\\nExiting...\")\n    except Exception as e:\n        print(f\"Fatal error: {e}\")\n        sys.exit(1)"}, {"filepath": "pytest.ini", "start_line": 1, "end_line": 20, "type": "block", "name": "block_0", "content": "[tool:pytest]\ntestpaths = tests\npython_files = test_*.py\npython_classes = Test*\npython_functions = test_*\naddopts = \n    -v\n    --tb=short\n    --strict-markers\n    --disable-warnings\n    --cov=main\n    --cov-report=term-missing\n    --cov-report=html\n    --cov-fail-under=80\nmarkers =\n    slow: marks tests as slow (deselect with '-m \"not slow\"')\n    integration: marks tests as integration tests\n    security: marks tests as security tests\n    unit: marks tests as unit tests\nasyncio_mode = auto", "searchable_text": "[tool:pytest]\ntestpaths = tests\npython_files = test_*.py\npython_classes = Test*\npython_functions = test_*\naddopts = \n    -v\n    --tb=short\n    --strict-markers\n    --disable-warnings\n    --cov=main\n    --cov-report=term-missing\n    --cov-report=html\n    --cov-fail-under=80\nmarkers =\n    slow: marks tests as slow (deselect with '-m \"not slow\"')\n    integration: marks tests as integration tests\n    security: marks tests as security tests\n    unit: marks tests as unit tests\nasyncio_mode = auto"}, {"filepath": "query_analyzer.py", "start_line": 13, "end_line": 190, "type": "class", "name": "QueryAnalyzer", "content": "class QueryAnalyzer:\n    \"\"\"\n    Analyze user queries to determine intent and extract entities.\n    \"\"\"\n    \n    INTENT_PATTERNS = {\n        'find_function': [\n            r'where is (?:the )?(\\w+) function',\n            r'find (?:the )?(\\w+) function',\n            r'show me (?:the )?(\\w+) function',\n            r'locate (?:the )?(\\w+) function',\n        ],\n        'find_class': [\n            r'where is (?:the )?(\\w+) class',\n            r'find (?:the )?(\\w+) class',\n            r'show me (?:the )?(\\w+) class',\n            r'locate (?:the )?(\\w+) class',\n        ],\n        'find_usages': [\n            r'where is (\\w+) used',\n            r'find usages of (\\w+)',\n            r'who calls (\\w+)',\n            r'what calls (\\w+)',\n            r'find references to (\\w+)',\n        ],\n        'explain_code': [\n            r'explain (\\w+)',\n            r'what does (\\w+) do',\n            r'how does (\\w+) work',\n            r'describe (\\w+)',\n            r'tell me about (\\w+)',\n        ],\n        'find_bugs': [\n            r'find bugs',\n            r'security issues',\n            r'vulnerabilities',\n            r'code smells',\n            r'potential problems',\n            r'review.*security',\n        ],\n        'architecture': [\n            r'architecture',\n            r'design patterns',\n            r'structure',\n            r'how is.*organized',\n            r'project structure',\n        ],\n        'find_api': [\n            r'api endpoints',\n            r'find.*endpoints',\n            r'list.*routes',\n            r'show.*api',\n        ],\n        'dependencies': [\n            r'dependencies',\n            r'what.*depends on',\n            r'imports',\n            r'requirements',\n        ],\n        'refactor': [\n            r'refactor',\n            r'improve',\n            r'optimize',\n            r'better way',\n        ],\n        'test': [\n            r'test',\n            r'unit test',\n            r'how to test',\n        ]\n    }\n    \n    def analyze(self, query: str) -> Dict:\n        \"\"\"Classify query intent and extract entities.\"\"\"\n        query_lower = query.lower()\n        \n        for intent, patterns in self.INTENT_PATTERNS.items():\n            for pattern in patterns:\n                match = re.search(pattern, query_lower)\n                if match:\n                    entities = [e for e in match.groups() if e] if match.groups() else []\n                    return {\n                        'intent': intent,\n                        'entities': entities,\n                        'original_query': query,\n                        'confidence': 'high'\n                    }\n        \n        # Check for file-specific queries\n        if self._is_file_query(query):\n            return {\n                'intent': 'file_specific',\n                'entities': self._extract_filenames(query),\n                'original_query': query,\n                'confidence': 'medium'\n            }\n        \n        return {\n            'intent': 'general',\n            'entities': [],\n            'original_query': query,\n            'confidence': 'low'\n        }\n    \n    def _is_file_query(self, query: str) -> bool:\n        \"\"\"Check if query mentions specific files.\"\"\"\n        file_patterns = [\n            r'\\w+\\.(py|js|ts|jsx|tsx|java|cpp|c|h|go|rs|rb|php)',\n            r'in (?:the )?file',\n            r'in (?:the )?(\\w+/)+\\w+',\n        ]\n        \n        for pattern in file_patterns:\n            if re.search(pattern, query.lower()):\n                return True\n        return False\n    \n    def _extract_filenames(self, query: str) -> List[str]:\n        \"\"\"Extract filenames from query.\"\"\"\n        filenames = []\n        \n        # Match common file extensions\n        file_pattern = r'(\\w+(?:/\\w+)*\\.\\w+)'\n        matches = re.findall(file_pattern, query)\n        filenames.extend(matches)\n        \n        return filenames\n    \n    def get_search_keywords(self, query: str) -> List[str]:\n        \"\"\"Extract important keywords from query for search.\"\"\"\n        # Remove common stop words\n        stop_words = {\n            'the', 'a', 'an', 'is', 'are', 'was', 'were', 'be', 'been', 'being',\n            'have', 'has', 'had', 'do', 'does', 'did', 'will', 'would', 'should',\n            'could', 'can', 'may', 'might', 'must', 'what', 'where', 'when', 'why',\n            'how', 'which', 'who', 'whom', 'this', 'that', 'these', 'those',\n            'i', 'you', 'he', 'she', 'it', 'we', 'they', 'me', 'him', 'her', 'us', 'them'\n        }\n        \n        # Tokenize and filter\n        words = re.findall(r'\\b\\w+\\b', query.lower())\n        keywords = [w for w in words if w not in stop_words and len(w) > 2]\n        \n        return keywords\n    \n    def should_use_semantic_search(self, analysis: Dict) -> bool:\n        \"\"\"Determine if semantic search should be used.\"\"\"\n        # Use semantic search for general queries and explanations\n        semantic_intents = ['general', 'explain_code', 'architecture', 'find_bugs', 'refactor']\n        return analysis['intent'] in semantic_intents\n    \n    def should_use_symbol_search(self, analysis: Dict) -> bool:\n        \"\"\"Determine if symbol search should be used.\"\"\"\n        # Use symbol search for specific lookups\n        symbol_intents = ['find_function', 'find_class', 'find_usages']\n        return analysis['intent'] in symbol_intents\n    \n    def format_context_prompt(self, query: str, context: str, analysis: Dict) -> str:\n        \"\"\"Format the final prompt with context based on intent.\"\"\"\n        intent = analysis['intent']\n        \n        if intent == 'find_function' or intent == 'find_class':\n            return f\"{query}\\n\\n[DEFINITION]:\\n{context}\"\n        \n        elif intent == 'find_usages':\n            return f\"{query}\\n\\n[USAGES]:\\n{context}\"\n        \n        elif intent == 'explain_code':\n            return f\"{query}\\n\\n[RELEVANT CODE]:\\n{context}\"\n        \n        elif intent == 'architecture':\n            return f\"{query}\\n\\n[PROJECT STRUCTURE]:\\n{context}\"\n        \n        elif intent == 'find_bugs':\n            return f\"{query}\\n\\n[CODE TO REVIEW]:\\n{context}\\n\\nPlease analyze for bugs, security issues, and code quality problems.\"\n        \n        else:\n            return f\"{query}\\n\\n[RELEVANT CONTEXT]:\\n{context}\"", "docstring": "Analyze user queries to determine intent and extract entities.", "searchable_text": "QueryAnalyzer Analyze user queries to determine intent and extract entities. class QueryAnalyzer:\n    \"\"\"\n    Analyze user queries to determine intent and extract entities.\n    \"\"\"\n    \n    INTENT_PATTERNS = {\n        'find_function': [\n            r'where is (?:the )?(\\w+) function',\n            r'find (?:the )?(\\w+) function',\n            r'show me (?:the )?(\\w+) function',\n            r'locate (?:the )?(\\w+) function',\n        ],\n        'find_class': [\n            r'where is (?:the )?(\\w+) class',\n            r'find (?:the )?(\\w+) class',\n            r'show me (?:the )?(\\w+) class',\n            r'locate (?:the )?(\\w+) class',\n        ],\n        'find_usages': [\n            r'where is (\\w+) used',\n            r'find usages of (\\w+)',\n            r'who calls (\\w+)',\n            r'what calls (\\w+)',\n            r'find references to (\\w+)',\n        ],\n        'explain_code': [\n            r'explain (\\w+)',\n            r'what does (\\w+) do',\n            r'how does (\\w+) work',\n            r'describe (\\w+)',\n            r'tell me about (\\w+)',\n        ],\n        'find_bugs': [\n            r'find bugs',\n            r'security issues',\n            r'vulnerabilities',\n            r'code smells',\n            r'potential problems',\n            r'review.*security',\n        ],\n        'architecture': [\n            r'architecture',\n            r'design patterns',\n            r'structure',\n            r'how is.*organized',\n            r'project structure',\n        ],\n        'find_api': [\n            r'api endpoints',\n            r'find.*endpoints',\n            r'list.*routes',\n            r'show.*api',\n        ],\n        'dependencies': [\n            r'dependencies',\n            r'what.*depends on',\n            r'imports',\n            r'requirements',\n        ],\n        'refactor': [\n            r'refactor',\n            r'improve',\n            r'optimize',\n            r'better way',\n        ],\n        'test': [\n            r'test',\n            r'unit test',\n            r'how to test',\n        ]\n    }\n    \n    def analyze(self, query: str) -> Dict:\n        \"\"\"Classify query intent and extract entities.\"\"\"\n        query_lower = query.lower()\n        \n        for intent, patterns in self.INTENT_PATTERNS.items():\n            for pattern in patterns:\n                match = re.search(pattern, query_lower)\n                if match:\n                    entities = [e for e in match.groups() if e] if match.groups() else []\n                    return {\n                        'intent': intent,\n                        'entities': entities,\n                        'original_query': query,\n                        'confidence': 'high'\n                    }\n        \n        # Check for file-specific queries\n        if self._is_file_query(query):\n            return {\n                'intent': 'file_specific',\n                'entities': self._extract_filenames(query),\n                'original_query': query,\n                'confidence': 'medium'\n            }\n        \n        return {\n            'intent': 'general',\n            'entities': [],\n            'original_query': query,\n            'confidence': 'low'\n        }\n    \n    def _is_file_query(self, query: str) -> bool:\n        \"\"\"Check if query mentions specific files.\"\"\"\n        file_patterns = [\n            r'\\w+\\.(py|js|ts|jsx|tsx|java|cpp|c|h|go|rs|rb|php)',\n            r'in (?:the )?file',\n            r'in (?:the )?(\\w+/)+\\w+',\n        ]\n        \n        for pattern in file_patterns:\n            if re.search(pattern, query.lower()):\n                return True\n        return False\n    \n    def _extract_filenames(self, query: str) -> List[str]:\n        \"\"\"Extract filenames from query.\"\"\"\n        filenames = []\n        \n        # Match common file extensions\n        file_pattern = r'(\\w+(?:/\\w+)*\\.\\w+)'\n        matches = re.findall(file_pattern, query)\n        filenames.extend(matches)\n        \n        return filenames\n    \n    def get_search_keywords(self, query: str) -> List[str]:\n        \"\"\"Extract important keywords from query for search.\"\"\"\n        # Remove common stop words\n        stop_words = {\n            'the', 'a', 'an', 'is', 'are', 'was', 'were', 'be', 'been', 'being',\n            'have', 'has', 'had', 'do', 'does', 'did', 'will', 'would', 'should',\n            'could', 'can', 'may', 'might', 'must', 'what', 'where', 'when', 'why',\n            'how', 'which', 'who', 'whom', 'this', 'that', 'these', 'those',\n            'i', 'you', 'he', 'she', 'it', 'we', 'they', 'me', 'him', 'her', 'us', 'them'\n        }\n        \n        # Tokenize and filter\n        words = re.findall(r'\\b\\w+\\b', query.lower())\n        keywords = [w for w in words if w not in stop_words and len(w) > 2]\n        \n        return keywords\n    \n    def should_use_semantic_search(self, analysis: Dict) -> bool:\n        \"\"\"Determine if semantic search should be used.\"\"\"\n        # Use semantic search for general queries and explanations\n        semantic_intents = ['general', 'explain_code', 'architecture', 'find_bugs', 'refactor']\n        return analysis['intent'] in semantic_intents\n    \n    def should_use_symbol_search(self, analysis: Dict) -> bool:\n        \"\"\"Determine if symbol search should be used.\"\"\"\n        # Use symbol search for specific lookups\n        symbol_intents = ['find_function', 'find_class', 'find_usages']\n        return analysis['intent'] in symbol_intents\n    \n    def format_context_prompt(self, query: str, context: str, analysis: Dict) -> str:\n        \"\"\"Format the final prompt with context based on intent.\"\"\"\n        intent = analysis['intent']\n        \n        if intent == 'find_function' or intent == 'find_class':\n            return f\"{query}\\n\\n[DEFINITION]:\\n{context}\"\n        \n        elif intent == 'find_usages':\n            return f\"{query}\\n\\n[USAGES]:\\n{context}\"\n        \n        elif intent == 'explain_code':\n            return f\"{query}\\n\\n[RELEVANT CODE]:\\n{context}\"\n        \n        elif intent == 'architecture':\n            return f\"{query}\\n\\n[PROJECT STRUCTURE]:\\n{context}\"\n        \n        elif intent == 'find_bugs':\n            return f\"{query}\\n\\n[CODE TO REVIEW]:\\n{context}\\n\\nPlease analyze for bugs, security issues, and code quality problems.\"\n        \n        else:\n            return f\"{query}\\n\\n[RELEVANT CONTEXT]:\\n{context}\""}, {"filepath": "query_analyzer.py", "start_line": 85, "end_line": 115, "type": "function", "name": "analyze", "content": "    def analyze(self, query: str) -> Dict:\n        \"\"\"Classify query intent and extract entities.\"\"\"\n        query_lower = query.lower()\n        \n        for intent, patterns in self.INTENT_PATTERNS.items():\n            for pattern in patterns:\n                match = re.search(pattern, query_lower)\n                if match:\n                    entities = [e for e in match.groups() if e] if match.groups() else []\n                    return {\n                        'intent': intent,\n                        'entities': entities,\n                        'original_query': query,\n                        'confidence': 'high'\n                    }\n        \n        # Check for file-specific queries\n        if self._is_file_query(query):\n            return {\n                'intent': 'file_specific',\n                'entities': self._extract_filenames(query),\n                'original_query': query,\n                'confidence': 'medium'\n            }\n        \n        return {\n            'intent': 'general',\n            'entities': [],\n            'original_query': query,\n            'confidence': 'low'\n        }", "docstring": "Classify query intent and extract entities.", "searchable_text": "analyze Classify query intent and extract entities.     def analyze(self, query: str) -> Dict:\n        \"\"\"Classify query intent and extract entities.\"\"\"\n        query_lower = query.lower()\n        \n        for intent, patterns in self.INTENT_PATTERNS.items():\n            for pattern in patterns:\n                match = re.search(pattern, query_lower)\n                if match:\n                    entities = [e for e in match.groups() if e] if match.groups() else []\n                    return {\n                        'intent': intent,\n                        'entities': entities,\n                        'original_query': query,\n                        'confidence': 'high'\n                    }\n        \n        # Check for file-specific queries\n        if self._is_file_query(query):\n            return {\n                'intent': 'file_specific',\n                'entities': self._extract_filenames(query),\n                'original_query': query,\n                'confidence': 'medium'\n            }\n        \n        return {\n            'intent': 'general',\n            'entities': [],\n            'original_query': query,\n            'confidence': 'low'\n        }"}, {"filepath": "query_analyzer.py", "start_line": 117, "end_line": 128, "type": "function", "name": "_is_file_query", "content": "    def _is_file_query(self, query: str) -> bool:\n        \"\"\"Check if query mentions specific files.\"\"\"\n        file_patterns = [\n            r'\\w+\\.(py|js|ts|jsx|tsx|java|cpp|c|h|go|rs|rb|php)',\n            r'in (?:the )?file',\n            r'in (?:the )?(\\w+/)+\\w+',\n        ]\n        \n        for pattern in file_patterns:\n            if re.search(pattern, query.lower()):\n                return True\n        return False", "docstring": "Check if query mentions specific files.", "searchable_text": "_is_file_query Check if query mentions specific files.     def _is_file_query(self, query: str) -> bool:\n        \"\"\"Check if query mentions specific files.\"\"\"\n        file_patterns = [\n            r'\\w+\\.(py|js|ts|jsx|tsx|java|cpp|c|h|go|rs|rb|php)',\n            r'in (?:the )?file',\n            r'in (?:the )?(\\w+/)+\\w+',\n        ]\n        \n        for pattern in file_patterns:\n            if re.search(pattern, query.lower()):\n                return True\n        return False"}, {"filepath": "query_analyzer.py", "start_line": 130, "end_line": 139, "type": "function", "name": "_extract_filenames", "content": "    def _extract_filenames(self, query: str) -> List[str]:\n        \"\"\"Extract filenames from query.\"\"\"\n        filenames = []\n        \n        # Match common file extensions\n        file_pattern = r'(\\w+(?:/\\w+)*\\.\\w+)'\n        matches = re.findall(file_pattern, query)\n        filenames.extend(matches)\n        \n        return filenames", "docstring": "Extract filenames from query.", "searchable_text": "_extract_filenames Extract filenames from query.     def _extract_filenames(self, query: str) -> List[str]:\n        \"\"\"Extract filenames from query.\"\"\"\n        filenames = []\n        \n        # Match common file extensions\n        file_pattern = r'(\\w+(?:/\\w+)*\\.\\w+)'\n        matches = re.findall(file_pattern, query)\n        filenames.extend(matches)\n        \n        return filenames"}, {"filepath": "query_analyzer.py", "start_line": 141, "end_line": 156, "type": "function", "name": "get_search_keywords", "content": "    def get_search_keywords(self, query: str) -> List[str]:\n        \"\"\"Extract important keywords from query for search.\"\"\"\n        # Remove common stop words\n        stop_words = {\n            'the', 'a', 'an', 'is', 'are', 'was', 'were', 'be', 'been', 'being',\n            'have', 'has', 'had', 'do', 'does', 'did', 'will', 'would', 'should',\n            'could', 'can', 'may', 'might', 'must', 'what', 'where', 'when', 'why',\n            'how', 'which', 'who', 'whom', 'this', 'that', 'these', 'those',\n            'i', 'you', 'he', 'she', 'it', 'we', 'they', 'me', 'him', 'her', 'us', 'them'\n        }\n        \n        # Tokenize and filter\n        words = re.findall(r'\\b\\w+\\b', query.lower())\n        keywords = [w for w in words if w not in stop_words and len(w) > 2]\n        \n        return keywords", "docstring": "Extract important keywords from query for search.", "searchable_text": "get_search_keywords Extract important keywords from query for search.     def get_search_keywords(self, query: str) -> List[str]:\n        \"\"\"Extract important keywords from query for search.\"\"\"\n        # Remove common stop words\n        stop_words = {\n            'the', 'a', 'an', 'is', 'are', 'was', 'were', 'be', 'been', 'being',\n            'have', 'has', 'had', 'do', 'does', 'did', 'will', 'would', 'should',\n            'could', 'can', 'may', 'might', 'must', 'what', 'where', 'when', 'why',\n            'how', 'which', 'who', 'whom', 'this', 'that', 'these', 'those',\n            'i', 'you', 'he', 'she', 'it', 'we', 'they', 'me', 'him', 'her', 'us', 'them'\n        }\n        \n        # Tokenize and filter\n        words = re.findall(r'\\b\\w+\\b', query.lower())\n        keywords = [w for w in words if w not in stop_words and len(w) > 2]\n        \n        return keywords"}, {"filepath": "query_analyzer.py", "start_line": 158, "end_line": 162, "type": "function", "name": "should_use_semantic_search", "content": "    def should_use_semantic_search(self, analysis: Dict) -> bool:\n        \"\"\"Determine if semantic search should be used.\"\"\"\n        # Use semantic search for general queries and explanations\n        semantic_intents = ['general', 'explain_code', 'architecture', 'find_bugs', 'refactor']\n        return analysis['intent'] in semantic_intents", "docstring": "Determine if semantic search should be used.", "searchable_text": "should_use_semantic_search Determine if semantic search should be used.     def should_use_semantic_search(self, analysis: Dict) -> bool:\n        \"\"\"Determine if semantic search should be used.\"\"\"\n        # Use semantic search for general queries and explanations\n        semantic_intents = ['general', 'explain_code', 'architecture', 'find_bugs', 'refactor']\n        return analysis['intent'] in semantic_intents"}, {"filepath": "query_analyzer.py", "start_line": 164, "end_line": 168, "type": "function", "name": "should_use_symbol_search", "content": "    def should_use_symbol_search(self, analysis: Dict) -> bool:\n        \"\"\"Determine if symbol search should be used.\"\"\"\n        # Use symbol search for specific lookups\n        symbol_intents = ['find_function', 'find_class', 'find_usages']\n        return analysis['intent'] in symbol_intents", "docstring": "Determine if symbol search should be used.", "searchable_text": "should_use_symbol_search Determine if symbol search should be used.     def should_use_symbol_search(self, analysis: Dict) -> bool:\n        \"\"\"Determine if symbol search should be used.\"\"\"\n        # Use symbol search for specific lookups\n        symbol_intents = ['find_function', 'find_class', 'find_usages']\n        return analysis['intent'] in symbol_intents"}, {"filepath": "query_analyzer.py", "start_line": 170, "end_line": 190, "type": "function", "name": "format_context_prompt", "content": "    def format_context_prompt(self, query: str, context: str, analysis: Dict) -> str:\n        \"\"\"Format the final prompt with context based on intent.\"\"\"\n        intent = analysis['intent']\n        \n        if intent == 'find_function' or intent == 'find_class':\n            return f\"{query}\\n\\n[DEFINITION]:\\n{context}\"\n        \n        elif intent == 'find_usages':\n            return f\"{query}\\n\\n[USAGES]:\\n{context}\"\n        \n        elif intent == 'explain_code':\n            return f\"{query}\\n\\n[RELEVANT CODE]:\\n{context}\"\n        \n        elif intent == 'architecture':\n            return f\"{query}\\n\\n[PROJECT STRUCTURE]:\\n{context}\"\n        \n        elif intent == 'find_bugs':\n            return f\"{query}\\n\\n[CODE TO REVIEW]:\\n{context}\\n\\nPlease analyze for bugs, security issues, and code quality problems.\"\n        \n        else:\n            return f\"{query}\\n\\n[RELEVANT CONTEXT]:\\n{context}\"", "docstring": "Format the final prompt with context based on intent.", "searchable_text": "format_context_prompt Format the final prompt with context based on intent.     def format_context_prompt(self, query: str, context: str, analysis: Dict) -> str:\n        \"\"\"Format the final prompt with context based on intent.\"\"\"\n        intent = analysis['intent']\n        \n        if intent == 'find_function' or intent == 'find_class':\n            return f\"{query}\\n\\n[DEFINITION]:\\n{context}\"\n        \n        elif intent == 'find_usages':\n            return f\"{query}\\n\\n[USAGES]:\\n{context}\"\n        \n        elif intent == 'explain_code':\n            return f\"{query}\\n\\n[RELEVANT CODE]:\\n{context}\"\n        \n        elif intent == 'architecture':\n            return f\"{query}\\n\\n[PROJECT STRUCTURE]:\\n{context}\"\n        \n        elif intent == 'find_bugs':\n            return f\"{query}\\n\\n[CODE TO REVIEW]:\\n{context}\\n\\nPlease analyze for bugs, security issues, and code quality problems.\"\n        \n        else:\n            return f\"{query}\\n\\n[RELEVANT CONTEXT]:\\n{context}\""}, {"filepath": "README.md", "start_line": 1, "end_line": 50, "type": "block", "name": "block_0", "content": "# MapleCLI\n\nA secure, feature-rich command-line interface for OpenAI-compatible APIs with **Augment-level intelligent code analysis**. \ud83d\ude80\n\n## \u2728 NEW: Intelligent Context Retrieval\n\nMapleCLI now features **semantic search and query-aware code understanding** similar to Augment CLI:\n\n- \ud83e\udde0 **Semantic Search** - Finds relevant code using AI embeddings, not just keywords\n- \ud83d\udd0d **Symbol Resolution** - Instantly locate function/class definitions and usages\n- \ud83c\udfaf **Query Intent Classification** - Understands what you're asking and retrieves the right context\n- \u26a1 **Fast & Efficient** - Only retrieves relevant code snippets, not entire files\n- \ud83d\udcbe **Smart Caching** - Index built once, reused for instant searches\n\n**[\ud83d\udcd6 Quick Start Guide](QUICKSTART_INTELLIGENT.md)** | **[\ud83d\udcda Full Documentation](INTELLIGENT_FEATURES.md)**\n\n### Installation with Intelligent Features\n```bash\npip install -e \".[intelligent]\"\n```\n\n## Features\n\n\u2728 **Multi-modal Support**: Chat, image generation, video generation, and text-to-speech  \n\ud83c\udfa8 **Rich Terminal UI**: Beautiful console output with Rich library  \n\u2699\ufe0f **Interactive Commands**: Qwen-style commands with `:` or `/` prefix  \n\ud83d\udd27 **Runtime Configuration**: Change temperature, tokens, seed on-the-fly  \n\ud83d\udcbe **Session Management**: Save/load conversations  \n\ud83c\udfb2 **Random Seed Control**: Reproducible outputs  \n\ud83d\ude80 **YOLO Mode**: Analyze entire codebases and get AI-powered insights!  \n\ud83d\udcc1 **Project Switching**: Navigate between different projects seamlessly  \n\ud83d\udd12 **Enhanced Security**: Path traversal protection, secure API key handling  \n\ud83d\udcca **Advanced Analysis**: Dependency extraction, complexity metrics, architecture patterns  \n\n## Installation\n\n### Quick Install\n```bash\npip install -e .\n```\n\n### Development Install\n```bash\ngit clone https://github.com/maplecli/maplecli.git\ncd maplecli\npip install -e \".[dev]\"\n```\n\n## Usage\n", "searchable_text": "# MapleCLI\n\nA secure, feature-rich command-line interface for OpenAI-compatible APIs with **Augment-level intelligent code analysis**. \ud83d\ude80\n\n## \u2728 NEW: Intelligent Context Retrieval\n\nMapleCLI now features **semantic search and query-aware code understanding** similar to Augment CLI:\n\n- \ud83e\udde0 **Semantic Search** - Finds relevant code using AI embeddings, not just keywords\n- \ud83d\udd0d **Symbol Resolution** - Instantly locate function/class definitions and usages\n- \ud83c\udfaf **Query Intent Classification** - Understands what you're asking and retrieves the right context\n- \u26a1 **Fast & Efficient** - Only retrieves relevant code snippets, not entire files\n- \ud83d\udcbe **Smart Caching** - Index built once, reused for instant searches\n\n**[\ud83d\udcd6 Quick Start Guide](QUICKSTART_INTELLIGENT.md)** | **[\ud83d\udcda Full Documentation](INTELLIGENT_FEATURES.md)**\n\n### Installation with Intelligent Features\n```bash\npip install -e \".[intelligent]\"\n```\n\n## Features\n\n\u2728 **Multi-modal Support**: Chat, image generation, video generation, and text-to-speech  \n\ud83c\udfa8 **Rich Terminal UI**: Beautiful console output with Rich library  \n\u2699\ufe0f **Interactive Commands**: Qwen-style commands with `:` or `/` prefix  \n\ud83d\udd27 **Runtime Configuration**: Change temperature, tokens, seed on-the-fly  \n\ud83d\udcbe **Session Management**: Save/load conversations  \n\ud83c\udfb2 **Random Seed Control**: Reproducible outputs  \n\ud83d\ude80 **YOLO Mode**: Analyze entire codebases and get AI-powered insights!  \n\ud83d\udcc1 **Project Switching**: Navigate between different projects seamlessly  \n\ud83d\udd12 **Enhanced Security**: Path traversal protection, secure API key handling  \n\ud83d\udcca **Advanced Analysis**: Dependency extraction, complexity metrics, architecture patterns  \n\n## Installation\n\n### Quick Install\n```bash\npip install -e .\n```\n\n### Development Install\n```bash\ngit clone https://github.com/maplecli/maplecli.git\ncd maplecli\npip install -e \".[dev]\"\n```\n\n## Usage\n"}, {"filepath": "README.md", "start_line": 51, "end_line": 100, "type": "block", "name": "block_50", "content": "### Chat\n```bash\n# Start interactive chat\nmaplecli chat\n\n# With specific model and settings\nmaplecli chat --model gpt-4 --temperature 0.9 --system \"You are a helpful coding assistant\"\n```\n\n### Image Generation\n```bash\n# Generate images\nmaplecli image \"sunset over mountains\"\nmaplecli image \"logo design\" --model dall-e-3 --size 1792x1024\n```\n\n### Video Generation\n```bash\n# Generate videos\nmaplecli video \"ocean waves\"\nmaplecli video \"fireworks display\" --model sora-2\n```\n\n### Text-to-Speech\n```bash\n# Convert text to speech\nmaplecli tts \"Hello world\"\nmaplecli tts \"Story time\" --voice nova --output story.mp3\n```\n\n### List Models\n```bash\n# List available models\nmaplecli models\nmaplecli models --type chat\nmaplecli models --type image\n```\n\n## Interactive Chat Commands\n\nWhen in chat mode, use these commands (with `:` or `/` prefix):\n\n### Essential Commands\n- `:help` or `:h` - Show help message\n- `:exit` or `:q` - Exit chat session\n- `:clear` or `:cl` - Clear screen\n- `:clear-history` or `:clh` - Clear conversation history\n\n### Session Management\n- `:save <file>` - Save conversation to file", "searchable_text": "### Chat\n```bash\n# Start interactive chat\nmaplecli chat\n\n# With specific model and settings\nmaplecli chat --model gpt-4 --temperature 0.9 --system \"You are a helpful coding assistant\"\n```\n\n### Image Generation\n```bash\n# Generate images\nmaplecli image \"sunset over mountains\"\nmaplecli image \"logo design\" --model dall-e-3 --size 1792x1024\n```\n\n### Video Generation\n```bash\n# Generate videos\nmaplecli video \"ocean waves\"\nmaplecli video \"fireworks display\" --model sora-2\n```\n\n### Text-to-Speech\n```bash\n# Convert text to speech\nmaplecli tts \"Hello world\"\nmaplecli tts \"Story time\" --voice nova --output story.mp3\n```\n\n### List Models\n```bash\n# List available models\nmaplecli models\nmaplecli models --type chat\nmaplecli models --type image\n```\n\n## Interactive Chat Commands\n\nWhen in chat mode, use these commands (with `:` or `/` prefix):\n\n### Essential Commands\n- `:help` or `:h` - Show help message\n- `:exit` or `:q` - Exit chat session\n- `:clear` or `:cl` - Clear screen\n- `:clear-history` or `:clh` - Clear conversation history\n\n### Session Management\n- `:save <file>` - Save conversation to file"}, {"filepath": "README.md", "start_line": 101, "end_line": 150, "type": "block", "name": "block_100", "content": "- `:load <file>` - Load conversation from file\n- `:history` or `:his` - Show conversation history\n\n### Configuration\n- `:conf` - Show current configuration\n- `:conf temperature=0.8` - Set temperature\n- `:conf max_tokens=1000` - Set max tokens\n- `:reset-conf` - Reset to defaults\n\n### YOLO Mode (Code Analysis) \ud83d\ude80\n- `:yolo` - Toggle YOLO mode on/off\n- `:analyze` - Scan and analyze current project (auto-reads key files)\n- `:read <file>` - Read a specific file and add to context\n- `:files` - List all files in current project\n- `:files <pattern>` - Filter files by pattern (e.g., `:files tsx`)\n- `:project` - Show current project path and recent projects\n- `:project <path>` - Switch to a different project\n- `:cd <path>` - Change directory (alias for :project)\n\n### Advanced\n- `:seed` - Show current random seed\n- `:seed 42` - Set random seed for reproducibility\n- `:model` - Show current model\n\n## YOLO Mode - Intelligent Code Analysis\n\n### Example Workflow\n```\n:yolo                           # Enable YOLO mode\n:cd ~/my-project                # Switch to your project\n:analyze                        # Build intelligent semantic index\n\"Where is the login function?\"  # Ask questions - AI retrieves relevant code automatically\n```\n\n### What :analyze Does (NEW - Intelligent Mode)\n- \ud83d\udcca Scans entire project structure\n- \ud83e\udde0 **Builds semantic index** with embeddings (functions, classes, methods)\n- \ud83d\udd0d **Analyzes symbols** and dependencies\n- \ud83d\udcc8 Calculates complexity metrics\n- \ud83c\udfd7\ufe0f Identifies architectural patterns\n- \ud83d\udcbe **Caches index** for instant future searches\n- \u26a1 **Query-aware retrieval** - only fetches relevant code for each question\n\n### Intelligent Query Examples\n```\n\"Where is the authenticate function?\"     # \u2192 Symbol search\n\"How does payment processing work?\"       # \u2192 Semantic search\n\"Find all API endpoints\"                  # \u2192 Pattern search\n\"What security issues exist?\"             # \u2192 Code analysis\n\"Explain the database schema\"             # \u2192 Architecture overview", "searchable_text": "- `:load <file>` - Load conversation from file\n- `:history` or `:his` - Show conversation history\n\n### Configuration\n- `:conf` - Show current configuration\n- `:conf temperature=0.8` - Set temperature\n- `:conf max_tokens=1000` - Set max tokens\n- `:reset-conf` - Reset to defaults\n\n### YOLO Mode (Code Analysis) \ud83d\ude80\n- `:yolo` - Toggle YOLO mode on/off\n- `:analyze` - Scan and analyze current project (auto-reads key files)\n- `:read <file>` - Read a specific file and add to context\n- `:files` - List all files in current project\n- `:files <pattern>` - Filter files by pattern (e.g., `:files tsx`)\n- `:project` - Show current project path and recent projects\n- `:project <path>` - Switch to a different project\n- `:cd <path>` - Change directory (alias for :project)\n\n### Advanced\n- `:seed` - Show current random seed\n- `:seed 42` - Set random seed for reproducibility\n- `:model` - Show current model\n\n## YOLO Mode - Intelligent Code Analysis\n\n### Example Workflow\n```\n:yolo                           # Enable YOLO mode\n:cd ~/my-project                # Switch to your project\n:analyze                        # Build intelligent semantic index\n\"Where is the login function?\"  # Ask questions - AI retrieves relevant code automatically\n```\n\n### What :analyze Does (NEW - Intelligent Mode)\n- \ud83d\udcca Scans entire project structure\n- \ud83e\udde0 **Builds semantic index** with embeddings (functions, classes, methods)\n- \ud83d\udd0d **Analyzes symbols** and dependencies\n- \ud83d\udcc8 Calculates complexity metrics\n- \ud83c\udfd7\ufe0f Identifies architectural patterns\n- \ud83d\udcbe **Caches index** for instant future searches\n- \u26a1 **Query-aware retrieval** - only fetches relevant code for each question\n\n### Intelligent Query Examples\n```\n\"Where is the authenticate function?\"     # \u2192 Symbol search\n\"How does payment processing work?\"       # \u2192 Semantic search\n\"Find all API endpoints\"                  # \u2192 Pattern search\n\"What security issues exist?\"             # \u2192 Code analysis\n\"Explain the database schema\"             # \u2192 Architecture overview"}, {"filepath": "README.md", "start_line": 151, "end_line": 200, "type": "block", "name": "block_150", "content": "```\n\n### Advanced Analysis Features\n- **Dependency Extraction**: Automatically identifies imports and dependencies\n- **Complexity Metrics**: Cyclomatic and cognitive complexity analysis\n- **Architecture Detection**: Identifies MVC, microservices, serverless patterns\n- **Security Scanning**: Path traversal protection and secure file handling\n- **Performance Monitoring**: Memory usage tracking and file size limits\n\n## Configuration\n\n### First-time Setup\nFirst run asks for API base URL and key. Saved to platform-specific secure location:\n- **Windows**: `%APPDATA%\\maplecli\\config.json`\n- **macOS**: `~/Library/Application Support/maplecli/config.json`\n- **Linux**: `~/.config/maplecli/config.json`\n\n### Environment Variables\n```bash\nexport OPENAI_API_BASE=\"https://api.openai.com/v1\"\nexport OPENAI_API_KEY=\"sk-your-key\"\n```\n\n### Security Features\n- **Secure API Key Input**: Hidden input using `getpass`\n- **Path Traversal Protection**: Prevents access outside project directory\n- **File Size Limits**: 10MB per file, 100MB total project size\n- **Atomic Config Updates**: Safe configuration file operations\n- **Cross-platform Permissions**: Proper file permissions on all platforms\n\n## Shortcuts\n\n### PowerShell Functions\nAdd to your PowerShell `$PROFILE`:\n```powershell\nfunction maple {\n    $env:OPENAI_API_BASE = \"https://api.mapleai.de/v1\"\n    $env:OPENAI_API_KEY = \"your-key\"\n    maplecli @args\n}\n\nfunction gemini {\n    $env:OPENAI_API_BASE = \"https://generativelanguage.googleapis.com/v1beta/openai\"\n    $env:OPENAI_API_KEY = \"your-key\"\n    maplecli @args\n}\n```\n\nUsage:\n```powershell", "searchable_text": "```\n\n### Advanced Analysis Features\n- **Dependency Extraction**: Automatically identifies imports and dependencies\n- **Complexity Metrics**: Cyclomatic and cognitive complexity analysis\n- **Architecture Detection**: Identifies MVC, microservices, serverless patterns\n- **Security Scanning**: Path traversal protection and secure file handling\n- **Performance Monitoring**: Memory usage tracking and file size limits\n\n## Configuration\n\n### First-time Setup\nFirst run asks for API base URL and key. Saved to platform-specific secure location:\n- **Windows**: `%APPDATA%\\maplecli\\config.json`\n- **macOS**: `~/Library/Application Support/maplecli/config.json`\n- **Linux**: `~/.config/maplecli/config.json`\n\n### Environment Variables\n```bash\nexport OPENAI_API_BASE=\"https://api.openai.com/v1\"\nexport OPENAI_API_KEY=\"sk-your-key\"\n```\n\n### Security Features\n- **Secure API Key Input**: Hidden input using `getpass`\n- **Path Traversal Protection**: Prevents access outside project directory\n- **File Size Limits**: 10MB per file, 100MB total project size\n- **Atomic Config Updates**: Safe configuration file operations\n- **Cross-platform Permissions**: Proper file permissions on all platforms\n\n## Shortcuts\n\n### PowerShell Functions\nAdd to your PowerShell `$PROFILE`:\n```powershell\nfunction maple {\n    $env:OPENAI_API_BASE = \"https://api.mapleai.de/v1\"\n    $env:OPENAI_API_KEY = \"your-key\"\n    maplecli @args\n}\n\nfunction gemini {\n    $env:OPENAI_API_BASE = \"https://generativelanguage.googleapis.com/v1beta/openai\"\n    $env:OPENAI_API_KEY = \"your-key\"\n    maplecli @args\n}\n```\n\nUsage:\n```powershell"}, {"filepath": "README.md", "start_line": 201, "end_line": 250, "type": "block", "name": "block_200", "content": "maple chat --model deepseek-v3.2-exp\ngemini chat --model gemini-2.5-pro\n```\n\n## Examples\n\n### Basic Chat with System Prompt\n```bash\nmaplecli chat --system \"You are a helpful coding assistant\"\n```\n\n### Image Generation with Custom Settings\n```bash\nmaplecli image \"cyberpunk city\" --model dall-e-3 --quality hd --size 1792x1024\n```\n\n### Video Generation\n```bash\nmaplecli video \"time-lapse of sunset over ocean\"\n```\n\n### Text-to-Speech with Custom Voice\n```bash\nmaplecli tts \"Welcome to MapleCLI\" --voice nova --speed 1.2\n```\n\n### Code Analysis Workflow\n```bash\nmaplecli chat\n:yolo\n:cd ~/my-project\n:analyze\n\"Review this codebase for security issues\"\n\"Explain the architecture\"\n\"Find all API endpoints\"\n```\n\n## Security Considerations\n\n- **API Key Protection**: Keys are stored with restricted permissions\n- **Input Validation**: All file paths are validated and sanitized\n- **Memory Limits**: Prevents resource exhaustion attacks\n- **Error Handling**: Comprehensive error logging without exposing sensitive data\n- **Audit Logging**: Security events are logged for monitoring\n\n## Development\n\n### Setup Development Environment\n```bash\ngit clone https://github.com/maplecli/maplecli.git", "searchable_text": "maple chat --model deepseek-v3.2-exp\ngemini chat --model gemini-2.5-pro\n```\n\n## Examples\n\n### Basic Chat with System Prompt\n```bash\nmaplecli chat --system \"You are a helpful coding assistant\"\n```\n\n### Image Generation with Custom Settings\n```bash\nmaplecli image \"cyberpunk city\" --model dall-e-3 --quality hd --size 1792x1024\n```\n\n### Video Generation\n```bash\nmaplecli video \"time-lapse of sunset over ocean\"\n```\n\n### Text-to-Speech with Custom Voice\n```bash\nmaplecli tts \"Welcome to MapleCLI\" --voice nova --speed 1.2\n```\n\n### Code Analysis Workflow\n```bash\nmaplecli chat\n:yolo\n:cd ~/my-project\n:analyze\n\"Review this codebase for security issues\"\n\"Explain the architecture\"\n\"Find all API endpoints\"\n```\n\n## Security Considerations\n\n- **API Key Protection**: Keys are stored with restricted permissions\n- **Input Validation**: All file paths are validated and sanitized\n- **Memory Limits**: Prevents resource exhaustion attacks\n- **Error Handling**: Comprehensive error logging without exposing sensitive data\n- **Audit Logging**: Security events are logged for monitoring\n\n## Development\n\n### Setup Development Environment\n```bash\ngit clone https://github.com/maplecli/maplecli.git"}, {"filepath": "README.md", "start_line": 251, "end_line": 300, "type": "block", "name": "block_250", "content": "cd maplecli\npip install -e \".[dev,security]\"\n```\n\n### Run Tests\n```bash\npytest\npytest --cov=maplecli\n```\n\n### Code Quality\n```bash\nblack main.py\nflake8 main.py\nmypy main.py\nbandit main.py\n```\n\n## Architecture\n\n### Core Components\n- **ConfigManager**: Secure configuration management\n- **ChatClient**: API communication with retry logic\n- **CodeAnalyzer**: Advanced code analysis with security\n- **CLI**: Command-line interface and user interaction\n\n### Security Features\n- **Path Validation**: Prevents directory traversal attacks\n- **Size Limits**: Memory and file size protection\n- **Error Handling**: Secure error reporting\n- **Logging**: Comprehensive audit trails\n\n## Performance\n\n### Optimizations\n- **Async Operations**: Non-blocking file operations\n- **Memory Management**: Bounded memory usage\n- **Progress Tracking**: Real-time progress indicators\n- **Caching**: Intelligent file analysis caching\n\n### Limits\n- **Max File Size**: 10MB per file\n- **Max Project Size**: 100MB total\n- **Max Depth**: 10 directory levels\n- **Max Files**: 1000 files per analysis\n\n## Troubleshooting\n\n### Common Issues\n", "searchable_text": "cd maplecli\npip install -e \".[dev,security]\"\n```\n\n### Run Tests\n```bash\npytest\npytest --cov=maplecli\n```\n\n### Code Quality\n```bash\nblack main.py\nflake8 main.py\nmypy main.py\nbandit main.py\n```\n\n## Architecture\n\n### Core Components\n- **ConfigManager**: Secure configuration management\n- **ChatClient**: API communication with retry logic\n- **CodeAnalyzer**: Advanced code analysis with security\n- **CLI**: Command-line interface and user interaction\n\n### Security Features\n- **Path Validation**: Prevents directory traversal attacks\n- **Size Limits**: Memory and file size protection\n- **Error Handling**: Secure error reporting\n- **Logging**: Comprehensive audit trails\n\n## Performance\n\n### Optimizations\n- **Async Operations**: Non-blocking file operations\n- **Memory Management**: Bounded memory usage\n- **Progress Tracking**: Real-time progress indicators\n- **Caching**: Intelligent file analysis caching\n\n### Limits\n- **Max File Size**: 10MB per file\n- **Max Project Size**: 100MB total\n- **Max Depth**: 10 directory levels\n- **Max Files**: 1000 files per analysis\n\n## Troubleshooting\n\n### Common Issues\n"}, {"filepath": "README.md", "start_line": 301, "end_line": 350, "type": "block", "name": "block_300", "content": "**Permission Denied**\n```bash\n# On Unix/Linux/macOS\nchmod +x install.sh\n\n# On Windows\n# Run PowerShell as Administrator\n```\n\n**Module Not Found**\n```bash\npip install -e .\n# or\npip install rich requests aiofiles\n```\n\n**Configuration Issues**\n```bash\n# Reset configuration\nrm -rf ~/.config/maplecli  # Linux\nrm -rf ~/Library/Application\\ Support/maplecli  # macOS\nrm -rf %APPDATA%\\maplecli  # Windows\n```\n\n### Debug Mode\n```bash\n# Enable debug logging\nexport MAPLECLI_DEBUG=1\nmaplecli chat\n```\n\n## Contributing\n\n1. Fork the repository\n2. Create a feature branch\n3. Make your changes\n4. Add tests\n5. Run the test suite\n6. Submit a pull request\n\n### Development Guidelines\n- Follow PEP 8 style guidelines\n- Add type hints for all functions\n- Include comprehensive error handling\n- Add security considerations for new features\n- Update documentation for changes\n\n## License\n\nMIT License - see LICENSE file for details.", "searchable_text": "**Permission Denied**\n```bash\n# On Unix/Linux/macOS\nchmod +x install.sh\n\n# On Windows\n# Run PowerShell as Administrator\n```\n\n**Module Not Found**\n```bash\npip install -e .\n# or\npip install rich requests aiofiles\n```\n\n**Configuration Issues**\n```bash\n# Reset configuration\nrm -rf ~/.config/maplecli  # Linux\nrm -rf ~/Library/Application\\ Support/maplecli  # macOS\nrm -rf %APPDATA%\\maplecli  # Windows\n```\n\n### Debug Mode\n```bash\n# Enable debug logging\nexport MAPLECLI_DEBUG=1\nmaplecli chat\n```\n\n## Contributing\n\n1. Fork the repository\n2. Create a feature branch\n3. Make your changes\n4. Add tests\n5. Run the test suite\n6. Submit a pull request\n\n### Development Guidelines\n- Follow PEP 8 style guidelines\n- Add type hints for all functions\n- Include comprehensive error handling\n- Add security considerations for new features\n- Update documentation for changes\n\n## License\n\nMIT License - see LICENSE file for details."}, {"filepath": "README.md", "start_line": 351, "end_line": 380, "type": "block", "name": "block_350", "content": "\n## Changelog\n\n### v2.0.0\n- \ud83d\udd12 Enhanced security with path traversal protection\n- \ud83d\udcca Advanced code analysis algorithms\n- \ud83d\ude80 Async operations for better performance\n- \ud83d\udcc8 Memory management and size limits\n- \ud83d\udee0\ufe0f Cross-platform compatibility improvements\n- \ud83d\udcdd Comprehensive error handling and logging\n\n### v1.0.0\n- \ud83c\udf89 Initial release\n- \ud83d\udcac Basic chat functionality\n- \ud83d\uddbc\ufe0f Image generation\n- \ud83c\udfa5 Video generation\n- \ud83d\udde3\ufe0f Text-to-speech\n- \ud83d\udcc1 YOLO mode code analysis\n\n## Support\n\n- **Documentation**: https://maplecli.readthedocs.io/\n- **Issues**: https://github.com/maplecli/maplecli/issues\n- **Discussions**: https://github.com/maplecli/maplecli/discussions\n- **Email**: team@maplecli.dev\n\n---\n\n**MapleCLI** - Secure, powerful, and intelligent AI interaction for developers. \ud83d\ude80\n", "searchable_text": "\n## Changelog\n\n### v2.0.0\n- \ud83d\udd12 Enhanced security with path traversal protection\n- \ud83d\udcca Advanced code analysis algorithms\n- \ud83d\ude80 Async operations for better performance\n- \ud83d\udcc8 Memory management and size limits\n- \ud83d\udee0\ufe0f Cross-platform compatibility improvements\n- \ud83d\udcdd Comprehensive error handling and logging\n\n### v1.0.0\n- \ud83c\udf89 Initial release\n- \ud83d\udcac Basic chat functionality\n- \ud83d\uddbc\ufe0f Image generation\n- \ud83c\udfa5 Video generation\n- \ud83d\udde3\ufe0f Text-to-speech\n- \ud83d\udcc1 YOLO mode code analysis\n\n## Support\n\n- **Documentation**: https://maplecli.readthedocs.io/\n- **Issues**: https://github.com/maplecli/maplecli/issues\n- **Discussions**: https://github.com/maplecli/maplecli/discussions\n- **Email**: team@maplecli.dev\n\n---\n\n**MapleCLI** - Secure, powerful, and intelligent AI interaction for developers. \ud83d\ude80\n"}, {"filepath": "setup.py", "start_line": 1, "end_line": 50, "type": "block", "name": "block_0", "content": "\"\"\"Setup script for MapleCLI - Enhanced OpenAI-compatible CLI with code analysis\"\"\"\nfrom setuptools import setup, find_packages\n\nsetup(\n    name=\"maplecli\",\n    version=\"2.0.0\",\n    description=\"A secure, feature-rich CLI for OpenAI-compatible APIs with advanced code analysis\",\n    author=\"MapleCLI Team\",\n    author_email=\"team@maplecli.dev\",\n    url=\"https://github.com/maplecli/maplecli\",\n    py_modules=[\"main\", \"cli\", \"chat_client\", \"code_analyzer\", \"config_manager\", \"context_engine\", \"logger\", \"query_analyzer\", \"symbol_resolver\"],\n    install_requires=[\n        \"rich>=13.0.0\",\n        \"requests>=2.31.0\",\n        \"aiofiles>=23.0.0\",\n        \"pathlib2>=2.3.0; python_version<'3.4'\",\n    ],\n    extras_require={\n        \"intelligent\": [\n            \"sentence-transformers>=2.2.0\",  # For semantic search\n            \"faiss-cpu>=1.7.4\",  # For vector search (use faiss-gpu for GPU support)\n            \"torch>=2.0.0\",  # Required by sentence-transformers\n            \"numpy>=1.24.0\",\n        ],\n        \"dev\": [\n            \"pytest>=7.0.0\",\n            \"pytest-asyncio>=0.21.0\",\n            \"black>=23.0.0\",\n            \"flake8>=6.0.0\",\n            \"mypy>=1.0.0\",\n            \"coverage>=7.0.0\",\n        ],\n        \"security\": [\n            \"bandit>=1.7.0\",\n            \"safety>=2.3.0\",\n        ]\n    },\n    entry_points={\n        \"console_scripts\": [\n            \"maplecli=main:main\",\n            \"openaicli=main:main\"\n        ],\n    },\n    python_requires=\">=3.8\",\n    classifiers=[\n        \"Development Status :: 5 - Production/Stable\",\n        \"Intended Audience :: Developers\",\n        \"License :: OSI Approved :: MIT License\",\n        \"Programming Language :: Python :: 3\",\n        \"Programming Language :: Python :: 3.8\",", "searchable_text": "\"\"\"Setup script for MapleCLI - Enhanced OpenAI-compatible CLI with code analysis\"\"\"\nfrom setuptools import setup, find_packages\n\nsetup(\n    name=\"maplecli\",\n    version=\"2.0.0\",\n    description=\"A secure, feature-rich CLI for OpenAI-compatible APIs with advanced code analysis\",\n    author=\"MapleCLI Team\",\n    author_email=\"team@maplecli.dev\",\n    url=\"https://github.com/maplecli/maplecli\",\n    py_modules=[\"main\", \"cli\", \"chat_client\", \"code_analyzer\", \"config_manager\", \"context_engine\", \"logger\", \"query_analyzer\", \"symbol_resolver\"],\n    install_requires=[\n        \"rich>=13.0.0\",\n        \"requests>=2.31.0\",\n        \"aiofiles>=23.0.0\",\n        \"pathlib2>=2.3.0; python_version<'3.4'\",\n    ],\n    extras_require={\n        \"intelligent\": [\n            \"sentence-transformers>=2.2.0\",  # For semantic search\n            \"faiss-cpu>=1.7.4\",  # For vector search (use faiss-gpu for GPU support)\n            \"torch>=2.0.0\",  # Required by sentence-transformers\n            \"numpy>=1.24.0\",\n        ],\n        \"dev\": [\n            \"pytest>=7.0.0\",\n            \"pytest-asyncio>=0.21.0\",\n            \"black>=23.0.0\",\n            \"flake8>=6.0.0\",\n            \"mypy>=1.0.0\",\n            \"coverage>=7.0.0\",\n        ],\n        \"security\": [\n            \"bandit>=1.7.0\",\n            \"safety>=2.3.0\",\n        ]\n    },\n    entry_points={\n        \"console_scripts\": [\n            \"maplecli=main:main\",\n            \"openaicli=main:main\"\n        ],\n    },\n    python_requires=\">=3.8\",\n    classifiers=[\n        \"Development Status :: 5 - Production/Stable\",\n        \"Intended Audience :: Developers\",\n        \"License :: OSI Approved :: MIT License\",\n        \"Programming Language :: Python :: 3\",\n        \"Programming Language :: Python :: 3.8\","}, {"filepath": "setup.py", "start_line": 51, "end_line": 67, "type": "block", "name": "block_50", "content": "        \"Programming Language :: Python :: 3.9\",\n        \"Programming Language :: Python :: 3.10\",\n        \"Programming Language :: Python :: 3.11\",\n        \"Programming Language :: Python :: 3.12\",\n        \"Topic :: Software Development :: Libraries :: Python Modules\",\n        \"Topic :: Terminals\",\n        \"Topic :: Utilities\",\n        \"Security :: Developers\",\n    ],\n    keywords=\"openai cli chat code analysis ai security\",\n    project_urls={\n        \"Bug Reports\": \"https://github.com/maplecli/maplecli/issues\",\n        \"Source\": \"https://github.com/maplecli/maplecli\",\n        \"Documentation\": \"https://maplecli.readthedocs.io/\",\n    },\n)\n", "searchable_text": "        \"Programming Language :: Python :: 3.9\",\n        \"Programming Language :: Python :: 3.10\",\n        \"Programming Language :: Python :: 3.11\",\n        \"Programming Language :: Python :: 3.12\",\n        \"Topic :: Software Development :: Libraries :: Python Modules\",\n        \"Topic :: Terminals\",\n        \"Topic :: Utilities\",\n        \"Security :: Developers\",\n    ],\n    keywords=\"openai cli chat code analysis ai security\",\n    project_urls={\n        \"Bug Reports\": \"https://github.com/maplecli/maplecli/issues\",\n        \"Source\": \"https://github.com/maplecli/maplecli\",\n        \"Documentation\": \"https://maplecli.readthedocs.io/\",\n    },\n)\n"}, {"filepath": "symbol_resolver.py", "start_line": 15, "end_line": 274, "type": "class", "name": "SymbolResolver", "content": "class SymbolResolver:\n    \"\"\"\n    Track symbols, imports, and call relationships.\n    Similar to LSP (Language Server Protocol) capabilities.\n    \"\"\"\n    \n    def __init__(self, project_path: str):\n        self.project_path = project_path\n        self.symbols = {}  # symbol_name -> [locations]\n        self.imports = {}  # file -> [imported_symbols]\n        self.exports = {}  # file -> [exported_symbols]\n        self.call_graph = {}  # function -> [called_functions]\n    \n    def analyze_file(self, filepath: str, content: str):\n        \"\"\"Analyze a file and extract symbols.\"\"\"\n        ext = os.path.splitext(filepath)[1].lower()\n        \n        if ext == '.py':\n            self.analyze_python_symbols(filepath, content)\n        elif ext in ['.js', '.ts', '.jsx', '.tsx']:\n            self.analyze_javascript_symbols(filepath, content)\n        elif ext == '.java':\n            self.analyze_java_symbols(filepath, content)\n    \n    def analyze_python_symbols(self, filepath: str, content: str):\n        \"\"\"Extract all symbols from Python file.\"\"\"\n        try:\n            tree = ast.parse(content)\n            \n            for node in ast.walk(tree):\n                if isinstance(node, ast.FunctionDef):\n                    self.symbols.setdefault(node.name, []).append({\n                        'file': filepath,\n                        'line': node.lineno,\n                        'type': 'function',\n                        'name': node.name\n                    })\n                elif isinstance(node, ast.ClassDef):\n                    self.symbols.setdefault(node.name, []).append({\n                        'file': filepath,\n                        'line': node.lineno,\n                        'type': 'class',\n                        'name': node.name\n                    })\n                elif isinstance(node, ast.Import):\n                    for alias in node.names:\n                        self.imports.setdefault(filepath, []).append(alias.name)\n                elif isinstance(node, ast.ImportFrom):\n                    if node.module:\n                        self.imports.setdefault(filepath, []).append(node.module)\n        except Exception as e:\n            logger.debug(f\"Failed to parse Python file {filepath}: {e}\")\n    \n    def analyze_javascript_symbols(self, filepath: str, content: str):\n        \"\"\"Extract symbols from JavaScript/TypeScript file using AST if available.\"\"\"\n        try:\n            import esprima\n            tree = esprima.parseModule(content, {'loc': True})\n            \n            for node in tree.body:\n                if node.type == 'FunctionDeclaration' and node.id:\n                    self.symbols.setdefault(node.id.name, []).append({\n                        'file': filepath, 'line': node.loc.start.line,\n                        'type': 'function', 'name': node.id.name\n                    })\n                elif node.type == 'ClassDeclaration' and node.id:\n                    self.symbols.setdefault(node.id.name, []).append({\n                        'file': filepath, 'line': node.loc.start.line,\n                        'type': 'class', 'name': node.id.name\n                    })\n                elif node.type == 'ImportDeclaration' and node.source:\n                    self.imports.setdefault(filepath, []).append(node.source.value)\n                elif node.type == 'ExportNamedDeclaration' and node.declaration:\n                    if node.declaration.id:\n                        self.exports.setdefault(filepath, []).append(node.declaration.id.name)\n        except (ImportError, Exception) as e:\n            if isinstance(e, ImportError):\n                logger.info(\"`esprima` not found. Falling back to regex for JS/TS analysis. Install with `pip install esprima` for better accuracy.\")\n            \n            # Fallback to regex-based analysis\n            lines = content.split('\\n')\n            func_pattern = r'(?:function\\s+(\\w+)|const\\s+(\\w+)\\s*=\\s*(?:async\\s*)?\\([^)]*\\)\\s*=>)'\n            class_pattern = r'class\\s+(\\w+)'\n            import_pattern = r'import\\s+.*?from\\s+[\\'\"]([^\\'\"]+)[\\'\"]'\n            export_pattern = r'export\\s+(?:default\\s+)?(?:function|class|const)\\s+(\\w+)'\n            \n            for i, line in enumerate(lines):\n                func_match = re.search(func_pattern, line)\n                if func_match:\n                    name = func_match.group(1) or func_match.group(2)\n                    if name: self.symbols.setdefault(name, []).append({'file': filepath, 'line': i + 1, 'type': 'function', 'name': name})\n                class_match = re.search(class_pattern, line)\n                if class_match:\n                    name = class_match.group(1)\n                    self.symbols.setdefault(name, []).append({'file': filepath, 'line': i + 1, 'type': 'class', 'name': name})\n                import_match = re.search(import_pattern, line)\n                if import_match:\n                    self.imports.setdefault(filepath, []).append(import_match.group(1))\n                export_match = re.search(export_pattern, line)\n                if export_match:\n                    self.exports.setdefault(filepath, []).append(export_match.group(1))\n    \n    def analyze_java_symbols(self, filepath: str, content: str):\n        \"\"\"Extract symbols from Java file using AST if available.\"\"\"\n        try:\n            import javalang\n            tree = javalang.parse.parse(content)\n            \n            for path, node in tree:\n                if isinstance(node, javalang.tree.ClassDeclaration):\n                    self.symbols.setdefault(node.name, []).append({\n                        'file': filepath, 'line': node.position.line,\n                        'type': 'class', 'name': node.name\n                    })\n                elif isinstance(node, javalang.tree.MethodDeclaration):\n                    self.symbols.setdefault(node.name, []).append({\n                        'file': filepath, 'line': node.position.line,\n                        'type': 'method', 'name': node.name\n                    })\n                elif isinstance(node, javalang.tree.Import):\n                    self.imports.setdefault(filepath, []).append(node.path)\n        except (ImportError, Exception) as e:\n            if isinstance(e, ImportError):\n                logger.info(\"`javalang` not found. Falling back to regex for Java analysis. Install with `pip install javalang` for better accuracy.\")\n\n            # Fallback to regex-based analysis\n            lines = content.split('\\n')\n            class_pattern = r'(?:public\\s+)?class\\s+(\\w+)'\n            method_pattern = r'(?:public|private|protected)?\\s*(?:static\\s+)?[\\w<>\\[\\]]+\\s+(\\w+)\\s*\\([^)]*\\)'\n            import_pattern = r'import\\s+([\\w.]+);'\n            \n            for i, line in enumerate(lines):\n                class_match = re.search(class_pattern, line)\n                if class_match:\n                    name = class_match.group(1)\n                    self.symbols.setdefault(name, []).append({'file': filepath, 'line': i + 1, 'type': 'class', 'name': name})\n                method_match = re.search(method_pattern, line)\n                if method_match:\n                    name = method_match.group(1)\n                    if name not in ['if', 'for', 'while', 'switch']:\n                        self.symbols.setdefault(name, []).append({'file': filepath, 'line': i + 1, 'type': 'method', 'name': name})\n                import_match = re.search(import_pattern, line)\n                if import_match:\n                    self.imports.setdefault(filepath, []).append(import_match.group(1))\n    \n    def find_definition(self, symbol_name: str) -> List[Dict]:\n        \"\"\"Find where a symbol is defined.\"\"\"\n        return self.symbols.get(symbol_name, [])\n    \n    def find_usages(self, symbol_name: str, content: str) -> List[int]:\n        \"\"\"Find all usages of a symbol in content.\"\"\"\n        lines = content.split('\\n')\n        usages = []\n        \n        for i, line in enumerate(lines):\n            if re.search(r'\\b' + re.escape(symbol_name) + r'\\b', line):\n                usages.append(i + 1)\n        \n        return usages\n    \n    def get_imports_for_file(self, filepath: str) -> List[str]:\n        \"\"\"Get all imports for a file.\"\"\"\n        return self.imports.get(filepath, [])\n    \n    def get_exports_for_file(self, filepath: str) -> List[str]:\n        \"\"\"Get all exports for a file.\"\"\"\n        return self.exports.get(filepath, [])\n    \n    def get_all_symbols(self) -> Dict[str, List[Dict]]:\n        \"\"\"Get all symbols in the project.\"\"\"\n        return self.symbols\n    \n    def search_symbols(self, query: str) -> List[Dict]:\n        \"\"\"Search for symbols matching a query.\"\"\"\n        results = []\n        query_lower = query.lower()\n        \n        for symbol_name, locations in self.symbols.items():\n            if query_lower in symbol_name.lower():\n                for loc in locations:\n                    results.append({\n                        'symbol': symbol_name,\n                        'file': loc['file'],\n                        'line': loc['line'],\n                        'type': loc['type']\n                    })\n        \n        return results\n    \n    def build_call_graph(self, filepath: str, content: str):\n        \"\"\"Build call graph for a file (simplified version).\"\"\"\n        ext = os.path.splitext(filepath)[1].lower()\n        \n        if ext == '.py':\n            self._build_python_call_graph(filepath, content)\n    \n    def _build_python_call_graph(self, filepath: str, content: str):\n        \"\"\"Build call graph for Python file.\"\"\"\n        try:\n            tree = ast.parse(content)\n            \n            current_function = None\n            \n            for node in ast.walk(tree):\n                if isinstance(node, ast.FunctionDef):\n                    current_function = node.name\n                    self.call_graph.setdefault(current_function, [])\n                \n                elif isinstance(node, ast.Call) and current_function:\n                    if isinstance(node.func, ast.Name):\n                        called_func = node.func.id\n                        if called_func not in self.call_graph[current_function]:\n                            self.call_graph[current_function].append(called_func)\n        except Exception as e:\n            logger.debug(f\"Failed to build call graph for {filepath}: {e}\")\n    \n    def get_callers(self, function_name: str) -> List[str]:\n        \"\"\"Find all functions that call a given function.\"\"\"\n        callers = []\n        for caller, callees in self.call_graph.items():\n            if function_name in callees:\n                callers.append(caller)\n        return callers\n    \n    def get_callees(self, function_name: str) -> List[str]:\n        \"\"\"Find all functions called by a given function.\"\"\"\n        return self.call_graph.get(function_name, [])\n    \n    def generate_dependency_summary(self) -> str:\n        \"\"\"Generate a summary of dependencies.\"\"\"\n        summary_parts = []\n        \n        summary_parts.append(\"=== DEPENDENCY SUMMARY ===\\n\")\n        \n        # Most imported modules\n        all_imports = {}\n        for filepath, imports in self.imports.items():\n            for imp in imports:\n                all_imports[imp] = all_imports.get(imp, 0) + 1\n        \n        if all_imports:\n            summary_parts.append(\"Top Imported Modules:\")\n            sorted_imports = sorted(all_imports.items(), key=lambda x: x[1], reverse=True)[:10]\n            for module, count in sorted_imports:\n                summary_parts.append(f\"  {module}: {count} files\")\n        \n        # Symbol statistics\n        summary_parts.append(f\"\\nTotal Symbols: {len(self.symbols)}\")\n        \n        symbol_types = {}\n        for symbol_name, locations in self.symbols.items():\n            for loc in locations:\n                symbol_types[loc['type']] = symbol_types.get(loc['type'], 0) + 1\n        \n        if symbol_types:\n            summary_parts.append(\"\\nSymbol Types:\")\n            for sym_type, count in sorted(symbol_types.items()):\n                summary_parts.append(f\"  {sym_type}: {count}\")\n        \n        return \"\\n\".join(summary_parts)", "docstring": "Track symbols, imports, and call relationships.\nSimilar to LSP (Language Server Protocol) capabilities.", "searchable_text": "SymbolResolver Track symbols, imports, and call relationships.\nSimilar to LSP (Language Server Protocol) capabilities. class SymbolResolver:\n    \"\"\"\n    Track symbols, imports, and call relationships.\n    Similar to LSP (Language Server Protocol) capabilities.\n    \"\"\"\n    \n    def __init__(self, project_path: str):\n        self.project_path = project_path\n        self.symbols = {}  # symbol_name -> [locations]\n        self.imports = {}  # file -> [imported_symbols]\n        self.exports = {}  # file -> [exported_symbols]\n        self.call_graph = {}  # function -> [called_functions]\n    \n    def analyze_file(self, filepath: str, content: str):\n        \"\"\"Analyze a file and extract symbols.\"\"\"\n        ext = os.path.splitext(filepath)[1].lower()\n        \n        if ext == '.py':\n            self.analyze_python_symbols(filepath, content)\n        elif ext in ['.js', '.ts', '.jsx', '.tsx']:\n            self.analyze_javascript_symbols(filepath, content)\n        elif ext == '.java':\n            self.analyze_java_symbols(filepath, content)\n    \n    def analyze_python_symbols(self, filepath: str, content: str):\n        \"\"\"Extract all symbols from Python file.\"\"\"\n        try:\n            tree = ast.parse(content)\n            \n            for node in ast.walk(tree):\n                if isinstance(node, ast.FunctionDef):\n                    self.symbols.setdefault(node.name, []).append({\n                        'file': filepath,\n                        'line': node.lineno,\n                        'type': 'function',\n                        'name': node.name\n                    })\n                elif isinstance(node, ast.ClassDef):\n                    self.symbols.setdefault(node.name, []).append({\n                        'file': filepath,\n                        'line': node.lineno,\n                        'type': 'class',\n                        'name': node.name\n                    })\n                elif isinstance(node, ast.Import):\n                    for alias in node.names:\n                        self.imports.setdefault(filepath, []).append(alias.name)\n                elif isinstance(node, ast.ImportFrom):\n                    if node.module:\n                        self.imports.setdefault(filepath, []).append(node.module)\n        except Exception as e:\n            logger.debug(f\"Failed to parse Python file {filepath}: {e}\")\n    \n    def analyze_javascript_symbols(self, filepath: str, content: str):\n        \"\"\"Extract symbols from JavaScript/TypeScript file using AST if available.\"\"\"\n        try:\n            import esprima\n            tree = esprima.parseModule(content, {'loc': True})\n            \n            for node in tree.body:\n                if node.type == 'FunctionDeclaration' and node.id:\n                    self.symbols.setdefault(node.id.name, []).append({\n                        'file': filepath, 'line': node.loc.start.line,\n                        'type': 'function', 'name': node.id.name\n                    })\n                elif node.type == 'ClassDeclaration' and node.id:\n                    self.symbols.setdefault(node.id.name, []).append({\n                        'file': filepath, 'line': node.loc.start.line,\n                        'type': 'class', 'name': node.id.name\n                    })\n                elif node.type == 'ImportDeclaration' and node.source:\n                    self.imports.setdefault(filepath, []).append(node.source.value)\n                elif node.type == 'ExportNamedDeclaration' and node.declaration:\n                    if node.declaration.id:\n                        self.exports.setdefault(filepath, []).append(node.declaration.id.name)\n        except (ImportError, Exception) as e:\n            if isinstance(e, ImportError):\n                logger.info(\"`esprima` not found. Falling back to regex for JS/TS analysis. Install with `pip install esprima` for better accuracy.\")\n            \n            # Fallback to regex-based analysis\n            lines = content.split('\\n')\n            func_pattern = r'(?:function\\s+(\\w+)|const\\s+(\\w+)\\s*=\\s*(?:async\\s*)?\\([^)]*\\)\\s*=>)'\n            class_pattern = r'class\\s+(\\w+)'\n            import_pattern = r'import\\s+.*?from\\s+[\\'\"]([^\\'\"]+)[\\'\"]'\n            export_pattern = r'export\\s+(?:default\\s+)?(?:function|class|const)\\s+(\\w+)'\n            \n            for i, line in enumerate(lines):\n                func_match = re.search(func_pattern, line)\n                if func_match:\n                    name = func_match.group(1) or func_match.group(2)\n                    if name: self.symbols.setdefault(name, []).append({'file': filepath, 'line': i + 1, 'type': 'function', 'name': name})\n                class_match = re.search(class_pattern, line)\n                if class_match:\n                    name = class_match.group(1)\n                    self.symbols.setdefault(name, []).append({'file': filepath, 'line': i + 1, 'type': 'class', 'name': name})\n                import_match = re.search(import_pattern, line)\n                if import_match:\n                    self.imports.setdefault(filepath, []).append(import_match.group(1))\n                export_match = re.search(export_pattern, line)\n                if export_match:\n                    self.exports.setdefault(filepath, []).append(export_match.group(1))\n    \n    def analyze_java_symbols(self, filepath: str, content: str):\n        \"\"\"Extract symbols from Java file using AST if available.\"\"\"\n        try:\n            import javalang\n            tree = javalang.parse.parse(content)\n            \n            for path, node in tree:\n                if isinstance(node, javalang.tree.ClassDeclaration):\n                    self.symbols.setdefault(node.name, []).append({\n                        'file': filepath, 'line': node.position.line,\n                        'type': 'class', 'name': node.name\n                    })\n                elif isinstance(node, javalang.tree.MethodDeclaration):\n                    self.symbols.setdefault(node.name, []).append({\n                        'file': filepath, 'line': node.position.line,\n                        'type': 'method', 'name': node.name\n                    })\n                elif isinstance(node, javalang.tree.Import):\n                    self.imports.setdefault(filepath, []).append(node.path)\n        except (ImportError, Exception) as e:\n            if isinstance(e, ImportError):\n                logger.info(\"`javalang` not found. Falling back to regex for Java analysis. Install with `pip install javalang` for better accuracy.\")\n\n            # Fallback to regex-based analysis\n            lines = content.split('\\n')\n            class_pattern = r'(?:public\\s+)?class\\s+(\\w+)'\n            method_pattern = r'(?:public|private|protected)?\\s*(?:static\\s+)?[\\w<>\\[\\]]+\\s+(\\w+)\\s*\\([^)]*\\)'\n            import_pattern = r'import\\s+([\\w.]+);'\n            \n            for i, line in enumerate(lines):\n                class_match = re.search(class_pattern, line)\n                if class_match:\n                    name = class_match.group(1)\n                    self.symbols.setdefault(name, []).append({'file': filepath, 'line': i + 1, 'type': 'class', 'name': name})\n                method_match = re.search(method_pattern, line)\n                if method_match:\n                    name = method_match.group(1)\n                    if name not in ['if', 'for', 'while', 'switch']:\n                        self.symbols.setdefault(name, []).append({'file': filepath, 'line': i + 1, 'type': 'method', 'name': name})\n                import_match = re.search(import_pattern, line)\n                if import_match:\n                    self.imports.setdefault(filepath, []).append(import_match.group(1))\n    \n    def find_definition(self, symbol_name: str) -> List[Dict]:\n        \"\"\"Find where a symbol is defined.\"\"\"\n        return self.symbols.get(symbol_name, [])\n    \n    def find_usages(self, symbol_name: str, content: str) -> List[int]:\n        \"\"\"Find all usages of a symbol in content.\"\"\"\n        lines = content.split('\\n')\n        usages = []\n        \n        for i, line in enumerate(lines):\n            if re.search(r'\\b' + re.escape(symbol_name) + r'\\b', line):\n                usages.append(i + 1)\n        \n        return usages\n    \n    def get_imports_for_file(self, filepath: str) -> List[str]:\n        \"\"\"Get all imports for a file.\"\"\"\n        return self.imports.get(filepath, [])\n    \n    def get_exports_for_file(self, filepath: str) -> List[str]:\n        \"\"\"Get all exports for a file.\"\"\"\n        return self.exports.get(filepath, [])\n    \n    def get_all_symbols(self) -> Dict[str, List[Dict]]:\n        \"\"\"Get all symbols in the project.\"\"\"\n        return self.symbols\n    \n    def search_symbols(self, query: str) -> List[Dict]:\n        \"\"\"Search for symbols matching a query.\"\"\"\n        results = []\n        query_lower = query.lower()\n        \n        for symbol_name, locations in self.symbols.items():\n            if query_lower in symbol_name.lower():\n                for loc in locations:\n                    results.append({\n                        'symbol': symbol_name,\n                        'file': loc['file'],\n                        'line': loc['line'],\n                        'type': loc['type']\n                    })\n        \n        return results\n    \n    def build_call_graph(self, filepath: str, content: str):\n        \"\"\"Build call graph for a file (simplified version).\"\"\"\n        ext = os.path.splitext(filepath)[1].lower()\n        \n        if ext == '.py':\n            self._build_python_call_graph(filepath, content)\n    \n    def _build_python_call_graph(self, filepath: str, content: str):\n        \"\"\"Build call graph for Python file.\"\"\"\n        try:\n            tree = ast.parse(content)\n            \n            current_function = None\n            \n            for node in ast.walk(tree):\n                if isinstance(node, ast.FunctionDef):\n                    current_function = node.name\n                    self.call_graph.setdefault(current_function, [])\n                \n                elif isinstance(node, ast.Call) and current_function:\n                    if isinstance(node.func, ast.Name):\n                        called_func = node.func.id\n                        if called_func not in self.call_graph[current_function]:\n                            self.call_graph[current_function].append(called_func)\n        except Exception as e:\n            logger.debug(f\"Failed to build call graph for {filepath}: {e}\")\n    \n    def get_callers(self, function_name: str) -> List[str]:\n        \"\"\"Find all functions that call a given function.\"\"\"\n        callers = []\n        for caller, callees in self.call_graph.items():\n            if function_name in callees:\n                callers.append(caller)\n        return callers\n    \n    def get_callees(self, function_name: str) -> List[str]:\n        \"\"\"Find all functions called by a given function.\"\"\"\n        return self.call_graph.get(function_name, [])\n    \n    def generate_dependency_summary(self) -> str:\n        \"\"\"Generate a summary of dependencies.\"\"\"\n        summary_parts = []\n        \n        summary_parts.append(\"=== DEPENDENCY SUMMARY ===\\n\")\n        \n        # Most imported modules\n        all_imports = {}\n        for filepath, imports in self.imports.items():\n            for imp in imports:\n                all_imports[imp] = all_imports.get(imp, 0) + 1\n        \n        if all_imports:\n            summary_parts.append(\"Top Imported Modules:\")\n            sorted_imports = sorted(all_imports.items(), key=lambda x: x[1], reverse=True)[:10]\n            for module, count in sorted_imports:\n                summary_parts.append(f\"  {module}: {count} files\")\n        \n        # Symbol statistics\n        summary_parts.append(f\"\\nTotal Symbols: {len(self.symbols)}\")\n        \n        symbol_types = {}\n        for symbol_name, locations in self.symbols.items():\n            for loc in locations:\n                symbol_types[loc['type']] = symbol_types.get(loc['type'], 0) + 1\n        \n        if symbol_types:\n            summary_parts.append(\"\\nSymbol Types:\")\n            for sym_type, count in sorted(symbol_types.items()):\n                summary_parts.append(f\"  {sym_type}: {count}\")\n        \n        return \"\\n\".join(summary_parts)"}, {"filepath": "symbol_resolver.py", "start_line": 21, "end_line": 26, "type": "function", "name": "__init__", "content": "    def __init__(self, project_path: str):\n        self.project_path = project_path\n        self.symbols = {}  # symbol_name -> [locations]\n        self.imports = {}  # file -> [imported_symbols]\n        self.exports = {}  # file -> [exported_symbols]\n        self.call_graph = {}  # function -> [called_functions]", "docstring": "", "searchable_text": "__init__      def __init__(self, project_path: str):\n        self.project_path = project_path\n        self.symbols = {}  # symbol_name -> [locations]\n        self.imports = {}  # file -> [imported_symbols]\n        self.exports = {}  # file -> [exported_symbols]\n        self.call_graph = {}  # function -> [called_functions]"}, {"filepath": "symbol_resolver.py", "start_line": 28, "end_line": 37, "type": "function", "name": "analyze_file", "content": "    def analyze_file(self, filepath: str, content: str):\n        \"\"\"Analyze a file and extract symbols.\"\"\"\n        ext = os.path.splitext(filepath)[1].lower()\n        \n        if ext == '.py':\n            self.analyze_python_symbols(filepath, content)\n        elif ext in ['.js', '.ts', '.jsx', '.tsx']:\n            self.analyze_javascript_symbols(filepath, content)\n        elif ext == '.java':\n            self.analyze_java_symbols(filepath, content)", "docstring": "Analyze a file and extract symbols.", "searchable_text": "analyze_file Analyze a file and extract symbols.     def analyze_file(self, filepath: str, content: str):\n        \"\"\"Analyze a file and extract symbols.\"\"\"\n        ext = os.path.splitext(filepath)[1].lower()\n        \n        if ext == '.py':\n            self.analyze_python_symbols(filepath, content)\n        elif ext in ['.js', '.ts', '.jsx', '.tsx']:\n            self.analyze_javascript_symbols(filepath, content)\n        elif ext == '.java':\n            self.analyze_java_symbols(filepath, content)"}, {"filepath": "symbol_resolver.py", "start_line": 39, "end_line": 66, "type": "function", "name": "analyze_python_symbols", "content": "    def analyze_python_symbols(self, filepath: str, content: str):\n        \"\"\"Extract all symbols from Python file.\"\"\"\n        try:\n            tree = ast.parse(content)\n            \n            for node in ast.walk(tree):\n                if isinstance(node, ast.FunctionDef):\n                    self.symbols.setdefault(node.name, []).append({\n                        'file': filepath,\n                        'line': node.lineno,\n                        'type': 'function',\n                        'name': node.name\n                    })\n                elif isinstance(node, ast.ClassDef):\n                    self.symbols.setdefault(node.name, []).append({\n                        'file': filepath,\n                        'line': node.lineno,\n                        'type': 'class',\n                        'name': node.name\n                    })\n                elif isinstance(node, ast.Import):\n                    for alias in node.names:\n                        self.imports.setdefault(filepath, []).append(alias.name)\n                elif isinstance(node, ast.ImportFrom):\n                    if node.module:\n                        self.imports.setdefault(filepath, []).append(node.module)\n        except Exception as e:\n            logger.debug(f\"Failed to parse Python file {filepath}: {e}\")", "docstring": "Extract all symbols from Python file.", "searchable_text": "analyze_python_symbols Extract all symbols from Python file.     def analyze_python_symbols(self, filepath: str, content: str):\n        \"\"\"Extract all symbols from Python file.\"\"\"\n        try:\n            tree = ast.parse(content)\n            \n            for node in ast.walk(tree):\n                if isinstance(node, ast.FunctionDef):\n                    self.symbols.setdefault(node.name, []).append({\n                        'file': filepath,\n                        'line': node.lineno,\n                        'type': 'function',\n                        'name': node.name\n                    })\n                elif isinstance(node, ast.ClassDef):\n                    self.symbols.setdefault(node.name, []).append({\n                        'file': filepath,\n                        'line': node.lineno,\n                        'type': 'class',\n                        'name': node.name\n                    })\n                elif isinstance(node, ast.Import):\n                    for alias in node.names:\n                        self.imports.setdefault(filepath, []).append(alias.name)\n                elif isinstance(node, ast.ImportFrom):\n                    if node.module:\n                        self.imports.setdefault(filepath, []).append(node.module)\n        except Exception as e:\n            logger.debug(f\"Failed to parse Python file {filepath}: {e}\")"}, {"filepath": "symbol_resolver.py", "start_line": 68, "end_line": 115, "type": "function", "name": "analyze_javascript_symbols", "content": "    def analyze_javascript_symbols(self, filepath: str, content: str):\n        \"\"\"Extract symbols from JavaScript/TypeScript file using AST if available.\"\"\"\n        try:\n            import esprima\n            tree = esprima.parseModule(content, {'loc': True})\n            \n            for node in tree.body:\n                if node.type == 'FunctionDeclaration' and node.id:\n                    self.symbols.setdefault(node.id.name, []).append({\n                        'file': filepath, 'line': node.loc.start.line,\n                        'type': 'function', 'name': node.id.name\n                    })\n                elif node.type == 'ClassDeclaration' and node.id:\n                    self.symbols.setdefault(node.id.name, []).append({\n                        'file': filepath, 'line': node.loc.start.line,\n                        'type': 'class', 'name': node.id.name\n                    })\n                elif node.type == 'ImportDeclaration' and node.source:\n                    self.imports.setdefault(filepath, []).append(node.source.value)\n                elif node.type == 'ExportNamedDeclaration' and node.declaration:\n                    if node.declaration.id:\n                        self.exports.setdefault(filepath, []).append(node.declaration.id.name)\n        except (ImportError, Exception) as e:\n            if isinstance(e, ImportError):\n                logger.info(\"`esprima` not found. Falling back to regex for JS/TS analysis. Install with `pip install esprima` for better accuracy.\")\n            \n            # Fallback to regex-based analysis\n            lines = content.split('\\n')\n            func_pattern = r'(?:function\\s+(\\w+)|const\\s+(\\w+)\\s*=\\s*(?:async\\s*)?\\([^)]*\\)\\s*=>)'\n            class_pattern = r'class\\s+(\\w+)'\n            import_pattern = r'import\\s+.*?from\\s+[\\'\"]([^\\'\"]+)[\\'\"]'\n            export_pattern = r'export\\s+(?:default\\s+)?(?:function|class|const)\\s+(\\w+)'\n            \n            for i, line in enumerate(lines):\n                func_match = re.search(func_pattern, line)\n                if func_match:\n                    name = func_match.group(1) or func_match.group(2)\n                    if name: self.symbols.setdefault(name, []).append({'file': filepath, 'line': i + 1, 'type': 'function', 'name': name})\n                class_match = re.search(class_pattern, line)\n                if class_match:\n                    name = class_match.group(1)\n                    self.symbols.setdefault(name, []).append({'file': filepath, 'line': i + 1, 'type': 'class', 'name': name})\n                import_match = re.search(import_pattern, line)\n                if import_match:\n                    self.imports.setdefault(filepath, []).append(import_match.group(1))\n                export_match = re.search(export_pattern, line)\n                if export_match:\n                    self.exports.setdefault(filepath, []).append(export_match.group(1))", "docstring": "Extract symbols from JavaScript/TypeScript file using AST if available.", "searchable_text": "analyze_javascript_symbols Extract symbols from JavaScript/TypeScript file using AST if available.     def analyze_javascript_symbols(self, filepath: str, content: str):\n        \"\"\"Extract symbols from JavaScript/TypeScript file using AST if available.\"\"\"\n        try:\n            import esprima\n            tree = esprima.parseModule(content, {'loc': True})\n            \n            for node in tree.body:\n                if node.type == 'FunctionDeclaration' and node.id:\n                    self.symbols.setdefault(node.id.name, []).append({\n                        'file': filepath, 'line': node.loc.start.line,\n                        'type': 'function', 'name': node.id.name\n                    })\n                elif node.type == 'ClassDeclaration' and node.id:\n                    self.symbols.setdefault(node.id.name, []).append({\n                        'file': filepath, 'line': node.loc.start.line,\n                        'type': 'class', 'name': node.id.name\n                    })\n                elif node.type == 'ImportDeclaration' and node.source:\n                    self.imports.setdefault(filepath, []).append(node.source.value)\n                elif node.type == 'ExportNamedDeclaration' and node.declaration:\n                    if node.declaration.id:\n                        self.exports.setdefault(filepath, []).append(node.declaration.id.name)\n        except (ImportError, Exception) as e:\n            if isinstance(e, ImportError):\n                logger.info(\"`esprima` not found. Falling back to regex for JS/TS analysis. Install with `pip install esprima` for better accuracy.\")\n            \n            # Fallback to regex-based analysis\n            lines = content.split('\\n')\n            func_pattern = r'(?:function\\s+(\\w+)|const\\s+(\\w+)\\s*=\\s*(?:async\\s*)?\\([^)]*\\)\\s*=>)'\n            class_pattern = r'class\\s+(\\w+)'\n            import_pattern = r'import\\s+.*?from\\s+[\\'\"]([^\\'\"]+)[\\'\"]'\n            export_pattern = r'export\\s+(?:default\\s+)?(?:function|class|const)\\s+(\\w+)'\n            \n            for i, line in enumerate(lines):\n                func_match = re.search(func_pattern, line)\n                if func_match:\n                    name = func_match.group(1) or func_match.group(2)\n                    if name: self.symbols.setdefault(name, []).append({'file': filepath, 'line': i + 1, 'type': 'function', 'name': name})\n                class_match = re.search(class_pattern, line)\n                if class_match:\n                    name = class_match.group(1)\n                    self.symbols.setdefault(name, []).append({'file': filepath, 'line': i + 1, 'type': 'class', 'name': name})\n                import_match = re.search(import_pattern, line)\n                if import_match:\n                    self.imports.setdefault(filepath, []).append(import_match.group(1))\n                export_match = re.search(export_pattern, line)\n                if export_match:\n                    self.exports.setdefault(filepath, []).append(export_match.group(1))"}, {"filepath": "symbol_resolver.py", "start_line": 117, "end_line": 158, "type": "function", "name": "analyze_java_symbols", "content": "    def analyze_java_symbols(self, filepath: str, content: str):\n        \"\"\"Extract symbols from Java file using AST if available.\"\"\"\n        try:\n            import javalang\n            tree = javalang.parse.parse(content)\n            \n            for path, node in tree:\n                if isinstance(node, javalang.tree.ClassDeclaration):\n                    self.symbols.setdefault(node.name, []).append({\n                        'file': filepath, 'line': node.position.line,\n                        'type': 'class', 'name': node.name\n                    })\n                elif isinstance(node, javalang.tree.MethodDeclaration):\n                    self.symbols.setdefault(node.name, []).append({\n                        'file': filepath, 'line': node.position.line,\n                        'type': 'method', 'name': node.name\n                    })\n                elif isinstance(node, javalang.tree.Import):\n                    self.imports.setdefault(filepath, []).append(node.path)\n        except (ImportError, Exception) as e:\n            if isinstance(e, ImportError):\n                logger.info(\"`javalang` not found. Falling back to regex for Java analysis. Install with `pip install javalang` for better accuracy.\")\n\n            # Fallback to regex-based analysis\n            lines = content.split('\\n')\n            class_pattern = r'(?:public\\s+)?class\\s+(\\w+)'\n            method_pattern = r'(?:public|private|protected)?\\s*(?:static\\s+)?[\\w<>\\[\\]]+\\s+(\\w+)\\s*\\([^)]*\\)'\n            import_pattern = r'import\\s+([\\w.]+);'\n            \n            for i, line in enumerate(lines):\n                class_match = re.search(class_pattern, line)\n                if class_match:\n                    name = class_match.group(1)\n                    self.symbols.setdefault(name, []).append({'file': filepath, 'line': i + 1, 'type': 'class', 'name': name})\n                method_match = re.search(method_pattern, line)\n                if method_match:\n                    name = method_match.group(1)\n                    if name not in ['if', 'for', 'while', 'switch']:\n                        self.symbols.setdefault(name, []).append({'file': filepath, 'line': i + 1, 'type': 'method', 'name': name})\n                import_match = re.search(import_pattern, line)\n                if import_match:\n                    self.imports.setdefault(filepath, []).append(import_match.group(1))", "docstring": "Extract symbols from Java file using AST if available.", "searchable_text": "analyze_java_symbols Extract symbols from Java file using AST if available.     def analyze_java_symbols(self, filepath: str, content: str):\n        \"\"\"Extract symbols from Java file using AST if available.\"\"\"\n        try:\n            import javalang\n            tree = javalang.parse.parse(content)\n            \n            for path, node in tree:\n                if isinstance(node, javalang.tree.ClassDeclaration):\n                    self.symbols.setdefault(node.name, []).append({\n                        'file': filepath, 'line': node.position.line,\n                        'type': 'class', 'name': node.name\n                    })\n                elif isinstance(node, javalang.tree.MethodDeclaration):\n                    self.symbols.setdefault(node.name, []).append({\n                        'file': filepath, 'line': node.position.line,\n                        'type': 'method', 'name': node.name\n                    })\n                elif isinstance(node, javalang.tree.Import):\n                    self.imports.setdefault(filepath, []).append(node.path)\n        except (ImportError, Exception) as e:\n            if isinstance(e, ImportError):\n                logger.info(\"`javalang` not found. Falling back to regex for Java analysis. Install with `pip install javalang` for better accuracy.\")\n\n            # Fallback to regex-based analysis\n            lines = content.split('\\n')\n            class_pattern = r'(?:public\\s+)?class\\s+(\\w+)'\n            method_pattern = r'(?:public|private|protected)?\\s*(?:static\\s+)?[\\w<>\\[\\]]+\\s+(\\w+)\\s*\\([^)]*\\)'\n            import_pattern = r'import\\s+([\\w.]+);'\n            \n            for i, line in enumerate(lines):\n                class_match = re.search(class_pattern, line)\n                if class_match:\n                    name = class_match.group(1)\n                    self.symbols.setdefault(name, []).append({'file': filepath, 'line': i + 1, 'type': 'class', 'name': name})\n                method_match = re.search(method_pattern, line)\n                if method_match:\n                    name = method_match.group(1)\n                    if name not in ['if', 'for', 'while', 'switch']:\n                        self.symbols.setdefault(name, []).append({'file': filepath, 'line': i + 1, 'type': 'method', 'name': name})\n                import_match = re.search(import_pattern, line)\n                if import_match:\n                    self.imports.setdefault(filepath, []).append(import_match.group(1))"}, {"filepath": "symbol_resolver.py", "start_line": 160, "end_line": 162, "type": "function", "name": "find_definition", "content": "    def find_definition(self, symbol_name: str) -> List[Dict]:\n        \"\"\"Find where a symbol is defined.\"\"\"\n        return self.symbols.get(symbol_name, [])", "docstring": "Find where a symbol is defined.", "searchable_text": "find_definition Find where a symbol is defined.     def find_definition(self, symbol_name: str) -> List[Dict]:\n        \"\"\"Find where a symbol is defined.\"\"\"\n        return self.symbols.get(symbol_name, [])"}, {"filepath": "symbol_resolver.py", "start_line": 164, "end_line": 173, "type": "function", "name": "find_usages", "content": "    def find_usages(self, symbol_name: str, content: str) -> List[int]:\n        \"\"\"Find all usages of a symbol in content.\"\"\"\n        lines = content.split('\\n')\n        usages = []\n        \n        for i, line in enumerate(lines):\n            if re.search(r'\\b' + re.escape(symbol_name) + r'\\b', line):\n                usages.append(i + 1)\n        \n        return usages", "docstring": "Find all usages of a symbol in content.", "searchable_text": "find_usages Find all usages of a symbol in content.     def find_usages(self, symbol_name: str, content: str) -> List[int]:\n        \"\"\"Find all usages of a symbol in content.\"\"\"\n        lines = content.split('\\n')\n        usages = []\n        \n        for i, line in enumerate(lines):\n            if re.search(r'\\b' + re.escape(symbol_name) + r'\\b', line):\n                usages.append(i + 1)\n        \n        return usages"}, {"filepath": "symbol_resolver.py", "start_line": 175, "end_line": 177, "type": "function", "name": "get_imports_for_file", "content": "    def get_imports_for_file(self, filepath: str) -> List[str]:\n        \"\"\"Get all imports for a file.\"\"\"\n        return self.imports.get(filepath, [])", "docstring": "Get all imports for a file.", "searchable_text": "get_imports_for_file Get all imports for a file.     def get_imports_for_file(self, filepath: str) -> List[str]:\n        \"\"\"Get all imports for a file.\"\"\"\n        return self.imports.get(filepath, [])"}, {"filepath": "symbol_resolver.py", "start_line": 179, "end_line": 181, "type": "function", "name": "get_exports_for_file", "content": "    def get_exports_for_file(self, filepath: str) -> List[str]:\n        \"\"\"Get all exports for a file.\"\"\"\n        return self.exports.get(filepath, [])", "docstring": "Get all exports for a file.", "searchable_text": "get_exports_for_file Get all exports for a file.     def get_exports_for_file(self, filepath: str) -> List[str]:\n        \"\"\"Get all exports for a file.\"\"\"\n        return self.exports.get(filepath, [])"}, {"filepath": "symbol_resolver.py", "start_line": 183, "end_line": 185, "type": "function", "name": "get_all_symbols", "content": "    def get_all_symbols(self) -> Dict[str, List[Dict]]:\n        \"\"\"Get all symbols in the project.\"\"\"\n        return self.symbols", "docstring": "Get all symbols in the project.", "searchable_text": "get_all_symbols Get all symbols in the project.     def get_all_symbols(self) -> Dict[str, List[Dict]]:\n        \"\"\"Get all symbols in the project.\"\"\"\n        return self.symbols"}, {"filepath": "symbol_resolver.py", "start_line": 187, "end_line": 202, "type": "function", "name": "search_symbols", "content": "    def search_symbols(self, query: str) -> List[Dict]:\n        \"\"\"Search for symbols matching a query.\"\"\"\n        results = []\n        query_lower = query.lower()\n        \n        for symbol_name, locations in self.symbols.items():\n            if query_lower in symbol_name.lower():\n                for loc in locations:\n                    results.append({\n                        'symbol': symbol_name,\n                        'file': loc['file'],\n                        'line': loc['line'],\n                        'type': loc['type']\n                    })\n        \n        return results", "docstring": "Search for symbols matching a query.", "searchable_text": "search_symbols Search for symbols matching a query.     def search_symbols(self, query: str) -> List[Dict]:\n        \"\"\"Search for symbols matching a query.\"\"\"\n        results = []\n        query_lower = query.lower()\n        \n        for symbol_name, locations in self.symbols.items():\n            if query_lower in symbol_name.lower():\n                for loc in locations:\n                    results.append({\n                        'symbol': symbol_name,\n                        'file': loc['file'],\n                        'line': loc['line'],\n                        'type': loc['type']\n                    })\n        \n        return results"}, {"filepath": "symbol_resolver.py", "start_line": 204, "end_line": 209, "type": "function", "name": "build_call_graph", "content": "    def build_call_graph(self, filepath: str, content: str):\n        \"\"\"Build call graph for a file (simplified version).\"\"\"\n        ext = os.path.splitext(filepath)[1].lower()\n        \n        if ext == '.py':\n            self._build_python_call_graph(filepath, content)", "docstring": "Build call graph for a file (simplified version).", "searchable_text": "build_call_graph Build call graph for a file (simplified version).     def build_call_graph(self, filepath: str, content: str):\n        \"\"\"Build call graph for a file (simplified version).\"\"\"\n        ext = os.path.splitext(filepath)[1].lower()\n        \n        if ext == '.py':\n            self._build_python_call_graph(filepath, content)"}, {"filepath": "symbol_resolver.py", "start_line": 211, "end_line": 229, "type": "function", "name": "_build_python_call_graph", "content": "    def _build_python_call_graph(self, filepath: str, content: str):\n        \"\"\"Build call graph for Python file.\"\"\"\n        try:\n            tree = ast.parse(content)\n            \n            current_function = None\n            \n            for node in ast.walk(tree):\n                if isinstance(node, ast.FunctionDef):\n                    current_function = node.name\n                    self.call_graph.setdefault(current_function, [])\n                \n                elif isinstance(node, ast.Call) and current_function:\n                    if isinstance(node.func, ast.Name):\n                        called_func = node.func.id\n                        if called_func not in self.call_graph[current_function]:\n                            self.call_graph[current_function].append(called_func)\n        except Exception as e:\n            logger.debug(f\"Failed to build call graph for {filepath}: {e}\")", "docstring": "Build call graph for Python file.", "searchable_text": "_build_python_call_graph Build call graph for Python file.     def _build_python_call_graph(self, filepath: str, content: str):\n        \"\"\"Build call graph for Python file.\"\"\"\n        try:\n            tree = ast.parse(content)\n            \n            current_function = None\n            \n            for node in ast.walk(tree):\n                if isinstance(node, ast.FunctionDef):\n                    current_function = node.name\n                    self.call_graph.setdefault(current_function, [])\n                \n                elif isinstance(node, ast.Call) and current_function:\n                    if isinstance(node.func, ast.Name):\n                        called_func = node.func.id\n                        if called_func not in self.call_graph[current_function]:\n                            self.call_graph[current_function].append(called_func)\n        except Exception as e:\n            logger.debug(f\"Failed to build call graph for {filepath}: {e}\")"}, {"filepath": "symbol_resolver.py", "start_line": 231, "end_line": 237, "type": "function", "name": "get_callers", "content": "    def get_callers(self, function_name: str) -> List[str]:\n        \"\"\"Find all functions that call a given function.\"\"\"\n        callers = []\n        for caller, callees in self.call_graph.items():\n            if function_name in callees:\n                callers.append(caller)\n        return callers", "docstring": "Find all functions that call a given function.", "searchable_text": "get_callers Find all functions that call a given function.     def get_callers(self, function_name: str) -> List[str]:\n        \"\"\"Find all functions that call a given function.\"\"\"\n        callers = []\n        for caller, callees in self.call_graph.items():\n            if function_name in callees:\n                callers.append(caller)\n        return callers"}, {"filepath": "symbol_resolver.py", "start_line": 239, "end_line": 241, "type": "function", "name": "get_callees", "content": "    def get_callees(self, function_name: str) -> List[str]:\n        \"\"\"Find all functions called by a given function.\"\"\"\n        return self.call_graph.get(function_name, [])", "docstring": "Find all functions called by a given function.", "searchable_text": "get_callees Find all functions called by a given function.     def get_callees(self, function_name: str) -> List[str]:\n        \"\"\"Find all functions called by a given function.\"\"\"\n        return self.call_graph.get(function_name, [])"}, {"filepath": "symbol_resolver.py", "start_line": 243, "end_line": 274, "type": "function", "name": "generate_dependency_summary", "content": "    def generate_dependency_summary(self) -> str:\n        \"\"\"Generate a summary of dependencies.\"\"\"\n        summary_parts = []\n        \n        summary_parts.append(\"=== DEPENDENCY SUMMARY ===\\n\")\n        \n        # Most imported modules\n        all_imports = {}\n        for filepath, imports in self.imports.items():\n            for imp in imports:\n                all_imports[imp] = all_imports.get(imp, 0) + 1\n        \n        if all_imports:\n            summary_parts.append(\"Top Imported Modules:\")\n            sorted_imports = sorted(all_imports.items(), key=lambda x: x[1], reverse=True)[:10]\n            for module, count in sorted_imports:\n                summary_parts.append(f\"  {module}: {count} files\")\n        \n        # Symbol statistics\n        summary_parts.append(f\"\\nTotal Symbols: {len(self.symbols)}\")\n        \n        symbol_types = {}\n        for symbol_name, locations in self.symbols.items():\n            for loc in locations:\n                symbol_types[loc['type']] = symbol_types.get(loc['type'], 0) + 1\n        \n        if symbol_types:\n            summary_parts.append(\"\\nSymbol Types:\")\n            for sym_type, count in sorted(symbol_types.items()):\n                summary_parts.append(f\"  {sym_type}: {count}\")\n        \n        return \"\\n\".join(summary_parts)", "docstring": "Generate a summary of dependencies.", "searchable_text": "generate_dependency_summary Generate a summary of dependencies.     def generate_dependency_summary(self) -> str:\n        \"\"\"Generate a summary of dependencies.\"\"\"\n        summary_parts = []\n        \n        summary_parts.append(\"=== DEPENDENCY SUMMARY ===\\n\")\n        \n        # Most imported modules\n        all_imports = {}\n        for filepath, imports in self.imports.items():\n            for imp in imports:\n                all_imports[imp] = all_imports.get(imp, 0) + 1\n        \n        if all_imports:\n            summary_parts.append(\"Top Imported Modules:\")\n            sorted_imports = sorted(all_imports.items(), key=lambda x: x[1], reverse=True)[:10]\n            for module, count in sorted_imports:\n                summary_parts.append(f\"  {module}: {count} files\")\n        \n        # Symbol statistics\n        summary_parts.append(f\"\\nTotal Symbols: {len(self.symbols)}\")\n        \n        symbol_types = {}\n        for symbol_name, locations in self.symbols.items():\n            for loc in locations:\n                symbol_types[loc['type']] = symbol_types.get(loc['type'], 0) + 1\n        \n        if symbol_types:\n            summary_parts.append(\"\\nSymbol Types:\")\n            for sym_type, count in sorted(symbol_types.items()):\n                summary_parts.append(f\"  {sym_type}: {count}\")\n        \n        return \"\\n\".join(summary_parts)"}, {"filepath": "__init__.py", "start_line": 1, "end_line": 1, "type": "block", "name": "block_0", "content": "# This file makes the src directory a Python package.", "searchable_text": "# This file makes the src directory a Python package."}, {"filepath": "maplecli.egg-info\\dependency_links.txt", "start_line": 1, "end_line": 2, "type": "block", "name": "block_0", "content": "\n", "searchable_text": "\n"}, {"filepath": "maplecli.egg-info\\entry_points.txt", "start_line": 1, "end_line": 4, "type": "block", "name": "block_0", "content": "[console_scripts]\nmaplecli = main:main\nopenaicli = main:main\n", "searchable_text": "[console_scripts]\nmaplecli = main:main\nopenaicli = main:main\n"}, {"filepath": "maplecli.egg-info\\requires.txt", "start_line": 1, "end_line": 25, "type": "block", "name": "block_0", "content": "rich>=13.0.0\nrequests>=2.31.0\naiofiles>=23.0.0\n\n[:python_version < \"3.4\"]\npathlib2>=2.3.0\n\n[dev]\npytest>=7.0.0\npytest-asyncio>=0.21.0\nblack>=23.0.0\nflake8>=6.0.0\nmypy>=1.0.0\ncoverage>=7.0.0\n\n[intelligent]\nsentence-transformers>=2.2.0\nfaiss-cpu>=1.7.4\ntorch>=2.0.0\nnumpy>=1.24.0\n\n[security]\nbandit>=1.7.0\nsafety>=2.3.0\n", "searchable_text": "rich>=13.0.0\nrequests>=2.31.0\naiofiles>=23.0.0\n\n[:python_version < \"3.4\"]\npathlib2>=2.3.0\n\n[dev]\npytest>=7.0.0\npytest-asyncio>=0.21.0\nblack>=23.0.0\nflake8>=6.0.0\nmypy>=1.0.0\ncoverage>=7.0.0\n\n[intelligent]\nsentence-transformers>=2.2.0\nfaiss-cpu>=1.7.4\ntorch>=2.0.0\nnumpy>=1.24.0\n\n[security]\nbandit>=1.7.0\nsafety>=2.3.0\n"}, {"filepath": "maplecli.egg-info\\SOURCES.txt", "start_line": 1, "end_line": 20, "type": "block", "name": "block_0", "content": "README.md\nchat_client.py\ncli.py\ncode_analyzer.py\nconfig_manager.py\ncontext_engine.py\nlogger.py\nmain.py\nquery_analyzer.py\nsetup.py\nsymbol_resolver.py\nmaplecli.egg-info/PKG-INFO\nmaplecli.egg-info/SOURCES.txt\nmaplecli.egg-info/dependency_links.txt\nmaplecli.egg-info/entry_points.txt\nmaplecli.egg-info/requires.txt\nmaplecli.egg-info/top_level.txt\ntests/test_analyzer.py\ntests/test_intelligent_features.py\ntests/test_security.py", "searchable_text": "README.md\nchat_client.py\ncli.py\ncode_analyzer.py\nconfig_manager.py\ncontext_engine.py\nlogger.py\nmain.py\nquery_analyzer.py\nsetup.py\nsymbol_resolver.py\nmaplecli.egg-info/PKG-INFO\nmaplecli.egg-info/SOURCES.txt\nmaplecli.egg-info/dependency_links.txt\nmaplecli.egg-info/entry_points.txt\nmaplecli.egg-info/requires.txt\nmaplecli.egg-info/top_level.txt\ntests/test_analyzer.py\ntests/test_intelligent_features.py\ntests/test_security.py"}, {"filepath": "maplecli.egg-info\\top_level.txt", "start_line": 1, "end_line": 10, "type": "block", "name": "block_0", "content": "chat_client\ncli\ncode_analyzer\nconfig_manager\ncontext_engine\nlogger\nmain\nquery_analyzer\nsymbol_resolver\n", "searchable_text": "chat_client\ncli\ncode_analyzer\nconfig_manager\ncontext_engine\nlogger\nmain\nquery_analyzer\nsymbol_resolver\n"}, {"filepath": "tests\\test_analyzer.py", "start_line": 16, "end_line": 350, "type": "class", "name": "TestCodeAnalyzer", "content": "class TestCodeAnalyzer:\n    \"\"\"Test code analysis functionality\"\"\"\n    \n    def setup_method(self):\n        \"\"\"Set up test fixtures\"\"\"\n        self.console_mock = Mock()\n        self.temp_dir = tempfile.mkdtemp()\n        self.analyzer = CodeAnalyzer(self.temp_dir, self.console_mock)\n    \n    def teardown_method(self):\n        \"\"\"Clean up test fixtures\"\"\"\n        import shutil\n        shutil.rmtree(self.temp_dir, ignore_errors=True)\n    \n    def create_test_file(self, filename: str, content: str):\n        \"\"\"Helper to create test files\"\"\"\n        filepath = os.path.join(self.temp_dir, filename)\n        os.makedirs(os.path.dirname(filepath), exist_ok=True)\n        with open(filepath, 'w') as f:\n            f.write(content)\n        return filepath\n    \n    def test_project_type_detection(self):\n        \"\"\"Test project type detection\"\"\"\n        # Python project\n        self.create_test_file('requirements.txt', 'requests==2.25.0')\n        self.create_test_file('setup.py', 'from setuptools import setup')\n        self.analyzer._detect_project_type()\n        assert self.analyzer.project_type == 'python'\n        \n        # Node.js project\n        self.create_test_file('package.json', '{\"name\": \"test\"}')\n        self.analyzer._detect_project_type()\n        assert self.analyzer.project_type == 'node'\n        \n        # Rust project\n        self.create_test_file('Cargo.toml', '[package]\\nname = \"test\"')\n        self.analyzer._detect_project_type()\n        assert self.analyzer.project_type == 'rust'\n    \n    def test_file_filtering(self):\n        \"\"\"Test that files are properly filtered\"\"\"\n        # Should be ignored\n        assert self.analyzer._should_ignore_file('.git/config')\n        assert self.analyzer._should_ignore_file('node_modules/package.json')\n        assert self.analyzer._should_ignore_file('test.pyc')\n        assert self.analyzer._should_ignore_file('test.log')\n        \n        # Should not be ignored\n        assert not self.analyzer._should_ignore_file('src/main.py')\n        assert not self.analyzer._should_ignore_file('.env.example')\n        assert not self.analyzer._should_ignore_file('.gitignore')\n    \n    @pytest.mark.asyncio\n    async def test_async_project_scan(self):\n        \"\"\"Test async project scanning\"\"\"\n        # Create test files\n        self.create_test_file('src/main.py', 'print(\"hello\")')\n        self.create_test_file('src/utils.py', 'def helper(): pass')\n        self.create_test_file('README.md', '# Test Project')\n        self.create_test_file('package.json', '{\"name\": \"test\"}')\n        \n        result = await self.analyzer.scan_project_async()\n        \n        assert result['total_files'] >= 2  # At least the Python files\n        assert result['project_type'] == 'node'  # Should detect Node.js\n        assert 'processed_size_mb' in result\n        assert 'skipped_files' in result\n    \n    def test_python_dependency_analysis(self):\n        \"\"\"Test Python dependency extraction\"\"\"\n        python_code = \"\"\"\nimport os\nimport sys\nfrom datetime import datetime\nfrom collections import defaultdict\nimport requests\nimport numpy as np\n\"\"\"\n        self.create_test_file('test.py', python_code)\n        \n        deps = self.analyzer.analyze_dependencies('test.py')\n        \n        expected_imports = ['os', 'sys', 'datetime', 'collections', 'requests', 'numpy']\n        for imp in expected_imports:\n            assert imp in deps['imports']\n    \n    def test_javascript_dependency_analysis(self):\n        \"\"\"Test JavaScript dependency extraction\"\"\"\n        js_code = \"\"\"\nimport React from 'react';\nimport { useState } from 'react';\nimport axios from 'axios';\nconst express = require('express');\nimport lodash from 'lodash';\n\"\"\"\n        self.create_test_file('test.js', js_code)\n        \n        deps = self.analyzer.analyze_dependencies('test.js')\n        \n        expected_imports = ['react', 'axios', 'express', 'lodash']\n        for imp in expected_imports:\n            assert imp in deps['imports']\n    \n    def test_java_dependency_analysis(self):\n        \"\"\"Test Java dependency extraction\"\"\"\n        java_code = \"\"\"\nimport java.util.List;\nimport java.util.ArrayList;\nimport com.example.MyClass;\nimport org.springframework.boot.SpringApplication;\n\"\"\"\n        self.create_test_file('Test.java', java_code)\n        \n        deps = self.analyzer.analyze_dependencies('Test.java')\n        \n        expected_imports = ['java', 'com', 'org']\n        for imp in expected_imports:\n            assert imp in deps['imports']\n    \n    def test_architecture_pattern_detection(self):\n        \"\"\"Test architecture pattern detection\"\"\"\n        files = [\n            'src/controllers/UserController.java',\n            'src/models/User.java',\n            'src/views/UserView.jsp',\n            'services/auth-service.js',\n            'api/gateway.js',\n            'functions/lambda-handler.js',\n            'modules/user/module.py',\n            'components/Button.jsx',\n            'repositories/UserRepository.java',\n            'dao/UserDAO.java'\n        ]\n        \n        patterns = self.analyzer.detect_architecture_patterns(files)\n        \n        assert patterns['mvc'] >= 3  # controller, model, view\n        assert patterns['microservices'] >= 2  # service, gateway\n        assert patterns['serverless'] >= 1  # lambda\n        assert patterns['modular'] >= 2  # modules, components\n        assert patterns['repository'] >= 2  # repository, dao\n    \n    def test_python_complexity_analysis(self):\n        \"\"\"Test Python complexity analysis\"\"\"\n        python_code = \"\"\"\ndef simple_function():\n    return \"hello\"\n\ndef complex_function(x):\n    if x > 0:\n        for i in range(x):\n            if i % 2 == 0:\n                try:\n                    result = i * 2\n                except Exception:\n                    result = 0\n            else:\n                result = i / 2\n    elif x < 0:\n        while x < 0:\n            x += 1\n    return result\n\"\"\"\n        self.create_test_file('test.py', python_code)\n        \n        complexity = self.analyzer.analyze_complexity('test.py')\n        \n        assert complexity['functions'] == 2\n        assert complexity['lines_of_code'] > 0\n        assert complexity['cyclomatic'] > 1  # Should have decision points\n        assert complexity['cognitive'] >= 1  # Should have nesting\n    \n    def test_javascript_complexity_analysis(self):\n        \"\"\"Test JavaScript complexity analysis\"\"\"\n        js_code = \"\"\"\nfunction simpleFunction() {\n    return \"hello\";\n}\n\nconst complexFunction = (x) => {\n    if (x > 0) {\n        for (let i = 0; i < x; i++) {\n            if (i % 2 === 0) {\n                try {\n                    const result = i * 2;\n                } catch (error) {\n                    const result = 0;\n                }\n            } else {\n                const result = i / 2;\n            }\n        }\n    } else if (x < 0) {\n        while (x < 0) {\n            x++;\n        }\n    }\n    return result;\n};\n\"\"\"\n        self.create_test_file('test.js', js_code)\n        \n        complexity = self.analyzer.analyze_complexity('test.js')\n        \n        assert complexity['functions'] == 2\n        assert complexity['lines_of_code'] > 0\n        assert complexity['cyclomatic'] > 1\n    \n    def test_java_complexity_analysis(self):\n        \"\"\"Test Java complexity analysis\"\"\"\n        java_code = \"\"\"\npublic class TestClass {\n    public void simpleMethod() {\n        System.out.println(\"hello\");\n    }\n    \n    public void complexMethod(int x) {\n        if (x > 0) {\n            for (int i = 0; i < x; i++) {\n                if (i % 2 == 0) {\n                    try {\n                        int result = i * 2;\n                    } catch (Exception e) {\n                        int result = 0;\n                    }\n                } else {\n                    int result = i / 2;\n                }\n            }\n        } else if (x < 0) {\n            while (x < 0) {\n                x++;\n            }\n        }\n    }\n}\n\"\"\"\n        self.create_test_file('TestClass.java', java_code)\n        \n        complexity = self.analyzer.analyze_complexity('TestClass.java')\n        \n        assert complexity['functions'] >= 2\n        assert complexity['lines_of_code'] > 0\n        assert complexity['cyclomatic'] > 1\n    \n    def test_go_dependency_analysis(self):\n        \"\"\"Test Go dependency extraction\"\"\"\n        go_code = \"\"\"\npackage main\n\nimport (\n    \"fmt\"\n    \"net/http\"\n    \"github.com/gin-gonic/gin\"\n    \"database/sql\"\n)\n\"\"\"\n        self.create_test_file('main.go', go_code)\n        \n        deps = self.analyzer.analyze_dependencies('main.go')\n        \n        expected_imports = ['fmt', 'net/http', 'github.com/gin-gonic/gin', 'database/sql']\n        for imp in expected_imports:\n            assert imp in deps['imports']\n    \n    def test_rust_dependency_analysis(self):\n        \"\"\"Test Rust dependency extraction\"\"\"\n        rust_code = \"\"\"\nuse std::collections::HashMap;\nuse std::io::Result;\nuse serde::{Deserialize, Serialize};\nextern crate tokio;\nuse crate::utils::helper;\n\"\"\"\n        self.create_test_file('main.rs', rust_code)\n        \n        deps = self.analyzer.analyze_dependencies('main.rs')\n        \n        expected_imports = ['std', 'serde', 'tokio', 'crate']\n        for imp in expected_imports:\n            assert imp in deps['imports']\n    \n    def test_project_tree_generation(self):\n        \"\"\"Test project tree visualization\"\"\"\n        # Create a test directory structure\n        self.create_test_file('src/main.py', 'print(\"hello\")')\n        self.create_test_file('src/utils/helper.py', 'def help(): pass')\n        self.create_test_file('tests/test_main.py', 'def test(): pass')\n        self.create_test_file('README.md', '# Test')\n        \n        tree = self.analyzer.get_project_tree(max_depth=2)\n        \n        # The tree should contain the project name\n        assert os.path.basename(self.temp_dir) in str(tree)\n    \n    def test_summary_generation(self):\n        \"\"\"Test project summary generation\"\"\"\n        # Create test files\n        self.create_test_file('src/main.py', 'print(\"hello\")')\n        self.create_test_file('src/utils.py', 'def help(): pass')\n        self.create_test_file('README.md', '# Test Project')\n        \n        # Run scan first to populate data\n        asyncio.run(self.analyzer.scan_project_async())\n        \n        summary = self.analyzer.generate_summary()\n        \n        assert \"WORKSPACE CONTEXT\" in summary\n        assert \"PROJECT STATISTICS\" in summary\n        assert str(self.analyzer.total_files) in summary\n        assert str(self.analyzer.total_lines) in summary\n    \n    def test_error_handling_in_analysis(self):\n        \"\"\"Test error handling during analysis\"\"\"\n        # Create a file with invalid encoding\n        invalid_file = os.path.join(self.temp_dir, 'invalid.py')\n        with open(invalid_file, 'wb') as f:\n            f.write(b'\\xff\\xfe invalid content')\n        \n        # Should handle gracefully\n        result = asyncio.run(self.analyzer.scan_project_async())\n        assert 'skipped_files' in result\n    \n    def test_memory_management(self):\n        \"\"\"Test memory management during large scans\"\"\"\n        # Create many small files\n        for i in range(100):\n            self.create_test_file(f'src/file_{i}.py', f'def func_{i}(): return {i}')\n        \n        result = asyncio.run(self.analyzer.scan_project_async())\n        \n        # Should process files but respect limits\n        assert result['total_files'] == 100\n        assert result['processed_size_mb'] < 100  # Should be reasonable", "docstring": "Test code analysis functionality", "searchable_text": "TestCodeAnalyzer Test code analysis functionality class TestCodeAnalyzer:\n    \"\"\"Test code analysis functionality\"\"\"\n    \n    def setup_method(self):\n        \"\"\"Set up test fixtures\"\"\"\n        self.console_mock = Mock()\n        self.temp_dir = tempfile.mkdtemp()\n        self.analyzer = CodeAnalyzer(self.temp_dir, self.console_mock)\n    \n    def teardown_method(self):\n        \"\"\"Clean up test fixtures\"\"\"\n        import shutil\n        shutil.rmtree(self.temp_dir, ignore_errors=True)\n    \n    def create_test_file(self, filename: str, content: str):\n        \"\"\"Helper to create test files\"\"\"\n        filepath = os.path.join(self.temp_dir, filename)\n        os.makedirs(os.path.dirname(filepath), exist_ok=True)\n        with open(filepath, 'w') as f:\n            f.write(content)\n        return filepath\n    \n    def test_project_type_detection(self):\n        \"\"\"Test project type detection\"\"\"\n        # Python project\n        self.create_test_file('requirements.txt', 'requests==2.25.0')\n        self.create_test_file('setup.py', 'from setuptools import setup')\n        self.analyzer._detect_project_type()\n        assert self.analyzer.project_type == 'python'\n        \n        # Node.js project\n        self.create_test_file('package.json', '{\"name\": \"test\"}')\n        self.analyzer._detect_project_type()\n        assert self.analyzer.project_type == 'node'\n        \n        # Rust project\n        self.create_test_file('Cargo.toml', '[package]\\nname = \"test\"')\n        self.analyzer._detect_project_type()\n        assert self.analyzer.project_type == 'rust'\n    \n    def test_file_filtering(self):\n        \"\"\"Test that files are properly filtered\"\"\"\n        # Should be ignored\n        assert self.analyzer._should_ignore_file('.git/config')\n        assert self.analyzer._should_ignore_file('node_modules/package.json')\n        assert self.analyzer._should_ignore_file('test.pyc')\n        assert self.analyzer._should_ignore_file('test.log')\n        \n        # Should not be ignored\n        assert not self.analyzer._should_ignore_file('src/main.py')\n        assert not self.analyzer._should_ignore_file('.env.example')\n        assert not self.analyzer._should_ignore_file('.gitignore')\n    \n    @pytest.mark.asyncio\n    async def test_async_project_scan(self):\n        \"\"\"Test async project scanning\"\"\"\n        # Create test files\n        self.create_test_file('src/main.py', 'print(\"hello\")')\n        self.create_test_file('src/utils.py', 'def helper(): pass')\n        self.create_test_file('README.md', '# Test Project')\n        self.create_test_file('package.json', '{\"name\": \"test\"}')\n        \n        result = await self.analyzer.scan_project_async()\n        \n        assert result['total_files'] >= 2  # At least the Python files\n        assert result['project_type'] == 'node'  # Should detect Node.js\n        assert 'processed_size_mb' in result\n        assert 'skipped_files' in result\n    \n    def test_python_dependency_analysis(self):\n        \"\"\"Test Python dependency extraction\"\"\"\n        python_code = \"\"\"\nimport os\nimport sys\nfrom datetime import datetime\nfrom collections import defaultdict\nimport requests\nimport numpy as np\n\"\"\"\n        self.create_test_file('test.py', python_code)\n        \n        deps = self.analyzer.analyze_dependencies('test.py')\n        \n        expected_imports = ['os', 'sys', 'datetime', 'collections', 'requests', 'numpy']\n        for imp in expected_imports:\n            assert imp in deps['imports']\n    \n    def test_javascript_dependency_analysis(self):\n        \"\"\"Test JavaScript dependency extraction\"\"\"\n        js_code = \"\"\"\nimport React from 'react';\nimport { useState } from 'react';\nimport axios from 'axios';\nconst express = require('express');\nimport lodash from 'lodash';\n\"\"\"\n        self.create_test_file('test.js', js_code)\n        \n        deps = self.analyzer.analyze_dependencies('test.js')\n        \n        expected_imports = ['react', 'axios', 'express', 'lodash']\n        for imp in expected_imports:\n            assert imp in deps['imports']\n    \n    def test_java_dependency_analysis(self):\n        \"\"\"Test Java dependency extraction\"\"\"\n        java_code = \"\"\"\nimport java.util.List;\nimport java.util.ArrayList;\nimport com.example.MyClass;\nimport org.springframework.boot.SpringApplication;\n\"\"\"\n        self.create_test_file('Test.java', java_code)\n        \n        deps = self.analyzer.analyze_dependencies('Test.java')\n        \n        expected_imports = ['java', 'com', 'org']\n        for imp in expected_imports:\n            assert imp in deps['imports']\n    \n    def test_architecture_pattern_detection(self):\n        \"\"\"Test architecture pattern detection\"\"\"\n        files = [\n            'src/controllers/UserController.java',\n            'src/models/User.java',\n            'src/views/UserView.jsp',\n            'services/auth-service.js',\n            'api/gateway.js',\n            'functions/lambda-handler.js',\n            'modules/user/module.py',\n            'components/Button.jsx',\n            'repositories/UserRepository.java',\n            'dao/UserDAO.java'\n        ]\n        \n        patterns = self.analyzer.detect_architecture_patterns(files)\n        \n        assert patterns['mvc'] >= 3  # controller, model, view\n        assert patterns['microservices'] >= 2  # service, gateway\n        assert patterns['serverless'] >= 1  # lambda\n        assert patterns['modular'] >= 2  # modules, components\n        assert patterns['repository'] >= 2  # repository, dao\n    \n    def test_python_complexity_analysis(self):\n        \"\"\"Test Python complexity analysis\"\"\"\n        python_code = \"\"\"\ndef simple_function():\n    return \"hello\"\n\ndef complex_function(x):\n    if x > 0:\n        for i in range(x):\n            if i % 2 == 0:\n                try:\n                    result = i * 2\n                except Exception:\n                    result = 0\n            else:\n                result = i / 2\n    elif x < 0:\n        while x < 0:\n            x += 1\n    return result\n\"\"\"\n        self.create_test_file('test.py', python_code)\n        \n        complexity = self.analyzer.analyze_complexity('test.py')\n        \n        assert complexity['functions'] == 2\n        assert complexity['lines_of_code'] > 0\n        assert complexity['cyclomatic'] > 1  # Should have decision points\n        assert complexity['cognitive'] >= 1  # Should have nesting\n    \n    def test_javascript_complexity_analysis(self):\n        \"\"\"Test JavaScript complexity analysis\"\"\"\n        js_code = \"\"\"\nfunction simpleFunction() {\n    return \"hello\";\n}\n\nconst complexFunction = (x) => {\n    if (x > 0) {\n        for (let i = 0; i < x; i++) {\n            if (i % 2 === 0) {\n                try {\n                    const result = i * 2;\n                } catch (error) {\n                    const result = 0;\n                }\n            } else {\n                const result = i / 2;\n            }\n        }\n    } else if (x < 0) {\n        while (x < 0) {\n            x++;\n        }\n    }\n    return result;\n};\n\"\"\"\n        self.create_test_file('test.js', js_code)\n        \n        complexity = self.analyzer.analyze_complexity('test.js')\n        \n        assert complexity['functions'] == 2\n        assert complexity['lines_of_code'] > 0\n        assert complexity['cyclomatic'] > 1\n    \n    def test_java_complexity_analysis(self):\n        \"\"\"Test Java complexity analysis\"\"\"\n        java_code = \"\"\"\npublic class TestClass {\n    public void simpleMethod() {\n        System.out.println(\"hello\");\n    }\n    \n    public void complexMethod(int x) {\n        if (x > 0) {\n            for (int i = 0; i < x; i++) {\n                if (i % 2 == 0) {\n                    try {\n                        int result = i * 2;\n                    } catch (Exception e) {\n                        int result = 0;\n                    }\n                } else {\n                    int result = i / 2;\n                }\n            }\n        } else if (x < 0) {\n            while (x < 0) {\n                x++;\n            }\n        }\n    }\n}\n\"\"\"\n        self.create_test_file('TestClass.java', java_code)\n        \n        complexity = self.analyzer.analyze_complexity('TestClass.java')\n        \n        assert complexity['functions'] >= 2\n        assert complexity['lines_of_code'] > 0\n        assert complexity['cyclomatic'] > 1\n    \n    def test_go_dependency_analysis(self):\n        \"\"\"Test Go dependency extraction\"\"\"\n        go_code = \"\"\"\npackage main\n\nimport (\n    \"fmt\"\n    \"net/http\"\n    \"github.com/gin-gonic/gin\"\n    \"database/sql\"\n)\n\"\"\"\n        self.create_test_file('main.go', go_code)\n        \n        deps = self.analyzer.analyze_dependencies('main.go')\n        \n        expected_imports = ['fmt', 'net/http', 'github.com/gin-gonic/gin', 'database/sql']\n        for imp in expected_imports:\n            assert imp in deps['imports']\n    \n    def test_rust_dependency_analysis(self):\n        \"\"\"Test Rust dependency extraction\"\"\"\n        rust_code = \"\"\"\nuse std::collections::HashMap;\nuse std::io::Result;\nuse serde::{Deserialize, Serialize};\nextern crate tokio;\nuse crate::utils::helper;\n\"\"\"\n        self.create_test_file('main.rs', rust_code)\n        \n        deps = self.analyzer.analyze_dependencies('main.rs')\n        \n        expected_imports = ['std', 'serde', 'tokio', 'crate']\n        for imp in expected_imports:\n            assert imp in deps['imports']\n    \n    def test_project_tree_generation(self):\n        \"\"\"Test project tree visualization\"\"\"\n        # Create a test directory structure\n        self.create_test_file('src/main.py', 'print(\"hello\")')\n        self.create_test_file('src/utils/helper.py', 'def help(): pass')\n        self.create_test_file('tests/test_main.py', 'def test(): pass')\n        self.create_test_file('README.md', '# Test')\n        \n        tree = self.analyzer.get_project_tree(max_depth=2)\n        \n        # The tree should contain the project name\n        assert os.path.basename(self.temp_dir) in str(tree)\n    \n    def test_summary_generation(self):\n        \"\"\"Test project summary generation\"\"\"\n        # Create test files\n        self.create_test_file('src/main.py', 'print(\"hello\")')\n        self.create_test_file('src/utils.py', 'def help(): pass')\n        self.create_test_file('README.md', '# Test Project')\n        \n        # Run scan first to populate data\n        asyncio.run(self.analyzer.scan_project_async())\n        \n        summary = self.analyzer.generate_summary()\n        \n        assert \"WORKSPACE CONTEXT\" in summary\n        assert \"PROJECT STATISTICS\" in summary\n        assert str(self.analyzer.total_files) in summary\n        assert str(self.analyzer.total_lines) in summary\n    \n    def test_error_handling_in_analysis(self):\n        \"\"\"Test error handling during analysis\"\"\"\n        # Create a file with invalid encoding\n        invalid_file = os.path.join(self.temp_dir, 'invalid.py')\n        with open(invalid_file, 'wb') as f:\n            f.write(b'\\xff\\xfe invalid content')\n        \n        # Should handle gracefully\n        result = asyncio.run(self.analyzer.scan_project_async())\n        assert 'skipped_files' in result\n    \n    def test_memory_management(self):\n        \"\"\"Test memory management during large scans\"\"\"\n        # Create many small files\n        for i in range(100):\n            self.create_test_file(f'src/file_{i}.py', f'def func_{i}(): return {i}')\n        \n        result = asyncio.run(self.analyzer.scan_project_async())\n        \n        # Should process files but respect limits\n        assert result['total_files'] == 100\n        assert result['processed_size_mb'] < 100  # Should be reasonable"}, {"filepath": "tests\\test_analyzer.py", "start_line": 19, "end_line": 23, "type": "function", "name": "setup_method", "content": "    def setup_method(self):\n        \"\"\"Set up test fixtures\"\"\"\n        self.console_mock = Mock()\n        self.temp_dir = tempfile.mkdtemp()\n        self.analyzer = CodeAnalyzer(self.temp_dir, self.console_mock)", "docstring": "Set up test fixtures", "searchable_text": "setup_method Set up test fixtures     def setup_method(self):\n        \"\"\"Set up test fixtures\"\"\"\n        self.console_mock = Mock()\n        self.temp_dir = tempfile.mkdtemp()\n        self.analyzer = CodeAnalyzer(self.temp_dir, self.console_mock)"}, {"filepath": "tests\\test_analyzer.py", "start_line": 25, "end_line": 28, "type": "function", "name": "teardown_method", "content": "    def teardown_method(self):\n        \"\"\"Clean up test fixtures\"\"\"\n        import shutil\n        shutil.rmtree(self.temp_dir, ignore_errors=True)", "docstring": "Clean up test fixtures", "searchable_text": "teardown_method Clean up test fixtures     def teardown_method(self):\n        \"\"\"Clean up test fixtures\"\"\"\n        import shutil\n        shutil.rmtree(self.temp_dir, ignore_errors=True)"}, {"filepath": "tests\\test_analyzer.py", "start_line": 30, "end_line": 36, "type": "function", "name": "create_test_file", "content": "    def create_test_file(self, filename: str, content: str):\n        \"\"\"Helper to create test files\"\"\"\n        filepath = os.path.join(self.temp_dir, filename)\n        os.makedirs(os.path.dirname(filepath), exist_ok=True)\n        with open(filepath, 'w') as f:\n            f.write(content)\n        return filepath", "docstring": "Helper to create test files", "searchable_text": "create_test_file Helper to create test files     def create_test_file(self, filename: str, content: str):\n        \"\"\"Helper to create test files\"\"\"\n        filepath = os.path.join(self.temp_dir, filename)\n        os.makedirs(os.path.dirname(filepath), exist_ok=True)\n        with open(filepath, 'w') as f:\n            f.write(content)\n        return filepath"}, {"filepath": "tests\\test_analyzer.py", "start_line": 38, "end_line": 54, "type": "function", "name": "test_project_type_detection", "content": "    def test_project_type_detection(self):\n        \"\"\"Test project type detection\"\"\"\n        # Python project\n        self.create_test_file('requirements.txt', 'requests==2.25.0')\n        self.create_test_file('setup.py', 'from setuptools import setup')\n        self.analyzer._detect_project_type()\n        assert self.analyzer.project_type == 'python'\n        \n        # Node.js project\n        self.create_test_file('package.json', '{\"name\": \"test\"}')\n        self.analyzer._detect_project_type()\n        assert self.analyzer.project_type == 'node'\n        \n        # Rust project\n        self.create_test_file('Cargo.toml', '[package]\\nname = \"test\"')\n        self.analyzer._detect_project_type()\n        assert self.analyzer.project_type == 'rust'", "docstring": "Test project type detection", "searchable_text": "test_project_type_detection Test project type detection     def test_project_type_detection(self):\n        \"\"\"Test project type detection\"\"\"\n        # Python project\n        self.create_test_file('requirements.txt', 'requests==2.25.0')\n        self.create_test_file('setup.py', 'from setuptools import setup')\n        self.analyzer._detect_project_type()\n        assert self.analyzer.project_type == 'python'\n        \n        # Node.js project\n        self.create_test_file('package.json', '{\"name\": \"test\"}')\n        self.analyzer._detect_project_type()\n        assert self.analyzer.project_type == 'node'\n        \n        # Rust project\n        self.create_test_file('Cargo.toml', '[package]\\nname = \"test\"')\n        self.analyzer._detect_project_type()\n        assert self.analyzer.project_type == 'rust'"}, {"filepath": "tests\\test_analyzer.py", "start_line": 56, "end_line": 67, "type": "function", "name": "test_file_filtering", "content": "    def test_file_filtering(self):\n        \"\"\"Test that files are properly filtered\"\"\"\n        # Should be ignored\n        assert self.analyzer._should_ignore_file('.git/config')\n        assert self.analyzer._should_ignore_file('node_modules/package.json')\n        assert self.analyzer._should_ignore_file('test.pyc')\n        assert self.analyzer._should_ignore_file('test.log')\n        \n        # Should not be ignored\n        assert not self.analyzer._should_ignore_file('src/main.py')\n        assert not self.analyzer._should_ignore_file('.env.example')\n        assert not self.analyzer._should_ignore_file('.gitignore')", "docstring": "Test that files are properly filtered", "searchable_text": "test_file_filtering Test that files are properly filtered     def test_file_filtering(self):\n        \"\"\"Test that files are properly filtered\"\"\"\n        # Should be ignored\n        assert self.analyzer._should_ignore_file('.git/config')\n        assert self.analyzer._should_ignore_file('node_modules/package.json')\n        assert self.analyzer._should_ignore_file('test.pyc')\n        assert self.analyzer._should_ignore_file('test.log')\n        \n        # Should not be ignored\n        assert not self.analyzer._should_ignore_file('src/main.py')\n        assert not self.analyzer._should_ignore_file('.env.example')\n        assert not self.analyzer._should_ignore_file('.gitignore')"}, {"filepath": "tests\\test_analyzer.py", "start_line": 70, "end_line": 83, "type": "function", "name": "test_async_project_scan", "content": "    async def test_async_project_scan(self):\n        \"\"\"Test async project scanning\"\"\"\n        # Create test files\n        self.create_test_file('src/main.py', 'print(\"hello\")')\n        self.create_test_file('src/utils.py', 'def helper(): pass')\n        self.create_test_file('README.md', '# Test Project')\n        self.create_test_file('package.json', '{\"name\": \"test\"}')\n        \n        result = await self.analyzer.scan_project_async()\n        \n        assert result['total_files'] >= 2  # At least the Python files\n        assert result['project_type'] == 'node'  # Should detect Node.js\n        assert 'processed_size_mb' in result\n        assert 'skipped_files' in result", "docstring": "Test async project scanning", "searchable_text": "test_async_project_scan Test async project scanning     async def test_async_project_scan(self):\n        \"\"\"Test async project scanning\"\"\"\n        # Create test files\n        self.create_test_file('src/main.py', 'print(\"hello\")')\n        self.create_test_file('src/utils.py', 'def helper(): pass')\n        self.create_test_file('README.md', '# Test Project')\n        self.create_test_file('package.json', '{\"name\": \"test\"}')\n        \n        result = await self.analyzer.scan_project_async()\n        \n        assert result['total_files'] >= 2  # At least the Python files\n        assert result['project_type'] == 'node'  # Should detect Node.js\n        assert 'processed_size_mb' in result\n        assert 'skipped_files' in result"}, {"filepath": "tests\\test_analyzer.py", "start_line": 85, "end_line": 101, "type": "function", "name": "test_python_dependency_analysis", "content": "    def test_python_dependency_analysis(self):\n        \"\"\"Test Python dependency extraction\"\"\"\n        python_code = \"\"\"\nimport os\nimport sys\nfrom datetime import datetime\nfrom collections import defaultdict\nimport requests\nimport numpy as np\n\"\"\"\n        self.create_test_file('test.py', python_code)\n        \n        deps = self.analyzer.analyze_dependencies('test.py')\n        \n        expected_imports = ['os', 'sys', 'datetime', 'collections', 'requests', 'numpy']\n        for imp in expected_imports:\n            assert imp in deps['imports']", "docstring": "Test Python dependency extraction", "searchable_text": "test_python_dependency_analysis Test Python dependency extraction     def test_python_dependency_analysis(self):\n        \"\"\"Test Python dependency extraction\"\"\"\n        python_code = \"\"\"\nimport os\nimport sys\nfrom datetime import datetime\nfrom collections import defaultdict\nimport requests\nimport numpy as np\n\"\"\"\n        self.create_test_file('test.py', python_code)\n        \n        deps = self.analyzer.analyze_dependencies('test.py')\n        \n        expected_imports = ['os', 'sys', 'datetime', 'collections', 'requests', 'numpy']\n        for imp in expected_imports:\n            assert imp in deps['imports']"}, {"filepath": "tests\\test_analyzer.py", "start_line": 103, "end_line": 118, "type": "function", "name": "test_javascript_dependency_analysis", "content": "    def test_javascript_dependency_analysis(self):\n        \"\"\"Test JavaScript dependency extraction\"\"\"\n        js_code = \"\"\"\nimport React from 'react';\nimport { useState } from 'react';\nimport axios from 'axios';\nconst express = require('express');\nimport lodash from 'lodash';\n\"\"\"\n        self.create_test_file('test.js', js_code)\n        \n        deps = self.analyzer.analyze_dependencies('test.js')\n        \n        expected_imports = ['react', 'axios', 'express', 'lodash']\n        for imp in expected_imports:\n            assert imp in deps['imports']", "docstring": "Test JavaScript dependency extraction", "searchable_text": "test_javascript_dependency_analysis Test JavaScript dependency extraction     def test_javascript_dependency_analysis(self):\n        \"\"\"Test JavaScript dependency extraction\"\"\"\n        js_code = \"\"\"\nimport React from 'react';\nimport { useState } from 'react';\nimport axios from 'axios';\nconst express = require('express');\nimport lodash from 'lodash';\n\"\"\"\n        self.create_test_file('test.js', js_code)\n        \n        deps = self.analyzer.analyze_dependencies('test.js')\n        \n        expected_imports = ['react', 'axios', 'express', 'lodash']\n        for imp in expected_imports:\n            assert imp in deps['imports']"}, {"filepath": "tests\\test_analyzer.py", "start_line": 120, "end_line": 134, "type": "function", "name": "test_java_dependency_analysis", "content": "    def test_java_dependency_analysis(self):\n        \"\"\"Test Java dependency extraction\"\"\"\n        java_code = \"\"\"\nimport java.util.List;\nimport java.util.ArrayList;\nimport com.example.MyClass;\nimport org.springframework.boot.SpringApplication;\n\"\"\"\n        self.create_test_file('Test.java', java_code)\n        \n        deps = self.analyzer.analyze_dependencies('Test.java')\n        \n        expected_imports = ['java', 'com', 'org']\n        for imp in expected_imports:\n            assert imp in deps['imports']", "docstring": "Test Java dependency extraction", "searchable_text": "test_java_dependency_analysis Test Java dependency extraction     def test_java_dependency_analysis(self):\n        \"\"\"Test Java dependency extraction\"\"\"\n        java_code = \"\"\"\nimport java.util.List;\nimport java.util.ArrayList;\nimport com.example.MyClass;\nimport org.springframework.boot.SpringApplication;\n\"\"\"\n        self.create_test_file('Test.java', java_code)\n        \n        deps = self.analyzer.analyze_dependencies('Test.java')\n        \n        expected_imports = ['java', 'com', 'org']\n        for imp in expected_imports:\n            assert imp in deps['imports']"}, {"filepath": "tests\\test_analyzer.py", "start_line": 136, "end_line": 157, "type": "function", "name": "test_architecture_pattern_detection", "content": "    def test_architecture_pattern_detection(self):\n        \"\"\"Test architecture pattern detection\"\"\"\n        files = [\n            'src/controllers/UserController.java',\n            'src/models/User.java',\n            'src/views/UserView.jsp',\n            'services/auth-service.js',\n            'api/gateway.js',\n            'functions/lambda-handler.js',\n            'modules/user/module.py',\n            'components/Button.jsx',\n            'repositories/UserRepository.java',\n            'dao/UserDAO.java'\n        ]\n        \n        patterns = self.analyzer.detect_architecture_patterns(files)\n        \n        assert patterns['mvc'] >= 3  # controller, model, view\n        assert patterns['microservices'] >= 2  # service, gateway\n        assert patterns['serverless'] >= 1  # lambda\n        assert patterns['modular'] >= 2  # modules, components\n        assert patterns['repository'] >= 2  # repository, dao", "docstring": "Test architecture pattern detection", "searchable_text": "test_architecture_pattern_detection Test architecture pattern detection     def test_architecture_pattern_detection(self):\n        \"\"\"Test architecture pattern detection\"\"\"\n        files = [\n            'src/controllers/UserController.java',\n            'src/models/User.java',\n            'src/views/UserView.jsp',\n            'services/auth-service.js',\n            'api/gateway.js',\n            'functions/lambda-handler.js',\n            'modules/user/module.py',\n            'components/Button.jsx',\n            'repositories/UserRepository.java',\n            'dao/UserDAO.java'\n        ]\n        \n        patterns = self.analyzer.detect_architecture_patterns(files)\n        \n        assert patterns['mvc'] >= 3  # controller, model, view\n        assert patterns['microservices'] >= 2  # service, gateway\n        assert patterns['serverless'] >= 1  # lambda\n        assert patterns['modular'] >= 2  # modules, components\n        assert patterns['repository'] >= 2  # repository, dao"}, {"filepath": "tests\\test_analyzer.py", "start_line": 159, "end_line": 187, "type": "function", "name": "test_python_complexity_analysis", "content": "    def test_python_complexity_analysis(self):\n        \"\"\"Test Python complexity analysis\"\"\"\n        python_code = \"\"\"\ndef simple_function():\n    return \"hello\"\n\ndef complex_function(x):\n    if x > 0:\n        for i in range(x):\n            if i % 2 == 0:\n                try:\n                    result = i * 2\n                except Exception:\n                    result = 0\n            else:\n                result = i / 2\n    elif x < 0:\n        while x < 0:\n            x += 1\n    return result\n\"\"\"\n        self.create_test_file('test.py', python_code)\n        \n        complexity = self.analyzer.analyze_complexity('test.py')\n        \n        assert complexity['functions'] == 2\n        assert complexity['lines_of_code'] > 0\n        assert complexity['cyclomatic'] > 1  # Should have decision points\n        assert complexity['cognitive'] >= 1  # Should have nesting", "docstring": "Test Python complexity analysis", "searchable_text": "test_python_complexity_analysis Test Python complexity analysis     def test_python_complexity_analysis(self):\n        \"\"\"Test Python complexity analysis\"\"\"\n        python_code = \"\"\"\ndef simple_function():\n    return \"hello\"\n\ndef complex_function(x):\n    if x > 0:\n        for i in range(x):\n            if i % 2 == 0:\n                try:\n                    result = i * 2\n                except Exception:\n                    result = 0\n            else:\n                result = i / 2\n    elif x < 0:\n        while x < 0:\n            x += 1\n    return result\n\"\"\"\n        self.create_test_file('test.py', python_code)\n        \n        complexity = self.analyzer.analyze_complexity('test.py')\n        \n        assert complexity['functions'] == 2\n        assert complexity['lines_of_code'] > 0\n        assert complexity['cyclomatic'] > 1  # Should have decision points\n        assert complexity['cognitive'] >= 1  # Should have nesting"}, {"filepath": "tests\\test_analyzer.py", "start_line": 189, "end_line": 223, "type": "function", "name": "test_javascript_complexity_analysis", "content": "    def test_javascript_complexity_analysis(self):\n        \"\"\"Test JavaScript complexity analysis\"\"\"\n        js_code = \"\"\"\nfunction simpleFunction() {\n    return \"hello\";\n}\n\nconst complexFunction = (x) => {\n    if (x > 0) {\n        for (let i = 0; i < x; i++) {\n            if (i % 2 === 0) {\n                try {\n                    const result = i * 2;\n                } catch (error) {\n                    const result = 0;\n                }\n            } else {\n                const result = i / 2;\n            }\n        }\n    } else if (x < 0) {\n        while (x < 0) {\n            x++;\n        }\n    }\n    return result;\n};\n\"\"\"\n        self.create_test_file('test.js', js_code)\n        \n        complexity = self.analyzer.analyze_complexity('test.js')\n        \n        assert complexity['functions'] == 2\n        assert complexity['lines_of_code'] > 0\n        assert complexity['cyclomatic'] > 1", "docstring": "Test JavaScript complexity analysis", "searchable_text": "test_javascript_complexity_analysis Test JavaScript complexity analysis     def test_javascript_complexity_analysis(self):\n        \"\"\"Test JavaScript complexity analysis\"\"\"\n        js_code = \"\"\"\nfunction simpleFunction() {\n    return \"hello\";\n}\n\nconst complexFunction = (x) => {\n    if (x > 0) {\n        for (let i = 0; i < x; i++) {\n            if (i % 2 === 0) {\n                try {\n                    const result = i * 2;\n                } catch (error) {\n                    const result = 0;\n                }\n            } else {\n                const result = i / 2;\n            }\n        }\n    } else if (x < 0) {\n        while (x < 0) {\n            x++;\n        }\n    }\n    return result;\n};\n\"\"\"\n        self.create_test_file('test.js', js_code)\n        \n        complexity = self.analyzer.analyze_complexity('test.js')\n        \n        assert complexity['functions'] == 2\n        assert complexity['lines_of_code'] > 0\n        assert complexity['cyclomatic'] > 1"}, {"filepath": "tests\\test_analyzer.py", "start_line": 225, "end_line": 260, "type": "function", "name": "test_java_complexity_analysis", "content": "    def test_java_complexity_analysis(self):\n        \"\"\"Test Java complexity analysis\"\"\"\n        java_code = \"\"\"\npublic class TestClass {\n    public void simpleMethod() {\n        System.out.println(\"hello\");\n    }\n    \n    public void complexMethod(int x) {\n        if (x > 0) {\n            for (int i = 0; i < x; i++) {\n                if (i % 2 == 0) {\n                    try {\n                        int result = i * 2;\n                    } catch (Exception e) {\n                        int result = 0;\n                    }\n                } else {\n                    int result = i / 2;\n                }\n            }\n        } else if (x < 0) {\n            while (x < 0) {\n                x++;\n            }\n        }\n    }\n}\n\"\"\"\n        self.create_test_file('TestClass.java', java_code)\n        \n        complexity = self.analyzer.analyze_complexity('TestClass.java')\n        \n        assert complexity['functions'] >= 2\n        assert complexity['lines_of_code'] > 0\n        assert complexity['cyclomatic'] > 1", "docstring": "Test Java complexity analysis", "searchable_text": "test_java_complexity_analysis Test Java complexity analysis     def test_java_complexity_analysis(self):\n        \"\"\"Test Java complexity analysis\"\"\"\n        java_code = \"\"\"\npublic class TestClass {\n    public void simpleMethod() {\n        System.out.println(\"hello\");\n    }\n    \n    public void complexMethod(int x) {\n        if (x > 0) {\n            for (int i = 0; i < x; i++) {\n                if (i % 2 == 0) {\n                    try {\n                        int result = i * 2;\n                    } catch (Exception e) {\n                        int result = 0;\n                    }\n                } else {\n                    int result = i / 2;\n                }\n            }\n        } else if (x < 0) {\n            while (x < 0) {\n                x++;\n            }\n        }\n    }\n}\n\"\"\"\n        self.create_test_file('TestClass.java', java_code)\n        \n        complexity = self.analyzer.analyze_complexity('TestClass.java')\n        \n        assert complexity['functions'] >= 2\n        assert complexity['lines_of_code'] > 0\n        assert complexity['cyclomatic'] > 1"}, {"filepath": "tests\\test_analyzer.py", "start_line": 262, "end_line": 280, "type": "function", "name": "test_go_dependency_analysis", "content": "    def test_go_dependency_analysis(self):\n        \"\"\"Test Go dependency extraction\"\"\"\n        go_code = \"\"\"\npackage main\n\nimport (\n    \"fmt\"\n    \"net/http\"\n    \"github.com/gin-gonic/gin\"\n    \"database/sql\"\n)\n\"\"\"\n        self.create_test_file('main.go', go_code)\n        \n        deps = self.analyzer.analyze_dependencies('main.go')\n        \n        expected_imports = ['fmt', 'net/http', 'github.com/gin-gonic/gin', 'database/sql']\n        for imp in expected_imports:\n            assert imp in deps['imports']", "docstring": "Test Go dependency extraction", "searchable_text": "test_go_dependency_analysis Test Go dependency extraction     def test_go_dependency_analysis(self):\n        \"\"\"Test Go dependency extraction\"\"\"\n        go_code = \"\"\"\npackage main\n\nimport (\n    \"fmt\"\n    \"net/http\"\n    \"github.com/gin-gonic/gin\"\n    \"database/sql\"\n)\n\"\"\"\n        self.create_test_file('main.go', go_code)\n        \n        deps = self.analyzer.analyze_dependencies('main.go')\n        \n        expected_imports = ['fmt', 'net/http', 'github.com/gin-gonic/gin', 'database/sql']\n        for imp in expected_imports:\n            assert imp in deps['imports']"}, {"filepath": "tests\\test_analyzer.py", "start_line": 282, "end_line": 297, "type": "function", "name": "test_rust_dependency_analysis", "content": "    def test_rust_dependency_analysis(self):\n        \"\"\"Test Rust dependency extraction\"\"\"\n        rust_code = \"\"\"\nuse std::collections::HashMap;\nuse std::io::Result;\nuse serde::{Deserialize, Serialize};\nextern crate tokio;\nuse crate::utils::helper;\n\"\"\"\n        self.create_test_file('main.rs', rust_code)\n        \n        deps = self.analyzer.analyze_dependencies('main.rs')\n        \n        expected_imports = ['std', 'serde', 'tokio', 'crate']\n        for imp in expected_imports:\n            assert imp in deps['imports']", "docstring": "Test Rust dependency extraction", "searchable_text": "test_rust_dependency_analysis Test Rust dependency extraction     def test_rust_dependency_analysis(self):\n        \"\"\"Test Rust dependency extraction\"\"\"\n        rust_code = \"\"\"\nuse std::collections::HashMap;\nuse std::io::Result;\nuse serde::{Deserialize, Serialize};\nextern crate tokio;\nuse crate::utils::helper;\n\"\"\"\n        self.create_test_file('main.rs', rust_code)\n        \n        deps = self.analyzer.analyze_dependencies('main.rs')\n        \n        expected_imports = ['std', 'serde', 'tokio', 'crate']\n        for imp in expected_imports:\n            assert imp in deps['imports']"}, {"filepath": "tests\\test_analyzer.py", "start_line": 299, "end_line": 310, "type": "function", "name": "test_project_tree_generation", "content": "    def test_project_tree_generation(self):\n        \"\"\"Test project tree visualization\"\"\"\n        # Create a test directory structure\n        self.create_test_file('src/main.py', 'print(\"hello\")')\n        self.create_test_file('src/utils/helper.py', 'def help(): pass')\n        self.create_test_file('tests/test_main.py', 'def test(): pass')\n        self.create_test_file('README.md', '# Test')\n        \n        tree = self.analyzer.get_project_tree(max_depth=2)\n        \n        # The tree should contain the project name\n        assert os.path.basename(self.temp_dir) in str(tree)", "docstring": "Test project tree visualization", "searchable_text": "test_project_tree_generation Test project tree visualization     def test_project_tree_generation(self):\n        \"\"\"Test project tree visualization\"\"\"\n        # Create a test directory structure\n        self.create_test_file('src/main.py', 'print(\"hello\")')\n        self.create_test_file('src/utils/helper.py', 'def help(): pass')\n        self.create_test_file('tests/test_main.py', 'def test(): pass')\n        self.create_test_file('README.md', '# Test')\n        \n        tree = self.analyzer.get_project_tree(max_depth=2)\n        \n        # The tree should contain the project name\n        assert os.path.basename(self.temp_dir) in str(tree)"}, {"filepath": "tests\\test_analyzer.py", "start_line": 312, "end_line": 327, "type": "function", "name": "test_summary_generation", "content": "    def test_summary_generation(self):\n        \"\"\"Test project summary generation\"\"\"\n        # Create test files\n        self.create_test_file('src/main.py', 'print(\"hello\")')\n        self.create_test_file('src/utils.py', 'def help(): pass')\n        self.create_test_file('README.md', '# Test Project')\n        \n        # Run scan first to populate data\n        asyncio.run(self.analyzer.scan_project_async())\n        \n        summary = self.analyzer.generate_summary()\n        \n        assert \"WORKSPACE CONTEXT\" in summary\n        assert \"PROJECT STATISTICS\" in summary\n        assert str(self.analyzer.total_files) in summary\n        assert str(self.analyzer.total_lines) in summary", "docstring": "Test project summary generation", "searchable_text": "test_summary_generation Test project summary generation     def test_summary_generation(self):\n        \"\"\"Test project summary generation\"\"\"\n        # Create test files\n        self.create_test_file('src/main.py', 'print(\"hello\")')\n        self.create_test_file('src/utils.py', 'def help(): pass')\n        self.create_test_file('README.md', '# Test Project')\n        \n        # Run scan first to populate data\n        asyncio.run(self.analyzer.scan_project_async())\n        \n        summary = self.analyzer.generate_summary()\n        \n        assert \"WORKSPACE CONTEXT\" in summary\n        assert \"PROJECT STATISTICS\" in summary\n        assert str(self.analyzer.total_files) in summary\n        assert str(self.analyzer.total_lines) in summary"}, {"filepath": "tests\\test_analyzer.py", "start_line": 329, "end_line": 338, "type": "function", "name": "test_error_handling_in_analysis", "content": "    def test_error_handling_in_analysis(self):\n        \"\"\"Test error handling during analysis\"\"\"\n        # Create a file with invalid encoding\n        invalid_file = os.path.join(self.temp_dir, 'invalid.py')\n        with open(invalid_file, 'wb') as f:\n            f.write(b'\\xff\\xfe invalid content')\n        \n        # Should handle gracefully\n        result = asyncio.run(self.analyzer.scan_project_async())\n        assert 'skipped_files' in result", "docstring": "Test error handling during analysis", "searchable_text": "test_error_handling_in_analysis Test error handling during analysis     def test_error_handling_in_analysis(self):\n        \"\"\"Test error handling during analysis\"\"\"\n        # Create a file with invalid encoding\n        invalid_file = os.path.join(self.temp_dir, 'invalid.py')\n        with open(invalid_file, 'wb') as f:\n            f.write(b'\\xff\\xfe invalid content')\n        \n        # Should handle gracefully\n        result = asyncio.run(self.analyzer.scan_project_async())\n        assert 'skipped_files' in result"}, {"filepath": "tests\\test_analyzer.py", "start_line": 340, "end_line": 350, "type": "function", "name": "test_memory_management", "content": "    def test_memory_management(self):\n        \"\"\"Test memory management during large scans\"\"\"\n        # Create many small files\n        for i in range(100):\n            self.create_test_file(f'src/file_{i}.py', f'def func_{i}(): return {i}')\n        \n        result = asyncio.run(self.analyzer.scan_project_async())\n        \n        # Should process files but respect limits\n        assert result['total_files'] == 100\n        assert result['processed_size_mb'] < 100  # Should be reasonable", "docstring": "Test memory management during large scans", "searchable_text": "test_memory_management Test memory management during large scans     def test_memory_management(self):\n        \"\"\"Test memory management during large scans\"\"\"\n        # Create many small files\n        for i in range(100):\n            self.create_test_file(f'src/file_{i}.py', f'def func_{i}(): return {i}')\n        \n        result = asyncio.run(self.analyzer.scan_project_async())\n        \n        # Should process files but respect limits\n        assert result['total_files'] == 100\n        assert result['processed_size_mb'] < 100  # Should be reasonable"}, {"filepath": "tests\\test_intelligent_features.py", "start_line": 22, "end_line": 107, "type": "class", "name": "TestContextEngine", "content": "class TestContextEngine:\n    \"\"\"Test semantic search and code chunking\"\"\"\n    \n    def setup_method(self):\n        \"\"\"Set up test fixtures\"\"\"\n        self.temp_dir = tempfile.mkdtemp()\n        self.console_mock = Mock()\n        self.engine = ContextEngine(self.temp_dir, self.console_mock)\n    \n    def teardown_method(self):\n        \"\"\"Clean up\"\"\"\n        import shutil\n        shutil.rmtree(self.temp_dir, ignore_errors=True)\n    \n    def test_chunk_python_code(self):\n        \"\"\"Test Python code chunking\"\"\"\n        python_code = \"\"\"\ndef hello():\n    '''Say hello'''\n    return \"Hello\"\n\nclass MyClass:\n    '''A test class'''\n    def method(self):\n        pass\n\"\"\"\n        chunks = self.engine._chunk_python('test.py', python_code)\n        \n        assert len(chunks) >= 2  # Should find function and class\n        assert any(c['type'] == 'function' for c in chunks)\n        assert any(c['type'] == 'class' for c in chunks)\n        assert any(c['name'] == 'hello' for c in chunks)\n        assert any(c['name'] == 'MyClass' for c in chunks)\n    \n    def test_chunk_javascript_code(self):\n        \"\"\"Test JavaScript code chunking\"\"\"\n        js_code = \"\"\"\nfunction greet(name) {\n    return `Hello ${name}`;\n}\n\nconst add = (a, b) => a + b;\n\nclass Calculator {\n    multiply(a, b) {\n        return a * b;\n    }\n}\n\"\"\"\n        chunks = self.engine._chunk_javascript('test.js', js_code)\n        \n        assert len(chunks) >= 3  # Function, arrow function, class\n        assert any('greet' in c['name'] for c in chunks)\n        assert any('add' in c['name'] for c in chunks)\n        assert any('Calculator' in c['name'] for c in chunks)\n    \n    def test_chunk_by_lines_fallback(self):\n        \"\"\"Test fallback line-based chunking\"\"\"\n        content = \"\\n\".join([f\"line {i}\" for i in range(100)])\n        chunks = self.engine._chunk_by_lines('test.txt', content, chunk_size=20)\n        \n        assert len(chunks) == 5  # 100 lines / 20 per chunk\n        assert chunks[0]['start_line'] == 1\n        assert chunks[0]['end_line'] == 20\n    \n    @pytest.mark.skipif(not EMBEDDINGS_AVAILABLE, reason=\"Embeddings not available\")\n    def test_search_functionality(self):\n        \"\"\"Test semantic search\"\"\"\n        # Create mock code analyzer\n        code_analyzer = Mock()\n        code_analyzer.files = ['test.py']\n        code_analyzer.read_file_content = Mock(return_value=\"\"\"\ndef authenticate_user(username, password):\n    '''Authenticate a user with credentials'''\n    return check_credentials(username, password)\n\"\"\")\n        \n        # Build index\n        success = self.engine.build_index(code_analyzer)\n        \n        if success:\n            # Search for authentication\n            results = self.engine.search(\"user authentication\", top_k=1)\n            \n            assert len(results) > 0\n            assert 'authenticate' in results[0]['content'].lower()", "docstring": "Test semantic search and code chunking", "searchable_text": "TestContextEngine Test semantic search and code chunking class TestContextEngine:\n    \"\"\"Test semantic search and code chunking\"\"\"\n    \n    def setup_method(self):\n        \"\"\"Set up test fixtures\"\"\"\n        self.temp_dir = tempfile.mkdtemp()\n        self.console_mock = Mock()\n        self.engine = ContextEngine(self.temp_dir, self.console_mock)\n    \n    def teardown_method(self):\n        \"\"\"Clean up\"\"\"\n        import shutil\n        shutil.rmtree(self.temp_dir, ignore_errors=True)\n    \n    def test_chunk_python_code(self):\n        \"\"\"Test Python code chunking\"\"\"\n        python_code = \"\"\"\ndef hello():\n    '''Say hello'''\n    return \"Hello\"\n\nclass MyClass:\n    '''A test class'''\n    def method(self):\n        pass\n\"\"\"\n        chunks = self.engine._chunk_python('test.py', python_code)\n        \n        assert len(chunks) >= 2  # Should find function and class\n        assert any(c['type'] == 'function' for c in chunks)\n        assert any(c['type'] == 'class' for c in chunks)\n        assert any(c['name'] == 'hello' for c in chunks)\n        assert any(c['name'] == 'MyClass' for c in chunks)\n    \n    def test_chunk_javascript_code(self):\n        \"\"\"Test JavaScript code chunking\"\"\"\n        js_code = \"\"\"\nfunction greet(name) {\n    return `Hello ${name}`;\n}\n\nconst add = (a, b) => a + b;\n\nclass Calculator {\n    multiply(a, b) {\n        return a * b;\n    }\n}\n\"\"\"\n        chunks = self.engine._chunk_javascript('test.js', js_code)\n        \n        assert len(chunks) >= 3  # Function, arrow function, class\n        assert any('greet' in c['name'] for c in chunks)\n        assert any('add' in c['name'] for c in chunks)\n        assert any('Calculator' in c['name'] for c in chunks)\n    \n    def test_chunk_by_lines_fallback(self):\n        \"\"\"Test fallback line-based chunking\"\"\"\n        content = \"\\n\".join([f\"line {i}\" for i in range(100)])\n        chunks = self.engine._chunk_by_lines('test.txt', content, chunk_size=20)\n        \n        assert len(chunks) == 5  # 100 lines / 20 per chunk\n        assert chunks[0]['start_line'] == 1\n        assert chunks[0]['end_line'] == 20\n    \n    @pytest.mark.skipif(not EMBEDDINGS_AVAILABLE, reason=\"Embeddings not available\")\n    def test_search_functionality(self):\n        \"\"\"Test semantic search\"\"\"\n        # Create mock code analyzer\n        code_analyzer = Mock()\n        code_analyzer.files = ['test.py']\n        code_analyzer.read_file_content = Mock(return_value=\"\"\"\ndef authenticate_user(username, password):\n    '''Authenticate a user with credentials'''\n    return check_credentials(username, password)\n\"\"\")\n        \n        # Build index\n        success = self.engine.build_index(code_analyzer)\n        \n        if success:\n            # Search for authentication\n            results = self.engine.search(\"user authentication\", top_k=1)\n            \n            assert len(results) > 0\n            assert 'authenticate' in results[0]['content'].lower()"}, {"filepath": "tests\\test_intelligent_features.py", "start_line": 110, "end_line": 217, "type": "class", "name": "TestSymbolResolver", "content": "class TestSymbolResolver:\n    \"\"\"Test symbol resolution and code understanding\"\"\"\n    \n    def setup_method(self):\n        \"\"\"Set up test fixtures\"\"\"\n        self.temp_dir = tempfile.mkdtemp()\n        self.resolver = SymbolResolver(self.temp_dir)\n    \n    def test_analyze_python_symbols(self):\n        \"\"\"Test Python symbol extraction\"\"\"\n        python_code = \"\"\"\nimport os\nfrom datetime import datetime\n\ndef process_data():\n    pass\n\nclass DataProcessor:\n    def run(self):\n        pass\n\"\"\"\n        self.resolver.analyze_python_symbols('test.py', python_code)\n        \n        # Check symbols\n        assert 'process_data' in self.resolver.symbols\n        assert 'DataProcessor' in self.resolver.symbols\n        \n        # Check imports\n        assert 'test.py' in self.resolver.imports\n        assert 'os' in self.resolver.imports['test.py']\n        assert 'datetime' in self.resolver.imports['test.py']\n    \n    def test_analyze_javascript_symbols(self):\n        \"\"\"Test JavaScript symbol extraction\"\"\"\n        js_code = \"\"\"\nimport React from 'react';\nimport axios from 'axios';\n\nfunction App() {\n    return <div>Hello</div>;\n}\n\nconst fetchData = async () => {\n    return await axios.get('/api/data');\n};\n\nexport default App;\n\"\"\"\n        self.resolver.analyze_javascript_symbols('App.jsx', js_code)\n        \n        # Check symbols\n        assert 'App' in self.resolver.symbols\n        assert 'fetchData' in self.resolver.symbols\n        \n        # Check imports\n        assert 'App.jsx' in self.resolver.imports\n        assert any('react' in imp for imp in self.resolver.imports['App.jsx'])\n        \n        # Check exports\n        assert 'App.jsx' in self.resolver.exports\n    \n    def test_find_definition(self):\n        \"\"\"Test finding symbol definitions\"\"\"\n        python_code = \"\"\"\ndef my_function():\n    pass\n\"\"\"\n        self.resolver.analyze_python_symbols('test.py', python_code)\n        \n        definitions = self.resolver.find_definition('my_function')\n        \n        assert len(definitions) == 1\n        assert definitions[0]['file'] == 'test.py'\n        assert definitions[0]['type'] == 'function'\n    \n    def test_find_usages(self):\n        \"\"\"Test finding symbol usages\"\"\"\n        code = \"\"\"\ndef helper():\n    pass\n\ndef main():\n    helper()\n    result = helper()\n    return helper\n\"\"\"\n        usages = self.resolver.find_usages('helper', code)\n        \n        assert len(usages) >= 3  # Should find all 3 usages\n    \n    def test_search_symbols(self):\n        \"\"\"Test symbol search\"\"\"\n        self.resolver.analyze_python_symbols('test.py', \"\"\"\ndef user_login():\n    pass\n\ndef user_logout():\n    pass\n\ndef admin_login():\n    pass\n\"\"\")\n        \n        results = self.resolver.search_symbols('user')\n        \n        assert len(results) >= 2  # user_login and user_logout\n        assert any('user_login' in r['symbol'] for r in results)\n        assert any('user_logout' in r['symbol'] for r in results)", "docstring": "Test symbol resolution and code understanding", "searchable_text": "TestSymbolResolver Test symbol resolution and code understanding class TestSymbolResolver:\n    \"\"\"Test symbol resolution and code understanding\"\"\"\n    \n    def setup_method(self):\n        \"\"\"Set up test fixtures\"\"\"\n        self.temp_dir = tempfile.mkdtemp()\n        self.resolver = SymbolResolver(self.temp_dir)\n    \n    def test_analyze_python_symbols(self):\n        \"\"\"Test Python symbol extraction\"\"\"\n        python_code = \"\"\"\nimport os\nfrom datetime import datetime\n\ndef process_data():\n    pass\n\nclass DataProcessor:\n    def run(self):\n        pass\n\"\"\"\n        self.resolver.analyze_python_symbols('test.py', python_code)\n        \n        # Check symbols\n        assert 'process_data' in self.resolver.symbols\n        assert 'DataProcessor' in self.resolver.symbols\n        \n        # Check imports\n        assert 'test.py' in self.resolver.imports\n        assert 'os' in self.resolver.imports['test.py']\n        assert 'datetime' in self.resolver.imports['test.py']\n    \n    def test_analyze_javascript_symbols(self):\n        \"\"\"Test JavaScript symbol extraction\"\"\"\n        js_code = \"\"\"\nimport React from 'react';\nimport axios from 'axios';\n\nfunction App() {\n    return <div>Hello</div>;\n}\n\nconst fetchData = async () => {\n    return await axios.get('/api/data');\n};\n\nexport default App;\n\"\"\"\n        self.resolver.analyze_javascript_symbols('App.jsx', js_code)\n        \n        # Check symbols\n        assert 'App' in self.resolver.symbols\n        assert 'fetchData' in self.resolver.symbols\n        \n        # Check imports\n        assert 'App.jsx' in self.resolver.imports\n        assert any('react' in imp for imp in self.resolver.imports['App.jsx'])\n        \n        # Check exports\n        assert 'App.jsx' in self.resolver.exports\n    \n    def test_find_definition(self):\n        \"\"\"Test finding symbol definitions\"\"\"\n        python_code = \"\"\"\ndef my_function():\n    pass\n\"\"\"\n        self.resolver.analyze_python_symbols('test.py', python_code)\n        \n        definitions = self.resolver.find_definition('my_function')\n        \n        assert len(definitions) == 1\n        assert definitions[0]['file'] == 'test.py'\n        assert definitions[0]['type'] == 'function'\n    \n    def test_find_usages(self):\n        \"\"\"Test finding symbol usages\"\"\"\n        code = \"\"\"\ndef helper():\n    pass\n\ndef main():\n    helper()\n    result = helper()\n    return helper\n\"\"\"\n        usages = self.resolver.find_usages('helper', code)\n        \n        assert len(usages) >= 3  # Should find all 3 usages\n    \n    def test_search_symbols(self):\n        \"\"\"Test symbol search\"\"\"\n        self.resolver.analyze_python_symbols('test.py', \"\"\"\ndef user_login():\n    pass\n\ndef user_logout():\n    pass\n\ndef admin_login():\n    pass\n\"\"\")\n        \n        results = self.resolver.search_symbols('user')\n        \n        assert len(results) >= 2  # user_login and user_logout\n        assert any('user_login' in r['symbol'] for r in results)\n        assert any('user_logout' in r['symbol'] for r in results)"}, {"filepath": "tests\\test_intelligent_features.py", "start_line": 220, "end_line": 333, "type": "class", "name": "TestQueryAnalyzer", "content": "class TestQueryAnalyzer:\n    \"\"\"Test query intent classification\"\"\"\n    \n    def setup_method(self):\n        \"\"\"Set up test fixtures\"\"\"\n        self.analyzer = QueryAnalyzer()\n    \n    def test_find_function_intent(self):\n        \"\"\"Test function finding intent\"\"\"\n        queries = [\n            \"Where is the login function?\",\n            \"Find the authenticate function\",\n            \"Show me the process_data function\"\n        ]\n        \n        for query in queries:\n            analysis = self.analyzer.analyze(query)\n            assert analysis['intent'] == 'find_function'\n            assert len(analysis['entities']) > 0\n    \n    def test_find_class_intent(self):\n        \"\"\"Test class finding intent\"\"\"\n        queries = [\n            \"Where is the User class?\",\n            \"Find the DataProcessor class\"\n        ]\n        \n        for query in queries:\n            analysis = self.analyzer.analyze(query)\n            assert analysis['intent'] == 'find_class'\n    \n    def test_find_usages_intent(self):\n        \"\"\"Test usage finding intent\"\"\"\n        queries = [\n            \"Where is authenticate used?\",\n            \"Who calls the save method?\",\n            \"Find usages of User\"\n        ]\n        \n        for query in queries:\n            analysis = self.analyzer.analyze(query)\n            assert analysis['intent'] == 'find_usages'\n    \n    def test_explain_code_intent(self):\n        \"\"\"Test code explanation intent\"\"\"\n        queries = [\n            \"Explain the authentication system\",\n            \"What does the login function do?\",\n            \"How does payment processing work?\"\n        ]\n        \n        for query in queries:\n            analysis = self.analyzer.analyze(query)\n            assert analysis['intent'] == 'explain_code'\n    \n    def test_architecture_intent(self):\n        \"\"\"Test architecture intent\"\"\"\n        queries = [\n            \"What's the project architecture?\",\n            \"Show me the design patterns\",\n            \"How is the project structured?\"\n        ]\n        \n        for query in queries:\n            analysis = self.analyzer.analyze(query)\n            assert analysis['intent'] == 'architecture'\n    \n    def test_find_bugs_intent(self):\n        \"\"\"Test bug finding intent\"\"\"\n        queries = [\n            \"Find security issues\",\n            \"Review for vulnerabilities\",\n            \"Find bugs in the code\"\n        ]\n        \n        for query in queries:\n            analysis = self.analyzer.analyze(query)\n            assert analysis['intent'] == 'find_bugs'\n    \n    def test_general_intent(self):\n        \"\"\"Test general queries\"\"\"\n        query = \"Tell me about this project\"\n        analysis = self.analyzer.analyze(query)\n        \n        assert analysis['intent'] == 'general'\n    \n    def test_extract_keywords(self):\n        \"\"\"Test keyword extraction\"\"\"\n        query = \"How does the authentication system work in this project?\"\n        keywords = self.analyzer.get_search_keywords(query)\n        \n        assert 'authentication' in keywords\n        assert 'system' in keywords\n        assert 'work' in keywords\n        assert 'project' in keywords\n        # Stop words should be removed\n        assert 'the' not in keywords\n        assert 'in' not in keywords\n    \n    def test_should_use_semantic_search(self):\n        \"\"\"Test semantic search decision\"\"\"\n        analysis = {'intent': 'explain_code', 'entities': []}\n        assert self.analyzer.should_use_semantic_search(analysis) == True\n        \n        analysis = {'intent': 'find_function', 'entities': ['login']}\n        assert self.analyzer.should_use_semantic_search(analysis) == False\n    \n    def test_should_use_symbol_search(self):\n        \"\"\"Test symbol search decision\"\"\"\n        analysis = {'intent': 'find_function', 'entities': ['login']}\n        assert self.analyzer.should_use_symbol_search(analysis) == True\n        \n        analysis = {'intent': 'explain_code', 'entities': []}\n        assert self.analyzer.should_use_symbol_search(analysis) == False", "docstring": "Test query intent classification", "searchable_text": "TestQueryAnalyzer Test query intent classification class TestQueryAnalyzer:\n    \"\"\"Test query intent classification\"\"\"\n    \n    def setup_method(self):\n        \"\"\"Set up test fixtures\"\"\"\n        self.analyzer = QueryAnalyzer()\n    \n    def test_find_function_intent(self):\n        \"\"\"Test function finding intent\"\"\"\n        queries = [\n            \"Where is the login function?\",\n            \"Find the authenticate function\",\n            \"Show me the process_data function\"\n        ]\n        \n        for query in queries:\n            analysis = self.analyzer.analyze(query)\n            assert analysis['intent'] == 'find_function'\n            assert len(analysis['entities']) > 0\n    \n    def test_find_class_intent(self):\n        \"\"\"Test class finding intent\"\"\"\n        queries = [\n            \"Where is the User class?\",\n            \"Find the DataProcessor class\"\n        ]\n        \n        for query in queries:\n            analysis = self.analyzer.analyze(query)\n            assert analysis['intent'] == 'find_class'\n    \n    def test_find_usages_intent(self):\n        \"\"\"Test usage finding intent\"\"\"\n        queries = [\n            \"Where is authenticate used?\",\n            \"Who calls the save method?\",\n            \"Find usages of User\"\n        ]\n        \n        for query in queries:\n            analysis = self.analyzer.analyze(query)\n            assert analysis['intent'] == 'find_usages'\n    \n    def test_explain_code_intent(self):\n        \"\"\"Test code explanation intent\"\"\"\n        queries = [\n            \"Explain the authentication system\",\n            \"What does the login function do?\",\n            \"How does payment processing work?\"\n        ]\n        \n        for query in queries:\n            analysis = self.analyzer.analyze(query)\n            assert analysis['intent'] == 'explain_code'\n    \n    def test_architecture_intent(self):\n        \"\"\"Test architecture intent\"\"\"\n        queries = [\n            \"What's the project architecture?\",\n            \"Show me the design patterns\",\n            \"How is the project structured?\"\n        ]\n        \n        for query in queries:\n            analysis = self.analyzer.analyze(query)\n            assert analysis['intent'] == 'architecture'\n    \n    def test_find_bugs_intent(self):\n        \"\"\"Test bug finding intent\"\"\"\n        queries = [\n            \"Find security issues\",\n            \"Review for vulnerabilities\",\n            \"Find bugs in the code\"\n        ]\n        \n        for query in queries:\n            analysis = self.analyzer.analyze(query)\n            assert analysis['intent'] == 'find_bugs'\n    \n    def test_general_intent(self):\n        \"\"\"Test general queries\"\"\"\n        query = \"Tell me about this project\"\n        analysis = self.analyzer.analyze(query)\n        \n        assert analysis['intent'] == 'general'\n    \n    def test_extract_keywords(self):\n        \"\"\"Test keyword extraction\"\"\"\n        query = \"How does the authentication system work in this project?\"\n        keywords = self.analyzer.get_search_keywords(query)\n        \n        assert 'authentication' in keywords\n        assert 'system' in keywords\n        assert 'work' in keywords\n        assert 'project' in keywords\n        # Stop words should be removed\n        assert 'the' not in keywords\n        assert 'in' not in keywords\n    \n    def test_should_use_semantic_search(self):\n        \"\"\"Test semantic search decision\"\"\"\n        analysis = {'intent': 'explain_code', 'entities': []}\n        assert self.analyzer.should_use_semantic_search(analysis) == True\n        \n        analysis = {'intent': 'find_function', 'entities': ['login']}\n        assert self.analyzer.should_use_semantic_search(analysis) == False\n    \n    def test_should_use_symbol_search(self):\n        \"\"\"Test symbol search decision\"\"\"\n        analysis = {'intent': 'find_function', 'entities': ['login']}\n        assert self.analyzer.should_use_symbol_search(analysis) == True\n        \n        analysis = {'intent': 'explain_code', 'entities': []}\n        assert self.analyzer.should_use_symbol_search(analysis) == False"}, {"filepath": "tests\\test_intelligent_features.py", "start_line": 25, "end_line": 29, "type": "function", "name": "setup_method", "content": "    def setup_method(self):\n        \"\"\"Set up test fixtures\"\"\"\n        self.temp_dir = tempfile.mkdtemp()\n        self.console_mock = Mock()\n        self.engine = ContextEngine(self.temp_dir, self.console_mock)", "docstring": "Set up test fixtures", "searchable_text": "setup_method Set up test fixtures     def setup_method(self):\n        \"\"\"Set up test fixtures\"\"\"\n        self.temp_dir = tempfile.mkdtemp()\n        self.console_mock = Mock()\n        self.engine = ContextEngine(self.temp_dir, self.console_mock)"}, {"filepath": "tests\\test_intelligent_features.py", "start_line": 31, "end_line": 34, "type": "function", "name": "teardown_method", "content": "    def teardown_method(self):\n        \"\"\"Clean up\"\"\"\n        import shutil\n        shutil.rmtree(self.temp_dir, ignore_errors=True)", "docstring": "Clean up", "searchable_text": "teardown_method Clean up     def teardown_method(self):\n        \"\"\"Clean up\"\"\"\n        import shutil\n        shutil.rmtree(self.temp_dir, ignore_errors=True)"}, {"filepath": "tests\\test_intelligent_features.py", "start_line": 36, "end_line": 54, "type": "function", "name": "test_chunk_python_code", "content": "    def test_chunk_python_code(self):\n        \"\"\"Test Python code chunking\"\"\"\n        python_code = \"\"\"\ndef hello():\n    '''Say hello'''\n    return \"Hello\"\n\nclass MyClass:\n    '''A test class'''\n    def method(self):\n        pass\n\"\"\"\n        chunks = self.engine._chunk_python('test.py', python_code)\n        \n        assert len(chunks) >= 2  # Should find function and class\n        assert any(c['type'] == 'function' for c in chunks)\n        assert any(c['type'] == 'class' for c in chunks)\n        assert any(c['name'] == 'hello' for c in chunks)\n        assert any(c['name'] == 'MyClass' for c in chunks)", "docstring": "Test Python code chunking", "searchable_text": "test_chunk_python_code Test Python code chunking     def test_chunk_python_code(self):\n        \"\"\"Test Python code chunking\"\"\"\n        python_code = \"\"\"\ndef hello():\n    '''Say hello'''\n    return \"Hello\"\n\nclass MyClass:\n    '''A test class'''\n    def method(self):\n        pass\n\"\"\"\n        chunks = self.engine._chunk_python('test.py', python_code)\n        \n        assert len(chunks) >= 2  # Should find function and class\n        assert any(c['type'] == 'function' for c in chunks)\n        assert any(c['type'] == 'class' for c in chunks)\n        assert any(c['name'] == 'hello' for c in chunks)\n        assert any(c['name'] == 'MyClass' for c in chunks)"}, {"filepath": "tests\\test_intelligent_features.py", "start_line": 56, "end_line": 76, "type": "function", "name": "test_chunk_javascript_code", "content": "    def test_chunk_javascript_code(self):\n        \"\"\"Test JavaScript code chunking\"\"\"\n        js_code = \"\"\"\nfunction greet(name) {\n    return `Hello ${name}`;\n}\n\nconst add = (a, b) => a + b;\n\nclass Calculator {\n    multiply(a, b) {\n        return a * b;\n    }\n}\n\"\"\"\n        chunks = self.engine._chunk_javascript('test.js', js_code)\n        \n        assert len(chunks) >= 3  # Function, arrow function, class\n        assert any('greet' in c['name'] for c in chunks)\n        assert any('add' in c['name'] for c in chunks)\n        assert any('Calculator' in c['name'] for c in chunks)", "docstring": "Test JavaScript code chunking", "searchable_text": "test_chunk_javascript_code Test JavaScript code chunking     def test_chunk_javascript_code(self):\n        \"\"\"Test JavaScript code chunking\"\"\"\n        js_code = \"\"\"\nfunction greet(name) {\n    return `Hello ${name}`;\n}\n\nconst add = (a, b) => a + b;\n\nclass Calculator {\n    multiply(a, b) {\n        return a * b;\n    }\n}\n\"\"\"\n        chunks = self.engine._chunk_javascript('test.js', js_code)\n        \n        assert len(chunks) >= 3  # Function, arrow function, class\n        assert any('greet' in c['name'] for c in chunks)\n        assert any('add' in c['name'] for c in chunks)\n        assert any('Calculator' in c['name'] for c in chunks)"}, {"filepath": "tests\\test_intelligent_features.py", "start_line": 78, "end_line": 85, "type": "function", "name": "test_chunk_by_lines_fallback", "content": "    def test_chunk_by_lines_fallback(self):\n        \"\"\"Test fallback line-based chunking\"\"\"\n        content = \"\\n\".join([f\"line {i}\" for i in range(100)])\n        chunks = self.engine._chunk_by_lines('test.txt', content, chunk_size=20)\n        \n        assert len(chunks) == 5  # 100 lines / 20 per chunk\n        assert chunks[0]['start_line'] == 1\n        assert chunks[0]['end_line'] == 20", "docstring": "Test fallback line-based chunking", "searchable_text": "test_chunk_by_lines_fallback Test fallback line-based chunking     def test_chunk_by_lines_fallback(self):\n        \"\"\"Test fallback line-based chunking\"\"\"\n        content = \"\\n\".join([f\"line {i}\" for i in range(100)])\n        chunks = self.engine._chunk_by_lines('test.txt', content, chunk_size=20)\n        \n        assert len(chunks) == 5  # 100 lines / 20 per chunk\n        assert chunks[0]['start_line'] == 1\n        assert chunks[0]['end_line'] == 20"}, {"filepath": "tests\\test_intelligent_features.py", "start_line": 88, "end_line": 107, "type": "function", "name": "test_search_functionality", "content": "    def test_search_functionality(self):\n        \"\"\"Test semantic search\"\"\"\n        # Create mock code analyzer\n        code_analyzer = Mock()\n        code_analyzer.files = ['test.py']\n        code_analyzer.read_file_content = Mock(return_value=\"\"\"\ndef authenticate_user(username, password):\n    '''Authenticate a user with credentials'''\n    return check_credentials(username, password)\n\"\"\")\n        \n        # Build index\n        success = self.engine.build_index(code_analyzer)\n        \n        if success:\n            # Search for authentication\n            results = self.engine.search(\"user authentication\", top_k=1)\n            \n            assert len(results) > 0\n            assert 'authenticate' in results[0]['content'].lower()", "docstring": "Test semantic search", "searchable_text": "test_search_functionality Test semantic search     def test_search_functionality(self):\n        \"\"\"Test semantic search\"\"\"\n        # Create mock code analyzer\n        code_analyzer = Mock()\n        code_analyzer.files = ['test.py']\n        code_analyzer.read_file_content = Mock(return_value=\"\"\"\ndef authenticate_user(username, password):\n    '''Authenticate a user with credentials'''\n    return check_credentials(username, password)\n\"\"\")\n        \n        # Build index\n        success = self.engine.build_index(code_analyzer)\n        \n        if success:\n            # Search for authentication\n            results = self.engine.search(\"user authentication\", top_k=1)\n            \n            assert len(results) > 0\n            assert 'authenticate' in results[0]['content'].lower()"}, {"filepath": "tests\\test_intelligent_features.py", "start_line": 113, "end_line": 116, "type": "function", "name": "setup_method", "content": "    def setup_method(self):\n        \"\"\"Set up test fixtures\"\"\"\n        self.temp_dir = tempfile.mkdtemp()\n        self.resolver = SymbolResolver(self.temp_dir)", "docstring": "Set up test fixtures", "searchable_text": "setup_method Set up test fixtures     def setup_method(self):\n        \"\"\"Set up test fixtures\"\"\"\n        self.temp_dir = tempfile.mkdtemp()\n        self.resolver = SymbolResolver(self.temp_dir)"}, {"filepath": "tests\\test_intelligent_features.py", "start_line": 118, "end_line": 140, "type": "function", "name": "test_analyze_python_symbols", "content": "    def test_analyze_python_symbols(self):\n        \"\"\"Test Python symbol extraction\"\"\"\n        python_code = \"\"\"\nimport os\nfrom datetime import datetime\n\ndef process_data():\n    pass\n\nclass DataProcessor:\n    def run(self):\n        pass\n\"\"\"\n        self.resolver.analyze_python_symbols('test.py', python_code)\n        \n        # Check symbols\n        assert 'process_data' in self.resolver.symbols\n        assert 'DataProcessor' in self.resolver.symbols\n        \n        # Check imports\n        assert 'test.py' in self.resolver.imports\n        assert 'os' in self.resolver.imports['test.py']\n        assert 'datetime' in self.resolver.imports['test.py']", "docstring": "Test Python symbol extraction", "searchable_text": "test_analyze_python_symbols Test Python symbol extraction     def test_analyze_python_symbols(self):\n        \"\"\"Test Python symbol extraction\"\"\"\n        python_code = \"\"\"\nimport os\nfrom datetime import datetime\n\ndef process_data():\n    pass\n\nclass DataProcessor:\n    def run(self):\n        pass\n\"\"\"\n        self.resolver.analyze_python_symbols('test.py', python_code)\n        \n        # Check symbols\n        assert 'process_data' in self.resolver.symbols\n        assert 'DataProcessor' in self.resolver.symbols\n        \n        # Check imports\n        assert 'test.py' in self.resolver.imports\n        assert 'os' in self.resolver.imports['test.py']\n        assert 'datetime' in self.resolver.imports['test.py']"}, {"filepath": "tests\\test_intelligent_features.py", "start_line": 142, "end_line": 169, "type": "function", "name": "test_analyze_javascript_symbols", "content": "    def test_analyze_javascript_symbols(self):\n        \"\"\"Test JavaScript symbol extraction\"\"\"\n        js_code = \"\"\"\nimport React from 'react';\nimport axios from 'axios';\n\nfunction App() {\n    return <div>Hello</div>;\n}\n\nconst fetchData = async () => {\n    return await axios.get('/api/data');\n};\n\nexport default App;\n\"\"\"\n        self.resolver.analyze_javascript_symbols('App.jsx', js_code)\n        \n        # Check symbols\n        assert 'App' in self.resolver.symbols\n        assert 'fetchData' in self.resolver.symbols\n        \n        # Check imports\n        assert 'App.jsx' in self.resolver.imports\n        assert any('react' in imp for imp in self.resolver.imports['App.jsx'])\n        \n        # Check exports\n        assert 'App.jsx' in self.resolver.exports", "docstring": "Test JavaScript symbol extraction", "searchable_text": "test_analyze_javascript_symbols Test JavaScript symbol extraction     def test_analyze_javascript_symbols(self):\n        \"\"\"Test JavaScript symbol extraction\"\"\"\n        js_code = \"\"\"\nimport React from 'react';\nimport axios from 'axios';\n\nfunction App() {\n    return <div>Hello</div>;\n}\n\nconst fetchData = async () => {\n    return await axios.get('/api/data');\n};\n\nexport default App;\n\"\"\"\n        self.resolver.analyze_javascript_symbols('App.jsx', js_code)\n        \n        # Check symbols\n        assert 'App' in self.resolver.symbols\n        assert 'fetchData' in self.resolver.symbols\n        \n        # Check imports\n        assert 'App.jsx' in self.resolver.imports\n        assert any('react' in imp for imp in self.resolver.imports['App.jsx'])\n        \n        # Check exports\n        assert 'App.jsx' in self.resolver.exports"}, {"filepath": "tests\\test_intelligent_features.py", "start_line": 171, "end_line": 183, "type": "function", "name": "test_find_definition", "content": "    def test_find_definition(self):\n        \"\"\"Test finding symbol definitions\"\"\"\n        python_code = \"\"\"\ndef my_function():\n    pass\n\"\"\"\n        self.resolver.analyze_python_symbols('test.py', python_code)\n        \n        definitions = self.resolver.find_definition('my_function')\n        \n        assert len(definitions) == 1\n        assert definitions[0]['file'] == 'test.py'\n        assert definitions[0]['type'] == 'function'", "docstring": "Test finding symbol definitions", "searchable_text": "test_find_definition Test finding symbol definitions     def test_find_definition(self):\n        \"\"\"Test finding symbol definitions\"\"\"\n        python_code = \"\"\"\ndef my_function():\n    pass\n\"\"\"\n        self.resolver.analyze_python_symbols('test.py', python_code)\n        \n        definitions = self.resolver.find_definition('my_function')\n        \n        assert len(definitions) == 1\n        assert definitions[0]['file'] == 'test.py'\n        assert definitions[0]['type'] == 'function'"}, {"filepath": "tests\\test_intelligent_features.py", "start_line": 185, "end_line": 198, "type": "function", "name": "test_find_usages", "content": "    def test_find_usages(self):\n        \"\"\"Test finding symbol usages\"\"\"\n        code = \"\"\"\ndef helper():\n    pass\n\ndef main():\n    helper()\n    result = helper()\n    return helper\n\"\"\"\n        usages = self.resolver.find_usages('helper', code)\n        \n        assert len(usages) >= 3  # Should find all 3 usages", "docstring": "Test finding symbol usages", "searchable_text": "test_find_usages Test finding symbol usages     def test_find_usages(self):\n        \"\"\"Test finding symbol usages\"\"\"\n        code = \"\"\"\ndef helper():\n    pass\n\ndef main():\n    helper()\n    result = helper()\n    return helper\n\"\"\"\n        usages = self.resolver.find_usages('helper', code)\n        \n        assert len(usages) >= 3  # Should find all 3 usages"}, {"filepath": "tests\\test_intelligent_features.py", "start_line": 200, "end_line": 217, "type": "function", "name": "test_search_symbols", "content": "    def test_search_symbols(self):\n        \"\"\"Test symbol search\"\"\"\n        self.resolver.analyze_python_symbols('test.py', \"\"\"\ndef user_login():\n    pass\n\ndef user_logout():\n    pass\n\ndef admin_login():\n    pass\n\"\"\")\n        \n        results = self.resolver.search_symbols('user')\n        \n        assert len(results) >= 2  # user_login and user_logout\n        assert any('user_login' in r['symbol'] for r in results)\n        assert any('user_logout' in r['symbol'] for r in results)", "docstring": "Test symbol search", "searchable_text": "test_search_symbols Test symbol search     def test_search_symbols(self):\n        \"\"\"Test symbol search\"\"\"\n        self.resolver.analyze_python_symbols('test.py', \"\"\"\ndef user_login():\n    pass\n\ndef user_logout():\n    pass\n\ndef admin_login():\n    pass\n\"\"\")\n        \n        results = self.resolver.search_symbols('user')\n        \n        assert len(results) >= 2  # user_login and user_logout\n        assert any('user_login' in r['symbol'] for r in results)\n        assert any('user_logout' in r['symbol'] for r in results)"}, {"filepath": "tests\\test_intelligent_features.py", "start_line": 223, "end_line": 225, "type": "function", "name": "setup_method", "content": "    def setup_method(self):\n        \"\"\"Set up test fixtures\"\"\"\n        self.analyzer = QueryAnalyzer()", "docstring": "Set up test fixtures", "searchable_text": "setup_method Set up test fixtures     def setup_method(self):\n        \"\"\"Set up test fixtures\"\"\"\n        self.analyzer = QueryAnalyzer()"}, {"filepath": "tests\\test_intelligent_features.py", "start_line": 227, "end_line": 238, "type": "function", "name": "test_find_function_intent", "content": "    def test_find_function_intent(self):\n        \"\"\"Test function finding intent\"\"\"\n        queries = [\n            \"Where is the login function?\",\n            \"Find the authenticate function\",\n            \"Show me the process_data function\"\n        ]\n        \n        for query in queries:\n            analysis = self.analyzer.analyze(query)\n            assert analysis['intent'] == 'find_function'\n            assert len(analysis['entities']) > 0", "docstring": "Test function finding intent", "searchable_text": "test_find_function_intent Test function finding intent     def test_find_function_intent(self):\n        \"\"\"Test function finding intent\"\"\"\n        queries = [\n            \"Where is the login function?\",\n            \"Find the authenticate function\",\n            \"Show me the process_data function\"\n        ]\n        \n        for query in queries:\n            analysis = self.analyzer.analyze(query)\n            assert analysis['intent'] == 'find_function'\n            assert len(analysis['entities']) > 0"}, {"filepath": "tests\\test_intelligent_features.py", "start_line": 240, "end_line": 249, "type": "function", "name": "test_find_class_intent", "content": "    def test_find_class_intent(self):\n        \"\"\"Test class finding intent\"\"\"\n        queries = [\n            \"Where is the User class?\",\n            \"Find the DataProcessor class\"\n        ]\n        \n        for query in queries:\n            analysis = self.analyzer.analyze(query)\n            assert analysis['intent'] == 'find_class'", "docstring": "Test class finding intent", "searchable_text": "test_find_class_intent Test class finding intent     def test_find_class_intent(self):\n        \"\"\"Test class finding intent\"\"\"\n        queries = [\n            \"Where is the User class?\",\n            \"Find the DataProcessor class\"\n        ]\n        \n        for query in queries:\n            analysis = self.analyzer.analyze(query)\n            assert analysis['intent'] == 'find_class'"}, {"filepath": "tests\\test_intelligent_features.py", "start_line": 251, "end_line": 261, "type": "function", "name": "test_find_usages_intent", "content": "    def test_find_usages_intent(self):\n        \"\"\"Test usage finding intent\"\"\"\n        queries = [\n            \"Where is authenticate used?\",\n            \"Who calls the save method?\",\n            \"Find usages of User\"\n        ]\n        \n        for query in queries:\n            analysis = self.analyzer.analyze(query)\n            assert analysis['intent'] == 'find_usages'", "docstring": "Test usage finding intent", "searchable_text": "test_find_usages_intent Test usage finding intent     def test_find_usages_intent(self):\n        \"\"\"Test usage finding intent\"\"\"\n        queries = [\n            \"Where is authenticate used?\",\n            \"Who calls the save method?\",\n            \"Find usages of User\"\n        ]\n        \n        for query in queries:\n            analysis = self.analyzer.analyze(query)\n            assert analysis['intent'] == 'find_usages'"}, {"filepath": "tests\\test_intelligent_features.py", "start_line": 263, "end_line": 273, "type": "function", "name": "test_explain_code_intent", "content": "    def test_explain_code_intent(self):\n        \"\"\"Test code explanation intent\"\"\"\n        queries = [\n            \"Explain the authentication system\",\n            \"What does the login function do?\",\n            \"How does payment processing work?\"\n        ]\n        \n        for query in queries:\n            analysis = self.analyzer.analyze(query)\n            assert analysis['intent'] == 'explain_code'", "docstring": "Test code explanation intent", "searchable_text": "test_explain_code_intent Test code explanation intent     def test_explain_code_intent(self):\n        \"\"\"Test code explanation intent\"\"\"\n        queries = [\n            \"Explain the authentication system\",\n            \"What does the login function do?\",\n            \"How does payment processing work?\"\n        ]\n        \n        for query in queries:\n            analysis = self.analyzer.analyze(query)\n            assert analysis['intent'] == 'explain_code'"}, {"filepath": "tests\\test_intelligent_features.py", "start_line": 275, "end_line": 285, "type": "function", "name": "test_architecture_intent", "content": "    def test_architecture_intent(self):\n        \"\"\"Test architecture intent\"\"\"\n        queries = [\n            \"What's the project architecture?\",\n            \"Show me the design patterns\",\n            \"How is the project structured?\"\n        ]\n        \n        for query in queries:\n            analysis = self.analyzer.analyze(query)\n            assert analysis['intent'] == 'architecture'", "docstring": "Test architecture intent", "searchable_text": "test_architecture_intent Test architecture intent     def test_architecture_intent(self):\n        \"\"\"Test architecture intent\"\"\"\n        queries = [\n            \"What's the project architecture?\",\n            \"Show me the design patterns\",\n            \"How is the project structured?\"\n        ]\n        \n        for query in queries:\n            analysis = self.analyzer.analyze(query)\n            assert analysis['intent'] == 'architecture'"}, {"filepath": "tests\\test_intelligent_features.py", "start_line": 287, "end_line": 297, "type": "function", "name": "test_find_bugs_intent", "content": "    def test_find_bugs_intent(self):\n        \"\"\"Test bug finding intent\"\"\"\n        queries = [\n            \"Find security issues\",\n            \"Review for vulnerabilities\",\n            \"Find bugs in the code\"\n        ]\n        \n        for query in queries:\n            analysis = self.analyzer.analyze(query)\n            assert analysis['intent'] == 'find_bugs'", "docstring": "Test bug finding intent", "searchable_text": "test_find_bugs_intent Test bug finding intent     def test_find_bugs_intent(self):\n        \"\"\"Test bug finding intent\"\"\"\n        queries = [\n            \"Find security issues\",\n            \"Review for vulnerabilities\",\n            \"Find bugs in the code\"\n        ]\n        \n        for query in queries:\n            analysis = self.analyzer.analyze(query)\n            assert analysis['intent'] == 'find_bugs'"}, {"filepath": "tests\\test_intelligent_features.py", "start_line": 299, "end_line": 304, "type": "function", "name": "test_general_intent", "content": "    def test_general_intent(self):\n        \"\"\"Test general queries\"\"\"\n        query = \"Tell me about this project\"\n        analysis = self.analyzer.analyze(query)\n        \n        assert analysis['intent'] == 'general'", "docstring": "Test general queries", "searchable_text": "test_general_intent Test general queries     def test_general_intent(self):\n        \"\"\"Test general queries\"\"\"\n        query = \"Tell me about this project\"\n        analysis = self.analyzer.analyze(query)\n        \n        assert analysis['intent'] == 'general'"}, {"filepath": "tests\\test_intelligent_features.py", "start_line": 306, "end_line": 317, "type": "function", "name": "test_extract_keywords", "content": "    def test_extract_keywords(self):\n        \"\"\"Test keyword extraction\"\"\"\n        query = \"How does the authentication system work in this project?\"\n        keywords = self.analyzer.get_search_keywords(query)\n        \n        assert 'authentication' in keywords\n        assert 'system' in keywords\n        assert 'work' in keywords\n        assert 'project' in keywords\n        # Stop words should be removed\n        assert 'the' not in keywords\n        assert 'in' not in keywords", "docstring": "Test keyword extraction", "searchable_text": "test_extract_keywords Test keyword extraction     def test_extract_keywords(self):\n        \"\"\"Test keyword extraction\"\"\"\n        query = \"How does the authentication system work in this project?\"\n        keywords = self.analyzer.get_search_keywords(query)\n        \n        assert 'authentication' in keywords\n        assert 'system' in keywords\n        assert 'work' in keywords\n        assert 'project' in keywords\n        # Stop words should be removed\n        assert 'the' not in keywords\n        assert 'in' not in keywords"}, {"filepath": "tests\\test_intelligent_features.py", "start_line": 319, "end_line": 325, "type": "function", "name": "test_should_use_semantic_search", "content": "    def test_should_use_semantic_search(self):\n        \"\"\"Test semantic search decision\"\"\"\n        analysis = {'intent': 'explain_code', 'entities': []}\n        assert self.analyzer.should_use_semantic_search(analysis) == True\n        \n        analysis = {'intent': 'find_function', 'entities': ['login']}\n        assert self.analyzer.should_use_semantic_search(analysis) == False", "docstring": "Test semantic search decision", "searchable_text": "test_should_use_semantic_search Test semantic search decision     def test_should_use_semantic_search(self):\n        \"\"\"Test semantic search decision\"\"\"\n        analysis = {'intent': 'explain_code', 'entities': []}\n        assert self.analyzer.should_use_semantic_search(analysis) == True\n        \n        analysis = {'intent': 'find_function', 'entities': ['login']}\n        assert self.analyzer.should_use_semantic_search(analysis) == False"}, {"filepath": "tests\\test_intelligent_features.py", "start_line": 327, "end_line": 333, "type": "function", "name": "test_should_use_symbol_search", "content": "    def test_should_use_symbol_search(self):\n        \"\"\"Test symbol search decision\"\"\"\n        analysis = {'intent': 'find_function', 'entities': ['login']}\n        assert self.analyzer.should_use_symbol_search(analysis) == True\n        \n        analysis = {'intent': 'explain_code', 'entities': []}\n        assert self.analyzer.should_use_symbol_search(analysis) == False", "docstring": "Test symbol search decision", "searchable_text": "test_should_use_symbol_search Test symbol search decision     def test_should_use_symbol_search(self):\n        \"\"\"Test symbol search decision\"\"\"\n        analysis = {'intent': 'find_function', 'entities': ['login']}\n        assert self.analyzer.should_use_symbol_search(analysis) == True\n        \n        analysis = {'intent': 'explain_code', 'entities': []}\n        assert self.analyzer.should_use_symbol_search(analysis) == False"}, {"filepath": "tests\\test_security.py", "start_line": 16, "end_line": 83, "type": "class", "name": "TestConfigManagerSecurity", "content": "class TestConfigManagerSecurity:\n    \"\"\"Test security features of ConfigManager\"\"\"\n    \n    def test_secure_config_directory_creation(self):\n        \"\"\"Test that config directory is created in secure location\"\"\"\n        console_mock = Mock()\n        \n        with patch('platform.system', return_value='Linux'):\n            config_manager = ConfigManager(console_mock)\n            expected_path = os.path.join(os.path.expanduser('~'), '.config', 'maplecli')\n            assert config_manager.config_dir == expected_path\n        \n        with patch('platform.system', return_value='Windows'):\n            config_manager = ConfigManager(console_mock)\n            expected_path = os.path.join(os.environ.get('APPDATA', ''), 'maplecli')\n            assert config_manager.config_dir == expected_path\n        \n        with patch('platform.system', return_value='Darwin'):\n            config_manager = ConfigManager(console_mock)\n            expected_path = os.path.join(os.path.expanduser('~'), 'Library', 'Application Support', 'maplecli')\n            assert config_manager.config_dir == expected_path\n    \n    def test_secure_api_key_input(self):\n        \"\"\"Test that API key input is hidden\"\"\"\n        console_mock = Mock()\n        config_manager = ConfigManager(console_mock)\n        \n        with patch('getpass.getpass', return_value='hidden_key'):\n            with patch('builtins.input', return_value='test_url'):\n                config_manager.api_base = None\n                config_manager.api_key = None\n                config_manager.load_config()\n                assert config_manager.api_key == 'hidden_key'\n    \n    def test_fallback_to_visible_input(self):\n        \"\"\"Test fallback to visible input when getpass fails\"\"\"\n        console_mock = Mock()\n        config_manager = ConfigManager(console_mock)\n        \n        with patch('getpass.getpass', side_effect=Exception(\"getpass failed\")):\n            with patch('builtins.input', return_value='visible_key'):\n                with patch.object(console_mock, 'print'):\n                    config_manager.api_base = None\n                    config_manager.api_key = None\n                    config_manager.load_config()\n                    assert config_manager.api_key == 'visible_key'\n    \n    def test_atomic_config_save(self):\n        \"\"\"Test that config is saved atomically\"\"\"\n        console_mock = Mock()\n        \n        with tempfile.TemporaryDirectory() as temp_dir:\n            config_manager = ConfigManager(console_mock)\n            config_manager.config_dir = temp_dir\n            config_manager.config_file = os.path.join(temp_dir, 'config.json')\n            config_manager.api_base = 'test_url'\n            config_manager.api_key = 'test_key'\n            \n            config_manager.save_config()\n            \n            # Verify file was created\n            assert os.path.exists(config_manager.config_file)\n            \n            # Verify content\n            with open(config_manager.config_file, 'r') as f:\n                data = json.load(f)\n                assert data['OPENAI_API_BASE'] == 'test_url'\n                assert data['OPENAI_API_KEY'] == 'test_key'", "docstring": "Test security features of ConfigManager", "searchable_text": "TestConfigManagerSecurity Test security features of ConfigManager class TestConfigManagerSecurity:\n    \"\"\"Test security features of ConfigManager\"\"\"\n    \n    def test_secure_config_directory_creation(self):\n        \"\"\"Test that config directory is created in secure location\"\"\"\n        console_mock = Mock()\n        \n        with patch('platform.system', return_value='Linux'):\n            config_manager = ConfigManager(console_mock)\n            expected_path = os.path.join(os.path.expanduser('~'), '.config', 'maplecli')\n            assert config_manager.config_dir == expected_path\n        \n        with patch('platform.system', return_value='Windows'):\n            config_manager = ConfigManager(console_mock)\n            expected_path = os.path.join(os.environ.get('APPDATA', ''), 'maplecli')\n            assert config_manager.config_dir == expected_path\n        \n        with patch('platform.system', return_value='Darwin'):\n            config_manager = ConfigManager(console_mock)\n            expected_path = os.path.join(os.path.expanduser('~'), 'Library', 'Application Support', 'maplecli')\n            assert config_manager.config_dir == expected_path\n    \n    def test_secure_api_key_input(self):\n        \"\"\"Test that API key input is hidden\"\"\"\n        console_mock = Mock()\n        config_manager = ConfigManager(console_mock)\n        \n        with patch('getpass.getpass', return_value='hidden_key'):\n            with patch('builtins.input', return_value='test_url'):\n                config_manager.api_base = None\n                config_manager.api_key = None\n                config_manager.load_config()\n                assert config_manager.api_key == 'hidden_key'\n    \n    def test_fallback_to_visible_input(self):\n        \"\"\"Test fallback to visible input when getpass fails\"\"\"\n        console_mock = Mock()\n        config_manager = ConfigManager(console_mock)\n        \n        with patch('getpass.getpass', side_effect=Exception(\"getpass failed\")):\n            with patch('builtins.input', return_value='visible_key'):\n                with patch.object(console_mock, 'print'):\n                    config_manager.api_base = None\n                    config_manager.api_key = None\n                    config_manager.load_config()\n                    assert config_manager.api_key == 'visible_key'\n    \n    def test_atomic_config_save(self):\n        \"\"\"Test that config is saved atomically\"\"\"\n        console_mock = Mock()\n        \n        with tempfile.TemporaryDirectory() as temp_dir:\n            config_manager = ConfigManager(console_mock)\n            config_manager.config_dir = temp_dir\n            config_manager.config_file = os.path.join(temp_dir, 'config.json')\n            config_manager.api_base = 'test_url'\n            config_manager.api_key = 'test_key'\n            \n            config_manager.save_config()\n            \n            # Verify file was created\n            assert os.path.exists(config_manager.config_file)\n            \n            # Verify content\n            with open(config_manager.config_file, 'r') as f:\n                data = json.load(f)\n                assert data['OPENAI_API_BASE'] == 'test_url'\n                assert data['OPENAI_API_KEY'] == 'test_key'"}, {"filepath": "tests\\test_security.py", "start_line": 86, "end_line": 156, "type": "class", "name": "TestCodeAnalyzerSecurity", "content": "class TestCodeAnalyzerSecurity:\n    \"\"\"Test security features of CodeAnalyzer\"\"\"\n    \n    def setup_method(self):\n        \"\"\"Set up test fixtures\"\"\"\n        self.console_mock = Mock()\n        self.temp_dir = tempfile.mkdtemp()\n        self.analyzer = CodeAnalyzer(self.temp_dir, self.console_mock)\n    \n    def teardown_method(self):\n        \"\"\"Clean up test fixtures\"\"\"\n        import shutil\n        shutil.rmtree(self.temp_dir, ignore_errors=True)\n    \n    def test_path_traversal_prevention(self):\n        \"\"\"Test that path traversal attacks are prevented\"\"\"\n        # Test direct path traversal\n        with pytest.raises(SecurityError):\n            self.analyzer._safe_join_path(self.temp_dir, '../../../etc/passwd')\n        \n        # Test encoded path traversal\n        with pytest.raises(SecurityError):\n            self.analyzer._safe_join_path(self.temp_dir, '..%2F..%2F..%2Fetc%2Fpasswd')\n        \n        # Test absolute path\n        with pytest.raises(SecurityError):\n            self.analyzer._safe_join_path(self.temp_dir, '/etc/passwd')\n    \n    def test_safe_path_validation(self):\n        \"\"\"Test safe path validation\"\"\"\n        # Test valid relative path\n        safe_path = self.analyzer._safe_join_path(self.temp_dir, 'src/main.py')\n        expected = os.path.join(self.temp_dir, 'src', 'main.py')\n        assert safe_path == expected\n        \n        # Test valid nested path\n        safe_path = self.analyzer._safe_join_path(self.temp_dir, 'src/utils/helper.py')\n        expected = os.path.join(self.temp_dir, 'src', 'utils', 'helper.py')\n        assert safe_path == expected\n    \n    def test_file_size_limits(self):\n        \"\"\"Test file size limits are enforced\"\"\"\n        # Create a large file\n        large_file = os.path.join(self.temp_dir, 'large.py')\n        with open(large_file, 'w') as f:\n            f.write('x' * (11 * 1024 * 1024))  # 11MB file\n        \n        result = self.analyzer.read_file_content('large.py')\n        assert 'too large' in result\n    \n    def test_regular_file_validation(self):\n        \"\"\"Test that only regular files are processed\"\"\"\n        # Create a directory\n        test_dir = os.path.join(self.temp_dir, 'test_dir')\n        os.makedirs(test_dir)\n        \n        result = self.analyzer.read_file_content('test_dir')\n        assert 'Not a regular file' in result\n    \n    def test_max_total_size_enforcement(self):\n        \"\"\"Test that total size limits are enforced\"\"\"\n        # Create multiple files that exceed the total limit\n        for i in range(12):  # 12 files of 10MB each = 120MB > 100MB limit\n            large_file = os.path.join(self.temp_dir, f'large_{i}.py')\n            with open(large_file, 'w') as f:\n                f.write('x' * (10 * 1024 * 1024))  # 10MB each\n        \n        # This should trigger the size limit during scanning\n        result = asyncio.run(self.analyzer.scan_project_async())\n        assert 'skipped_files' in result\n        assert len(result['skipped_files']) > 0", "docstring": "Test security features of CodeAnalyzer", "searchable_text": "TestCodeAnalyzerSecurity Test security features of CodeAnalyzer class TestCodeAnalyzerSecurity:\n    \"\"\"Test security features of CodeAnalyzer\"\"\"\n    \n    def setup_method(self):\n        \"\"\"Set up test fixtures\"\"\"\n        self.console_mock = Mock()\n        self.temp_dir = tempfile.mkdtemp()\n        self.analyzer = CodeAnalyzer(self.temp_dir, self.console_mock)\n    \n    def teardown_method(self):\n        \"\"\"Clean up test fixtures\"\"\"\n        import shutil\n        shutil.rmtree(self.temp_dir, ignore_errors=True)\n    \n    def test_path_traversal_prevention(self):\n        \"\"\"Test that path traversal attacks are prevented\"\"\"\n        # Test direct path traversal\n        with pytest.raises(SecurityError):\n            self.analyzer._safe_join_path(self.temp_dir, '../../../etc/passwd')\n        \n        # Test encoded path traversal\n        with pytest.raises(SecurityError):\n            self.analyzer._safe_join_path(self.temp_dir, '..%2F..%2F..%2Fetc%2Fpasswd')\n        \n        # Test absolute path\n        with pytest.raises(SecurityError):\n            self.analyzer._safe_join_path(self.temp_dir, '/etc/passwd')\n    \n    def test_safe_path_validation(self):\n        \"\"\"Test safe path validation\"\"\"\n        # Test valid relative path\n        safe_path = self.analyzer._safe_join_path(self.temp_dir, 'src/main.py')\n        expected = os.path.join(self.temp_dir, 'src', 'main.py')\n        assert safe_path == expected\n        \n        # Test valid nested path\n        safe_path = self.analyzer._safe_join_path(self.temp_dir, 'src/utils/helper.py')\n        expected = os.path.join(self.temp_dir, 'src', 'utils', 'helper.py')\n        assert safe_path == expected\n    \n    def test_file_size_limits(self):\n        \"\"\"Test file size limits are enforced\"\"\"\n        # Create a large file\n        large_file = os.path.join(self.temp_dir, 'large.py')\n        with open(large_file, 'w') as f:\n            f.write('x' * (11 * 1024 * 1024))  # 11MB file\n        \n        result = self.analyzer.read_file_content('large.py')\n        assert 'too large' in result\n    \n    def test_regular_file_validation(self):\n        \"\"\"Test that only regular files are processed\"\"\"\n        # Create a directory\n        test_dir = os.path.join(self.temp_dir, 'test_dir')\n        os.makedirs(test_dir)\n        \n        result = self.analyzer.read_file_content('test_dir')\n        assert 'Not a regular file' in result\n    \n    def test_max_total_size_enforcement(self):\n        \"\"\"Test that total size limits are enforced\"\"\"\n        # Create multiple files that exceed the total limit\n        for i in range(12):  # 12 files of 10MB each = 120MB > 100MB limit\n            large_file = os.path.join(self.temp_dir, f'large_{i}.py')\n            with open(large_file, 'w') as f:\n                f.write('x' * (10 * 1024 * 1024))  # 10MB each\n        \n        # This should trigger the size limit during scanning\n        result = asyncio.run(self.analyzer.scan_project_async())\n        assert 'skipped_files' in result\n        assert len(result['skipped_files']) > 0"}, {"filepath": "tests\\test_security.py", "start_line": 159, "end_line": 182, "type": "class", "name": "TestSecurityLogging", "content": "class TestSecurityLogging:\n    \"\"\"Test security logging functionality\"\"\"\n    \n    def test_security_event_logging(self):\n        \"\"\"Test that security events are properly logged\"\"\"\n        logger = MapleLogger()\n        \n        with patch.object(logger.logger, 'warning') as mock_warning:\n            logger.log_security_event(\"Test Event\", \"Test details\")\n            mock_warning.assert_called_once_with(\"SECURITY: Test Event - Test details\")\n    \n    def test_error_logging_with_context(self):\n        \"\"\"Test error logging with structured context\"\"\"\n        logger = MapleLogger()\n        test_error = ValueError(\"Test error\")\n        \n        with patch.object(logger.logger, 'error') as mock_error:\n            logger.log_error(test_error, \"test_operation\", \"test_file.py\", \"high\")\n            \n            # Verify the error was logged with correct context\n            mock_error.assert_called_once()\n            call_args = mock_error.call_args[0][0]\n            assert \"test_operation\" in call_args\n            assert \"Test error\" in call_args", "docstring": "Test security logging functionality", "searchable_text": "TestSecurityLogging Test security logging functionality class TestSecurityLogging:\n    \"\"\"Test security logging functionality\"\"\"\n    \n    def test_security_event_logging(self):\n        \"\"\"Test that security events are properly logged\"\"\"\n        logger = MapleLogger()\n        \n        with patch.object(logger.logger, 'warning') as mock_warning:\n            logger.log_security_event(\"Test Event\", \"Test details\")\n            mock_warning.assert_called_once_with(\"SECURITY: Test Event - Test details\")\n    \n    def test_error_logging_with_context(self):\n        \"\"\"Test error logging with structured context\"\"\"\n        logger = MapleLogger()\n        test_error = ValueError(\"Test error\")\n        \n        with patch.object(logger.logger, 'error') as mock_error:\n            logger.log_error(test_error, \"test_operation\", \"test_file.py\", \"high\")\n            \n            # Verify the error was logged with correct context\n            mock_error.assert_called_once()\n            call_args = mock_error.call_args[0][0]\n            assert \"test_operation\" in call_args\n            assert \"Test error\" in call_args"}, {"filepath": "tests\\test_security.py", "start_line": 185, "end_line": 223, "type": "class", "name": "TestInputValidation", "content": "class TestInputValidation:\n    \"\"\"Test input validation and sanitization\"\"\"\n    \n    def test_config_validation(self):\n        \"\"\"Test configuration validation\"\"\"\n        console_mock = Mock()\n        config_manager = ConfigManager(console_mock)\n        \n        # Test invalid API base URL\n        with patch('builtins.input', return_value='not-a-url'):\n            with patch.object(console_mock, 'print'):\n                config_manager.api_base = None\n                config_manager.load_config()\n                # Should still accept but warn about invalid format\n    \n    def test_file_extension_filtering(self):\n        \"\"\"Test that only allowed file extensions are processed\"\"\"\n        console_mock = Mock()\n        temp_dir = tempfile.mkdtemp()\n        \n        try:\n            analyzer = CodeAnalyzer(temp_dir, console_mock)\n            \n            # Create test files with different extensions\n            allowed_file = os.path.join(temp_dir, 'test.py')\n            disallowed_file = os.path.join(temp_dir, 'test.exe')\n            \n            with open(allowed_file, 'w') as f:\n                f.write('print(\"hello\")')\n            with open(disallowed_file, 'w') as f:\n                f.write('binary content')\n            \n            # Test that only allowed extensions are processed\n            assert '.py' in analyzer.CODE_EXTENSIONS\n            assert '.exe' not in analyzer.CODE_EXTENSIONS\n            \n        finally:\n            import shutil\n            shutil.rmtree(temp_dir, ignore_errors=True)", "docstring": "Test input validation and sanitization", "searchable_text": "TestInputValidation Test input validation and sanitization class TestInputValidation:\n    \"\"\"Test input validation and sanitization\"\"\"\n    \n    def test_config_validation(self):\n        \"\"\"Test configuration validation\"\"\"\n        console_mock = Mock()\n        config_manager = ConfigManager(console_mock)\n        \n        # Test invalid API base URL\n        with patch('builtins.input', return_value='not-a-url'):\n            with patch.object(console_mock, 'print'):\n                config_manager.api_base = None\n                config_manager.load_config()\n                # Should still accept but warn about invalid format\n    \n    def test_file_extension_filtering(self):\n        \"\"\"Test that only allowed file extensions are processed\"\"\"\n        console_mock = Mock()\n        temp_dir = tempfile.mkdtemp()\n        \n        try:\n            analyzer = CodeAnalyzer(temp_dir, console_mock)\n            \n            # Create test files with different extensions\n            allowed_file = os.path.join(temp_dir, 'test.py')\n            disallowed_file = os.path.join(temp_dir, 'test.exe')\n            \n            with open(allowed_file, 'w') as f:\n                f.write('print(\"hello\")')\n            with open(disallowed_file, 'w') as f:\n                f.write('binary content')\n            \n            # Test that only allowed extensions are processed\n            assert '.py' in analyzer.CODE_EXTENSIONS\n            assert '.exe' not in analyzer.CODE_EXTENSIONS\n            \n        finally:\n            import shutil\n            shutil.rmtree(temp_dir, ignore_errors=True)"}, {"filepath": "tests\\test_security.py", "start_line": 19, "end_line": 36, "type": "function", "name": "test_secure_config_directory_creation", "content": "    def test_secure_config_directory_creation(self):\n        \"\"\"Test that config directory is created in secure location\"\"\"\n        console_mock = Mock()\n        \n        with patch('platform.system', return_value='Linux'):\n            config_manager = ConfigManager(console_mock)\n            expected_path = os.path.join(os.path.expanduser('~'), '.config', 'maplecli')\n            assert config_manager.config_dir == expected_path\n        \n        with patch('platform.system', return_value='Windows'):\n            config_manager = ConfigManager(console_mock)\n            expected_path = os.path.join(os.environ.get('APPDATA', ''), 'maplecli')\n            assert config_manager.config_dir == expected_path\n        \n        with patch('platform.system', return_value='Darwin'):\n            config_manager = ConfigManager(console_mock)\n            expected_path = os.path.join(os.path.expanduser('~'), 'Library', 'Application Support', 'maplecli')\n            assert config_manager.config_dir == expected_path", "docstring": "Test that config directory is created in secure location", "searchable_text": "test_secure_config_directory_creation Test that config directory is created in secure location     def test_secure_config_directory_creation(self):\n        \"\"\"Test that config directory is created in secure location\"\"\"\n        console_mock = Mock()\n        \n        with patch('platform.system', return_value='Linux'):\n            config_manager = ConfigManager(console_mock)\n            expected_path = os.path.join(os.path.expanduser('~'), '.config', 'maplecli')\n            assert config_manager.config_dir == expected_path\n        \n        with patch('platform.system', return_value='Windows'):\n            config_manager = ConfigManager(console_mock)\n            expected_path = os.path.join(os.environ.get('APPDATA', ''), 'maplecli')\n            assert config_manager.config_dir == expected_path\n        \n        with patch('platform.system', return_value='Darwin'):\n            config_manager = ConfigManager(console_mock)\n            expected_path = os.path.join(os.path.expanduser('~'), 'Library', 'Application Support', 'maplecli')\n            assert config_manager.config_dir == expected_path"}, {"filepath": "tests\\test_security.py", "start_line": 38, "end_line": 48, "type": "function", "name": "test_secure_api_key_input", "content": "    def test_secure_api_key_input(self):\n        \"\"\"Test that API key input is hidden\"\"\"\n        console_mock = Mock()\n        config_manager = ConfigManager(console_mock)\n        \n        with patch('getpass.getpass', return_value='hidden_key'):\n            with patch('builtins.input', return_value='test_url'):\n                config_manager.api_base = None\n                config_manager.api_key = None\n                config_manager.load_config()\n                assert config_manager.api_key == 'hidden_key'", "docstring": "Test that API key input is hidden", "searchable_text": "test_secure_api_key_input Test that API key input is hidden     def test_secure_api_key_input(self):\n        \"\"\"Test that API key input is hidden\"\"\"\n        console_mock = Mock()\n        config_manager = ConfigManager(console_mock)\n        \n        with patch('getpass.getpass', return_value='hidden_key'):\n            with patch('builtins.input', return_value='test_url'):\n                config_manager.api_base = None\n                config_manager.api_key = None\n                config_manager.load_config()\n                assert config_manager.api_key == 'hidden_key'"}, {"filepath": "tests\\test_security.py", "start_line": 50, "end_line": 61, "type": "function", "name": "test_fallback_to_visible_input", "content": "    def test_fallback_to_visible_input(self):\n        \"\"\"Test fallback to visible input when getpass fails\"\"\"\n        console_mock = Mock()\n        config_manager = ConfigManager(console_mock)\n        \n        with patch('getpass.getpass', side_effect=Exception(\"getpass failed\")):\n            with patch('builtins.input', return_value='visible_key'):\n                with patch.object(console_mock, 'print'):\n                    config_manager.api_base = None\n                    config_manager.api_key = None\n                    config_manager.load_config()\n                    assert config_manager.api_key == 'visible_key'", "docstring": "Test fallback to visible input when getpass fails", "searchable_text": "test_fallback_to_visible_input Test fallback to visible input when getpass fails     def test_fallback_to_visible_input(self):\n        \"\"\"Test fallback to visible input when getpass fails\"\"\"\n        console_mock = Mock()\n        config_manager = ConfigManager(console_mock)\n        \n        with patch('getpass.getpass', side_effect=Exception(\"getpass failed\")):\n            with patch('builtins.input', return_value='visible_key'):\n                with patch.object(console_mock, 'print'):\n                    config_manager.api_base = None\n                    config_manager.api_key = None\n                    config_manager.load_config()\n                    assert config_manager.api_key == 'visible_key'"}, {"filepath": "tests\\test_security.py", "start_line": 63, "end_line": 83, "type": "function", "name": "test_atomic_config_save", "content": "    def test_atomic_config_save(self):\n        \"\"\"Test that config is saved atomically\"\"\"\n        console_mock = Mock()\n        \n        with tempfile.TemporaryDirectory() as temp_dir:\n            config_manager = ConfigManager(console_mock)\n            config_manager.config_dir = temp_dir\n            config_manager.config_file = os.path.join(temp_dir, 'config.json')\n            config_manager.api_base = 'test_url'\n            config_manager.api_key = 'test_key'\n            \n            config_manager.save_config()\n            \n            # Verify file was created\n            assert os.path.exists(config_manager.config_file)\n            \n            # Verify content\n            with open(config_manager.config_file, 'r') as f:\n                data = json.load(f)\n                assert data['OPENAI_API_BASE'] == 'test_url'\n                assert data['OPENAI_API_KEY'] == 'test_key'", "docstring": "Test that config is saved atomically", "searchable_text": "test_atomic_config_save Test that config is saved atomically     def test_atomic_config_save(self):\n        \"\"\"Test that config is saved atomically\"\"\"\n        console_mock = Mock()\n        \n        with tempfile.TemporaryDirectory() as temp_dir:\n            config_manager = ConfigManager(console_mock)\n            config_manager.config_dir = temp_dir\n            config_manager.config_file = os.path.join(temp_dir, 'config.json')\n            config_manager.api_base = 'test_url'\n            config_manager.api_key = 'test_key'\n            \n            config_manager.save_config()\n            \n            # Verify file was created\n            assert os.path.exists(config_manager.config_file)\n            \n            # Verify content\n            with open(config_manager.config_file, 'r') as f:\n                data = json.load(f)\n                assert data['OPENAI_API_BASE'] == 'test_url'\n                assert data['OPENAI_API_KEY'] == 'test_key'"}, {"filepath": "tests\\test_security.py", "start_line": 89, "end_line": 93, "type": "function", "name": "setup_method", "content": "    def setup_method(self):\n        \"\"\"Set up test fixtures\"\"\"\n        self.console_mock = Mock()\n        self.temp_dir = tempfile.mkdtemp()\n        self.analyzer = CodeAnalyzer(self.temp_dir, self.console_mock)", "docstring": "Set up test fixtures", "searchable_text": "setup_method Set up test fixtures     def setup_method(self):\n        \"\"\"Set up test fixtures\"\"\"\n        self.console_mock = Mock()\n        self.temp_dir = tempfile.mkdtemp()\n        self.analyzer = CodeAnalyzer(self.temp_dir, self.console_mock)"}, {"filepath": "tests\\test_security.py", "start_line": 95, "end_line": 98, "type": "function", "name": "teardown_method", "content": "    def teardown_method(self):\n        \"\"\"Clean up test fixtures\"\"\"\n        import shutil\n        shutil.rmtree(self.temp_dir, ignore_errors=True)", "docstring": "Clean up test fixtures", "searchable_text": "teardown_method Clean up test fixtures     def teardown_method(self):\n        \"\"\"Clean up test fixtures\"\"\"\n        import shutil\n        shutil.rmtree(self.temp_dir, ignore_errors=True)"}, {"filepath": "tests\\test_security.py", "start_line": 100, "end_line": 112, "type": "function", "name": "test_path_traversal_prevention", "content": "    def test_path_traversal_prevention(self):\n        \"\"\"Test that path traversal attacks are prevented\"\"\"\n        # Test direct path traversal\n        with pytest.raises(SecurityError):\n            self.analyzer._safe_join_path(self.temp_dir, '../../../etc/passwd')\n        \n        # Test encoded path traversal\n        with pytest.raises(SecurityError):\n            self.analyzer._safe_join_path(self.temp_dir, '..%2F..%2F..%2Fetc%2Fpasswd')\n        \n        # Test absolute path\n        with pytest.raises(SecurityError):\n            self.analyzer._safe_join_path(self.temp_dir, '/etc/passwd')", "docstring": "Test that path traversal attacks are prevented", "searchable_text": "test_path_traversal_prevention Test that path traversal attacks are prevented     def test_path_traversal_prevention(self):\n        \"\"\"Test that path traversal attacks are prevented\"\"\"\n        # Test direct path traversal\n        with pytest.raises(SecurityError):\n            self.analyzer._safe_join_path(self.temp_dir, '../../../etc/passwd')\n        \n        # Test encoded path traversal\n        with pytest.raises(SecurityError):\n            self.analyzer._safe_join_path(self.temp_dir, '..%2F..%2F..%2Fetc%2Fpasswd')\n        \n        # Test absolute path\n        with pytest.raises(SecurityError):\n            self.analyzer._safe_join_path(self.temp_dir, '/etc/passwd')"}, {"filepath": "tests\\test_security.py", "start_line": 114, "end_line": 124, "type": "function", "name": "test_safe_path_validation", "content": "    def test_safe_path_validation(self):\n        \"\"\"Test safe path validation\"\"\"\n        # Test valid relative path\n        safe_path = self.analyzer._safe_join_path(self.temp_dir, 'src/main.py')\n        expected = os.path.join(self.temp_dir, 'src', 'main.py')\n        assert safe_path == expected\n        \n        # Test valid nested path\n        safe_path = self.analyzer._safe_join_path(self.temp_dir, 'src/utils/helper.py')\n        expected = os.path.join(self.temp_dir, 'src', 'utils', 'helper.py')\n        assert safe_path == expected", "docstring": "Test safe path validation", "searchable_text": "test_safe_path_validation Test safe path validation     def test_safe_path_validation(self):\n        \"\"\"Test safe path validation\"\"\"\n        # Test valid relative path\n        safe_path = self.analyzer._safe_join_path(self.temp_dir, 'src/main.py')\n        expected = os.path.join(self.temp_dir, 'src', 'main.py')\n        assert safe_path == expected\n        \n        # Test valid nested path\n        safe_path = self.analyzer._safe_join_path(self.temp_dir, 'src/utils/helper.py')\n        expected = os.path.join(self.temp_dir, 'src', 'utils', 'helper.py')\n        assert safe_path == expected"}, {"filepath": "tests\\test_security.py", "start_line": 126, "end_line": 134, "type": "function", "name": "test_file_size_limits", "content": "    def test_file_size_limits(self):\n        \"\"\"Test file size limits are enforced\"\"\"\n        # Create a large file\n        large_file = os.path.join(self.temp_dir, 'large.py')\n        with open(large_file, 'w') as f:\n            f.write('x' * (11 * 1024 * 1024))  # 11MB file\n        \n        result = self.analyzer.read_file_content('large.py')\n        assert 'too large' in result", "docstring": "Test file size limits are enforced", "searchable_text": "test_file_size_limits Test file size limits are enforced     def test_file_size_limits(self):\n        \"\"\"Test file size limits are enforced\"\"\"\n        # Create a large file\n        large_file = os.path.join(self.temp_dir, 'large.py')\n        with open(large_file, 'w') as f:\n            f.write('x' * (11 * 1024 * 1024))  # 11MB file\n        \n        result = self.analyzer.read_file_content('large.py')\n        assert 'too large' in result"}, {"filepath": "tests\\test_security.py", "start_line": 136, "end_line": 143, "type": "function", "name": "test_regular_file_validation", "content": "    def test_regular_file_validation(self):\n        \"\"\"Test that only regular files are processed\"\"\"\n        # Create a directory\n        test_dir = os.path.join(self.temp_dir, 'test_dir')\n        os.makedirs(test_dir)\n        \n        result = self.analyzer.read_file_content('test_dir')\n        assert 'Not a regular file' in result", "docstring": "Test that only regular files are processed", "searchable_text": "test_regular_file_validation Test that only regular files are processed     def test_regular_file_validation(self):\n        \"\"\"Test that only regular files are processed\"\"\"\n        # Create a directory\n        test_dir = os.path.join(self.temp_dir, 'test_dir')\n        os.makedirs(test_dir)\n        \n        result = self.analyzer.read_file_content('test_dir')\n        assert 'Not a regular file' in result"}, {"filepath": "tests\\test_security.py", "start_line": 145, "end_line": 156, "type": "function", "name": "test_max_total_size_enforcement", "content": "    def test_max_total_size_enforcement(self):\n        \"\"\"Test that total size limits are enforced\"\"\"\n        # Create multiple files that exceed the total limit\n        for i in range(12):  # 12 files of 10MB each = 120MB > 100MB limit\n            large_file = os.path.join(self.temp_dir, f'large_{i}.py')\n            with open(large_file, 'w') as f:\n                f.write('x' * (10 * 1024 * 1024))  # 10MB each\n        \n        # This should trigger the size limit during scanning\n        result = asyncio.run(self.analyzer.scan_project_async())\n        assert 'skipped_files' in result\n        assert len(result['skipped_files']) > 0", "docstring": "Test that total size limits are enforced", "searchable_text": "test_max_total_size_enforcement Test that total size limits are enforced     def test_max_total_size_enforcement(self):\n        \"\"\"Test that total size limits are enforced\"\"\"\n        # Create multiple files that exceed the total limit\n        for i in range(12):  # 12 files of 10MB each = 120MB > 100MB limit\n            large_file = os.path.join(self.temp_dir, f'large_{i}.py')\n            with open(large_file, 'w') as f:\n                f.write('x' * (10 * 1024 * 1024))  # 10MB each\n        \n        # This should trigger the size limit during scanning\n        result = asyncio.run(self.analyzer.scan_project_async())\n        assert 'skipped_files' in result\n        assert len(result['skipped_files']) > 0"}, {"filepath": "tests\\test_security.py", "start_line": 162, "end_line": 168, "type": "function", "name": "test_security_event_logging", "content": "    def test_security_event_logging(self):\n        \"\"\"Test that security events are properly logged\"\"\"\n        logger = MapleLogger()\n        \n        with patch.object(logger.logger, 'warning') as mock_warning:\n            logger.log_security_event(\"Test Event\", \"Test details\")\n            mock_warning.assert_called_once_with(\"SECURITY: Test Event - Test details\")", "docstring": "Test that security events are properly logged", "searchable_text": "test_security_event_logging Test that security events are properly logged     def test_security_event_logging(self):\n        \"\"\"Test that security events are properly logged\"\"\"\n        logger = MapleLogger()\n        \n        with patch.object(logger.logger, 'warning') as mock_warning:\n            logger.log_security_event(\"Test Event\", \"Test details\")\n            mock_warning.assert_called_once_with(\"SECURITY: Test Event - Test details\")"}, {"filepath": "tests\\test_security.py", "start_line": 170, "end_line": 182, "type": "function", "name": "test_error_logging_with_context", "content": "    def test_error_logging_with_context(self):\n        \"\"\"Test error logging with structured context\"\"\"\n        logger = MapleLogger()\n        test_error = ValueError(\"Test error\")\n        \n        with patch.object(logger.logger, 'error') as mock_error:\n            logger.log_error(test_error, \"test_operation\", \"test_file.py\", \"high\")\n            \n            # Verify the error was logged with correct context\n            mock_error.assert_called_once()\n            call_args = mock_error.call_args[0][0]\n            assert \"test_operation\" in call_args\n            assert \"Test error\" in call_args", "docstring": "Test error logging with structured context", "searchable_text": "test_error_logging_with_context Test error logging with structured context     def test_error_logging_with_context(self):\n        \"\"\"Test error logging with structured context\"\"\"\n        logger = MapleLogger()\n        test_error = ValueError(\"Test error\")\n        \n        with patch.object(logger.logger, 'error') as mock_error:\n            logger.log_error(test_error, \"test_operation\", \"test_file.py\", \"high\")\n            \n            # Verify the error was logged with correct context\n            mock_error.assert_called_once()\n            call_args = mock_error.call_args[0][0]\n            assert \"test_operation\" in call_args\n            assert \"Test error\" in call_args"}, {"filepath": "tests\\test_security.py", "start_line": 188, "end_line": 197, "type": "function", "name": "test_config_validation", "content": "    def test_config_validation(self):\n        \"\"\"Test configuration validation\"\"\"\n        console_mock = Mock()\n        config_manager = ConfigManager(console_mock)\n        \n        # Test invalid API base URL\n        with patch('builtins.input', return_value='not-a-url'):\n            with patch.object(console_mock, 'print'):\n                config_manager.api_base = None\n                config_manager.load_config()", "docstring": "Test configuration validation", "searchable_text": "test_config_validation Test configuration validation     def test_config_validation(self):\n        \"\"\"Test configuration validation\"\"\"\n        console_mock = Mock()\n        config_manager = ConfigManager(console_mock)\n        \n        # Test invalid API base URL\n        with patch('builtins.input', return_value='not-a-url'):\n            with patch.object(console_mock, 'print'):\n                config_manager.api_base = None\n                config_manager.load_config()"}, {"filepath": "tests\\test_security.py", "start_line": 200, "end_line": 223, "type": "function", "name": "test_file_extension_filtering", "content": "    def test_file_extension_filtering(self):\n        \"\"\"Test that only allowed file extensions are processed\"\"\"\n        console_mock = Mock()\n        temp_dir = tempfile.mkdtemp()\n        \n        try:\n            analyzer = CodeAnalyzer(temp_dir, console_mock)\n            \n            # Create test files with different extensions\n            allowed_file = os.path.join(temp_dir, 'test.py')\n            disallowed_file = os.path.join(temp_dir, 'test.exe')\n            \n            with open(allowed_file, 'w') as f:\n                f.write('print(\"hello\")')\n            with open(disallowed_file, 'w') as f:\n                f.write('binary content')\n            \n            # Test that only allowed extensions are processed\n            assert '.py' in analyzer.CODE_EXTENSIONS\n            assert '.exe' not in analyzer.CODE_EXTENSIONS\n            \n        finally:\n            import shutil\n            shutil.rmtree(temp_dir, ignore_errors=True)", "docstring": "Test that only allowed file extensions are processed", "searchable_text": "test_file_extension_filtering Test that only allowed file extensions are processed     def test_file_extension_filtering(self):\n        \"\"\"Test that only allowed file extensions are processed\"\"\"\n        console_mock = Mock()\n        temp_dir = tempfile.mkdtemp()\n        \n        try:\n            analyzer = CodeAnalyzer(temp_dir, console_mock)\n            \n            # Create test files with different extensions\n            allowed_file = os.path.join(temp_dir, 'test.py')\n            disallowed_file = os.path.join(temp_dir, 'test.exe')\n            \n            with open(allowed_file, 'w') as f:\n                f.write('print(\"hello\")')\n            with open(disallowed_file, 'w') as f:\n                f.write('binary content')\n            \n            # Test that only allowed extensions are processed\n            assert '.py' in analyzer.CODE_EXTENSIONS\n            assert '.exe' not in analyzer.CODE_EXTENSIONS\n            \n        finally:\n            import shutil\n            shutil.rmtree(temp_dir, ignore_errors=True)"}, {"filepath": "tests\\__init__.py", "start_line": 1, "end_line": 1, "type": "block", "name": "block_0", "content": "\"\"\"Test suite for MapleCLI\"\"\"", "searchable_text": "\"\"\"Test suite for MapleCLI\"\"\""}], "metadata": [{"filepath": "chat_client.py", "start_line": 10}, {"filepath": "chat_client.py", "start_line": 12}, {"filepath": "chat_client.py", "start_line": 19}, {"filepath": "chat_client.py", "start_line": 45}, {"filepath": "chat_client.py", "start_line": 99}, {"filepath": "chat_client.py", "start_line": 119}, {"filepath": "chat_client.py", "start_line": 140}, {"filepath": "cli.py", "start_line": 76}, {"filepath": "cli.py", "start_line": 78}, {"filepath": "cli.py", "start_line": 91}, {"filepath": "cli.py", "start_line": 118}, {"filepath": "cli.py", "start_line": 201}, {"filepath": "cli.py", "start_line": 224}, {"filepath": "cli.py", "start_line": 241}, {"filepath": "cli.py", "start_line": 305}, {"filepath": "code_analyzer.py", "start_line": 15}, {"filepath": "code_analyzer.py", "start_line": 45}, {"filepath": "code_analyzer.py", "start_line": 59}, {"filepath": "code_analyzer.py", "start_line": 71}, {"filepath": "code_analyzer.py", "start_line": 80}, {"filepath": "code_analyzer.py", "start_line": 140}, {"filepath": "code_analyzer.py", "start_line": 144}, {"filepath": "code_analyzer.py", "start_line": 167}, {"filepath": "code_analyzer.py", "start_line": 187}, {"filepath": "code_analyzer.py", "start_line": 194}, {"filepath": "code_analyzer.py", "start_line": 148}, {"filepath": "config_manager.py", "start_line": 14}, {"filepath": "config_manager.py", "start_line": 16}, {"filepath": "config_manager.py", "start_line": 24}, {"filepath": "config_manager.py", "start_line": 51}, {"filepath": "config_manager.py", "start_line": 80}, {"filepath": "config_manager.py", "start_line": 94}, {"filepath": "context_engine.py", "start_line": 25}, {"filepath": "context_engine.py", "start_line": 31}, {"filepath": "context_engine.py", "start_line": 50}, {"filepath": "context_engine.py", "start_line": 69}, {"filepath": "context_engine.py", "start_line": 108}, {"filepath": "context_engine.py", "start_line": 154}, {"filepath": "context_engine.py", "start_line": 200}, {"filepath": "context_engine.py", "start_line": 219}, {"filepath": "context_engine.py", "start_line": 301}, {"filepath": "context_engine.py", "start_line": 328}, {"filepath": "context_engine.py", "start_line": 344}, {"filepath": "context_engine.py", "start_line": 366}, {"filepath": "install_intelligent.bat", "start_line": 1}, {"filepath": "install_intelligent.bat", "start_line": 51}, {"filepath": "install_intelligent.sh", "start_line": 1}, {"filepath": "install_intelligent.sh", "start_line": 51}, {"filepath": "logger.py", "start_line": 17}, {"filepath": "logger.py", "start_line": 21}, {"filepath": "logger.py", "start_line": 23}, {"filepath": "logger.py", "start_line": 26}, {"filepath": "logger.py", "start_line": 37}, {"filepath": "main.py", "start_line": 7}, {"filepath": "pytest.ini", "start_line": 1}, {"filepath": "query_analyzer.py", "start_line": 13}, {"filepath": "query_analyzer.py", "start_line": 85}, {"filepath": "query_analyzer.py", "start_line": 117}, {"filepath": "query_analyzer.py", "start_line": 130}, {"filepath": "query_analyzer.py", "start_line": 141}, {"filepath": "query_analyzer.py", "start_line": 158}, {"filepath": "query_analyzer.py", "start_line": 164}, {"filepath": "query_analyzer.py", "start_line": 170}, {"filepath": "README.md", "start_line": 1}, {"filepath": "README.md", "start_line": 51}, {"filepath": "README.md", "start_line": 101}, {"filepath": "README.md", "start_line": 151}, {"filepath": "README.md", "start_line": 201}, {"filepath": "README.md", "start_line": 251}, {"filepath": "README.md", "start_line": 301}, {"filepath": "README.md", "start_line": 351}, {"filepath": "setup.py", "start_line": 1}, {"filepath": "setup.py", "start_line": 51}, {"filepath": "symbol_resolver.py", "start_line": 15}, {"filepath": "symbol_resolver.py", "start_line": 21}, {"filepath": "symbol_resolver.py", "start_line": 28}, {"filepath": "symbol_resolver.py", "start_line": 39}, {"filepath": "symbol_resolver.py", "start_line": 68}, {"filepath": "symbol_resolver.py", "start_line": 117}, {"filepath": "symbol_resolver.py", "start_line": 160}, {"filepath": "symbol_resolver.py", "start_line": 164}, {"filepath": "symbol_resolver.py", "start_line": 175}, {"filepath": "symbol_resolver.py", "start_line": 179}, {"filepath": "symbol_resolver.py", "start_line": 183}, {"filepath": "symbol_resolver.py", "start_line": 187}, {"filepath": "symbol_resolver.py", "start_line": 204}, {"filepath": "symbol_resolver.py", "start_line": 211}, {"filepath": "symbol_resolver.py", "start_line": 231}, {"filepath": "symbol_resolver.py", "start_line": 239}, {"filepath": "symbol_resolver.py", "start_line": 243}, {"filepath": "__init__.py", "start_line": 1}, {"filepath": "maplecli.egg-info\\dependency_links.txt", "start_line": 1}, {"filepath": "maplecli.egg-info\\entry_points.txt", "start_line": 1}, {"filepath": "maplecli.egg-info\\requires.txt", "start_line": 1}, {"filepath": "maplecli.egg-info\\SOURCES.txt", "start_line": 1}, {"filepath": "maplecli.egg-info\\top_level.txt", "start_line": 1}, {"filepath": "tests\\test_analyzer.py", "start_line": 16}, {"filepath": "tests\\test_analyzer.py", "start_line": 19}, {"filepath": "tests\\test_analyzer.py", "start_line": 25}, {"filepath": "tests\\test_analyzer.py", "start_line": 30}, {"filepath": "tests\\test_analyzer.py", "start_line": 38}, {"filepath": "tests\\test_analyzer.py", "start_line": 56}, {"filepath": "tests\\test_analyzer.py", "start_line": 70}, {"filepath": "tests\\test_analyzer.py", "start_line": 85}, {"filepath": "tests\\test_analyzer.py", "start_line": 103}, {"filepath": "tests\\test_analyzer.py", "start_line": 120}, {"filepath": "tests\\test_analyzer.py", "start_line": 136}, {"filepath": "tests\\test_analyzer.py", "start_line": 159}, {"filepath": "tests\\test_analyzer.py", "start_line": 189}, {"filepath": "tests\\test_analyzer.py", "start_line": 225}, {"filepath": "tests\\test_analyzer.py", "start_line": 262}, {"filepath": "tests\\test_analyzer.py", "start_line": 282}, {"filepath": "tests\\test_analyzer.py", "start_line": 299}, {"filepath": "tests\\test_analyzer.py", "start_line": 312}, {"filepath": "tests\\test_analyzer.py", "start_line": 329}, {"filepath": "tests\\test_analyzer.py", "start_line": 340}, {"filepath": "tests\\test_intelligent_features.py", "start_line": 22}, {"filepath": "tests\\test_intelligent_features.py", "start_line": 110}, {"filepath": "tests\\test_intelligent_features.py", "start_line": 220}, {"filepath": "tests\\test_intelligent_features.py", "start_line": 25}, {"filepath": "tests\\test_intelligent_features.py", "start_line": 31}, {"filepath": "tests\\test_intelligent_features.py", "start_line": 36}, {"filepath": "tests\\test_intelligent_features.py", "start_line": 56}, {"filepath": "tests\\test_intelligent_features.py", "start_line": 78}, {"filepath": "tests\\test_intelligent_features.py", "start_line": 88}, {"filepath": "tests\\test_intelligent_features.py", "start_line": 113}, {"filepath": "tests\\test_intelligent_features.py", "start_line": 118}, {"filepath": "tests\\test_intelligent_features.py", "start_line": 142}, {"filepath": "tests\\test_intelligent_features.py", "start_line": 171}, {"filepath": "tests\\test_intelligent_features.py", "start_line": 185}, {"filepath": "tests\\test_intelligent_features.py", "start_line": 200}, {"filepath": "tests\\test_intelligent_features.py", "start_line": 223}, {"filepath": "tests\\test_intelligent_features.py", "start_line": 227}, {"filepath": "tests\\test_intelligent_features.py", "start_line": 240}, {"filepath": "tests\\test_intelligent_features.py", "start_line": 251}, {"filepath": "tests\\test_intelligent_features.py", "start_line": 263}, {"filepath": "tests\\test_intelligent_features.py", "start_line": 275}, {"filepath": "tests\\test_intelligent_features.py", "start_line": 287}, {"filepath": "tests\\test_intelligent_features.py", "start_line": 299}, {"filepath": "tests\\test_intelligent_features.py", "start_line": 306}, {"filepath": "tests\\test_intelligent_features.py", "start_line": 319}, {"filepath": "tests\\test_intelligent_features.py", "start_line": 327}, {"filepath": "tests\\test_security.py", "start_line": 16}, {"filepath": "tests\\test_security.py", "start_line": 86}, {"filepath": "tests\\test_security.py", "start_line": 159}, {"filepath": "tests\\test_security.py", "start_line": 185}, {"filepath": "tests\\test_security.py", "start_line": 19}, {"filepath": "tests\\test_security.py", "start_line": 38}, {"filepath": "tests\\test_security.py", "start_line": 50}, {"filepath": "tests\\test_security.py", "start_line": 63}, {"filepath": "tests\\test_security.py", "start_line": 89}, {"filepath": "tests\\test_security.py", "start_line": 95}, {"filepath": "tests\\test_security.py", "start_line": 100}, {"filepath": "tests\\test_security.py", "start_line": 114}, {"filepath": "tests\\test_security.py", "start_line": 126}, {"filepath": "tests\\test_security.py", "start_line": 136}, {"filepath": "tests\\test_security.py", "start_line": 145}, {"filepath": "tests\\test_security.py", "start_line": 162}, {"filepath": "tests\\test_security.py", "start_line": 170}, {"filepath": "tests\\test_security.py", "start_line": 188}, {"filepath": "tests\\test_security.py", "start_line": 200}, {"filepath": "tests\\__init__.py", "start_line": 1}]}